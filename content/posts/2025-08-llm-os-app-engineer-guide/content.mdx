## 前言：90% 的 LLM 專案為何停在 POC 階段？

根據 Gartner 與 McKinsey 的觀察，超過八成的 LLM 專案停留在概念驗證（POC），從未進入正式生產環境。

原因不在技術不可行，而在於：

- 缺乏穩定性設計（non-deterministic output）
- 成本不可預測（token usage 波動大）
- 無法與現有系統整合（data pipeline, auth, audit log）
- 缺少監控與除錯工具

換句話說：我們擅長「展示能力」，但不擅長「工程建構後交付」。

這門課的目標，是填補這道鴻溝——
讓你從「會用 Token API 的開發者」，轉型為「能交付穩定 LLM OS 應用的系統設計者」。

> 💡 **定位明確**：你不需要理解 Transformer 的數學原理，也不需要會訓練模型。你需要的是把 LLM OS 當作「新時代的 API」，並用工程化的方式打造應用。

---

## 📊 LLM OS 應用工程師 vs 系統工程師

| 面向 | 系統工程師 | **應用工程師** | 
|---|---|---|
| **關注重點** | 底層架構、模型優化 | 產品體驗、業務價值 |
| **核心技能** | RAG 原理、向量資料庫、模型部署 | Prompt 版本管理、UX 設計、成本控制 |
| **工作內容** | 建構 LLM 基礎設施 | 開發 Bot、Copilot、Agent |
| **市場需求** | 每家公司 1-2 位 | **每家公司 5-10 位** |
| **學習曲線** | 陡峭（6-12 個月） | **適中（2-3 個月）** |

---

## 🎯 學習目標與能力出口

完成這個課程後，你將能夠：

### ✅ 核心能力
- 把一個 LLM OS 應用從**設計 → API → UX → 上線**
- 處理 **80% 的產品問題**（不穩定、成本高、體驗差）
- 具備**產品思維**，知道何時該用/不該用 LLM

### 🚀 可開發的應用類型
1. 對話與互動應用
- 智慧客服：處理複雜詢問、多輪對話、情緒管理
- 企業內部 Copilot：幫助員工快速查詢文件、操作系統
- 多語翻譯與文化轉換助手 <br />

👉 用到的模組：System Prompt、上下文管理、Function Calling

2. 文件與知識應用
- 文件助手：摘要、問答、比對、法規檢索
- 研究助理：文獻搜尋、比較、生成研究筆記
- 會議/合約助理：自動生成摘要、風險提示、重點標註 <br />

👉 用到的模組：RAG、進階 Context Engineering、Plan→Solve→Verify

3. 數據與分析應用
- 報告生成器：輸入數據，自動產出決策報告
- 財務分析助手：KPI 分析、異常檢測、決策建議
- 行銷數據助理：從 GA/CRM 撈數據 → 分析 → 生成 insights <br />

👉 用到的模組：Function Calling、Chain-of-Thought、成本工程

4. 開發者工具
- Coding Copilot：程式碼生成、解釋、重構建議
- 測試案例生成器：根據需求自動產出測試案例
- Pipeline Debugger：輔助排查數據流程或 API 連動問題 <br />

👉 用到的模組：LLM 選型、進階 Prompt 工程、自檢 / 自一致性

5. 商務與生產力
- 銷售助理：自動生成銷售話術、郵件回覆
- 專案助理：追蹤進度、生成週報、風險預警
- 行銷內容工廠：自動產生廣告文案、標題、社群貼文 <br />

👉 用到的模組：成本工程、上下文管理、可觀測性

6. Agent 與自動化
- Workflow Agent：自動完成多步驟任務（如搜尋 → 分析 → 報告）
- 多 Agent 協作：分工完成複雜專案
- 系統自動化：串接 ERP / CRM / 工單系統，減少人工流程 <br />

👉 用到的模組：Agent 架構、ReAct、工具路由、進階 RAG

學完這些模組，您不只是能做一個聊天機器人，而是能：
- 讓 LLM 穩定輸出（可控 / 可驗證）
- 把 LLM 嵌入不同業務流程（文件、數據、客服、開發）
- 設計長期可用的框架（成本、觀測、流程管理） <br />

👉 這就是 LLM OS 應用工程師的核心價值：把 LLM OS 當作「新一代 API」，應用到各種產品與產業場景。

### 📈 職涯發展路徑

<CareerPathDiagram />

---

## 🏗 完整課程架構（8 週速成）

### 📚 課程設計理念
- **70% 應用實作**：直接動手做產品
- **30% 系統必修**：理解必要的底層概念
- **100% 實戰導向**：每個模組都有實際產出

---

## 📘 模組 1：Prompt for Applications（第 1 週）

### 🎯 學習目標
從「寫 Prompt」升級到「管理 Prompt 資產」

### 📚 核心內容
在這個模組中，我們將深入探討如何將 Prompt Engineering 從一門藝術轉化為一門科學。您將學習如何系統化地管理、優化和測試您的 prompts，確保它們在實際應用中能夠發揮最大效用，並為您的產品或服務帶來可預測、可依賴的成果。

### 1.1 Prompt 版本管理：從開發到部署的雙循環
為什麼版本管理至關重要？
因為在 AI 應用中，每一條 Prompt 都是一個直接影響用戶體驗和商業成果的「產品功能」。

想像一下，您是一家運動品牌的電商，正在使用 LLM 為新上架的跑鞋自動生成行銷文案。

- 初始版本 (Prompt A - by 產品經理):
產品經理設計了一個穩定、中性的 Prompt，專注於準確描述跑鞋的功能和規格。這個版本上線後，產品頁面的購買轉換率穩定在 2%。

- 新版實驗 (Prompt B - by 行銷團隊):
為了迎接夏季促銷，行銷團隊提出了一個更具創意、更富煽動性的 Prompt，希望能激發消費者的購買慾望。新版本上線後，團隊發現廣告的點擊率 (CTR) 明顯上升，看似是個好消息。

這時，問題出現了：

一週後，數據分析師發現，雖然點擊率變高了，但最終的購買轉換率卻從 2% 下滑到了 1.5%。團隊推測，可能是過於誇大的文案風格，反而讓進入頁面的訪客產生了不信任感。

現在，整個產品和行銷團隊都面臨著一系列緊急問題：

1. 如何快速反應？ 我們能立刻將文案切換回表現更穩定的 Prompt A 嗎？還是只能等工程師緊急加班重新部署？

2. 如何驗證假設？ 我們能不能設計一個 A/B 測試，讓 50% 的用戶看到 Prompt A 的文案，另外 50% 看到 Prompt B 的文案，用真實數據來驗證我們的猜測？

3. 如何追蹤與學習？ Prompt B 到底在哪一天上線的？它的完整內容是什麼？我們從這次失敗的實驗中學到了什麼，如何記錄下來以避免未來再犯？

如果沒有一個好的版本管理系統，上述問題的答案將是災難性的。團隊會陷入混亂：手忙腳亂地尋找舊的 Prompt 文字、無法進行科學的 A/B 測試來指導決策、也無法系統性地從昂貴的失敗中積累經驗。

這就是為什麼版本管理至關重要。 它不僅僅是為了防止工程師的「失誤」，更是為了賦能「產品和業務團隊」，讓他們能夠安全、高效地進行商業實驗，用數據驅動決策，並最終找到能夠最大化商業價值的溝通方式。

#### 1.1.1 版本控制 (Version Control) - 您的 Prompt 開發與品管中心

版本控制不僅僅是為 prompt 加上 v1, v2 的標籤。一個專業的流程需要一個中央化的平台來管理、評估和迭代。

- 集中式管理：除了使用 Git，更專業的做法是將所有 Prompts 存放在專為 LLM 應用設計的平台上，例如 LangFuse 或 Pezzo。

    - 優點：這些平台提供了 UI 介面，讓團隊（包括非工程師）可以輕鬆查看、比較和管理所有 Prompts。您可以為每個 Prompt 附加元數據(metadata)，如作者、目標、以及版本說明。

- 明確的命名與註釋：在 LangFuse/Pezzo 這類工具中，您可以為每個 Prompt 創建不同的版本 (e.g., v1, v2)。這些版本不僅僅是文本不同，它們是您可以獨立追蹤和評估的對象。

- 與程式碼分離 (Decoupling)：

    - 這是 LangFuse/Pezzo 的核心優勢之一。您的應用程式程式碼不再寫死 Prompt 內容，而是透過 SDK 向這些平台請求特定版本的 Prompt (langfuse.get_prompt("marketing-copy", version=2))。這使得 Prompt 的更新和程式碼的部署完全分離。

#### 1.1.2 Feature Flag - 您的 Prompt 線上策略部署中心

當您在 LangFuse 或 Pezzo 中擁有了幾個經過測試、品質優良的 Prompt 候選版本後，下一個問題是：如何策略性地將它們部署到線上，並衡量其商業影響？這就是 Feature Flag 工具 LaunchDarkly 或 Flagsmith 發揮作用的地方。

- 整個工作流程是：

    1. 開發階段：在 LangFuse/Pezzo 中創建和版本化您的 Prompts (e.g., product-description-v1, product-description-v2)。

    2. 部署階段：在 LaunchDarkly 中創建一個名為 productDescriptionPrompt 的 Feature Flag，並設定兩個變體 (variants)：一個返回字串 "v1"，另一個返回字串 "v2"。

    3. 應用程式邏輯：您的程式碼首先訪問 LaunchDarkly 得到當前用戶應該使用哪個版本 ("v1" 或 "v2")，然後拿著這個版本號去 LangFuse/Pezzo 獲取對應的 Prompt 內容來執行。

- Feature Flag（功能旗標）的主要功能：

    - 動態切換 Prompts：想像您有一個 greeting_prompt。透過 Feature Flag，您可以設定一個參數，讓系統根據不同的條件（例如：使用者地區、會員等級、或是一個開關）來決定要使用 greeting_prompt_v1 還是 greeting_prompt_v2_formal。

    - 灰度發布 (Canary Release)：當您開發了一個全新的、更強大的 prompt v3.0，您可能不想立刻將它推送給所有使用者。利用 Feature Flag，您可以先將 v3.0 開放給 1% 的內部員工或測試用戶，觀察其表現。如果效果良好，再逐步擴大到 5%、20%，最終全面上線。這個過程能大幅降低因新 prompt 表現不佳而導致的大規模負面影響。

    - 緊急回退 (Kill Switch)：如果在全面上線後，發現新版的 prompt 在某些邊界情況下會產生有害或不當的內容，您可以立即透過 Feature Flag 將其關閉，讓系統瞬間切換回上一個穩定的版本，而不需要緊急進行程式碼的重新部署。

### 1.2 核心 Prompt 技術的應用場景 (Few-shot, CoT, ReAct)
了解了如何管理 prompts 之後，我們來深入探討三種能顯著提升 LLM 表現的核心技術。了解它們各自的適用場景，是成為高級 Prompt Engineer 的關鍵。

#### 1.2.1 Few-shot Learning - 給模型幾個範例，讓它學得更快

- 核心思想：在您的問題之前，提供幾個完整的「問題-答案」範例。這就像是給模型一個小型的「速成班」，讓它在回答您真正的問題之前，先理解您期望的格式、風格和邏輯。

- 應用場景：

    - 格式化輸出：當您需要模型以特定的格式（如 JSON、Markdown 表格、特定 XML 結構）回覆時，提供幾個範例是最高效的方式。

    - 特定風格的文本生成：例如，要求模型撰寫一封正式的商業郵件。您可以先提供一兩個符合公司風格的範例郵件，模型就能更好地模仿該語氣和用詞。

    - 複雜分類任務：假設您要將用戶評論分為「正面」、「負面」、「中性」和「建議」。您可以給出幾個典型評論及其分類的範例，幫助模型更精準地進行判斷。

    - Zero-shot vs. Few-shot：Zero-shot (零樣本) 是指不給任何範例，直接提出問題。當任務非常簡單或模型已經非常熟悉時，可以使用。但只要任務稍微複雜或需要特定格式，Few-shot 的效果通常會遠勝於 Zero-shot。

#### 1.2.2 Chain of Thought (CoT) - 引導模型「思考」而不是「猜測」

- 核心思想：不僅僅給模型答案，而是向模型展示「如何一步步推導出答案」的思考過程。這會激發模型內部的推理能力，使其在處理複雜問題時，表現得更像一個真正的思考者。

- 應用場景：

    - 數學與邏輯推理：這是 CoT 最經典的應用。例如，在解決一個數學應用題時，您會先列出已知條件，然後寫下計算步驟，最後得出答案。將這個過程展示給模型，它解決類似問題的準確率會大幅提升。

    - 多步驟指令的任務規劃：例如，「幫我規劃一個週末去陽明山的行程，需要包含午餐地點，並且考慮到下午可能會下雨。」使用 CoT，您可以引導模型先思考：1. 選擇陽明山景點。2. 查詢天氣預報。3. 根據天氣推薦上午的戶外景點和下午的室內備案。4. 推薦午餐地點。

    - 根本原因分析 (Root Cause Analysis)：當面對一個複雜的問題（例如：「為什麼我的電商網站上個月的轉換率下降了？」），您可以透過 CoT 引導模型從數據、市場、產品等多個角度進行逐步分析，而不是直接給出一個猜測性的答案。

#### 1.2.3 ReAct (Reasoning and Acting) - 讓模型成為一個會使用工具的智慧體

- 核心思想：結合了 CoT 的「思考」(Reasoning) 和與外部工具「互動」(Acting) 的能力。這讓 LLM 不再只是一個封閉的知識庫，而是一個可以主動獲取最新資訊、執行程式碼、與 API 互動的智慧代理 (Agent)。

- 運作流程：思考 (Thought) -> 行動 (Action) -> 觀察 (Observation) -> 思考 (Thought) ...

- 應用場景：

    - 需要即時資訊的問答：例如，「Google 最近的股價是多少？並分析一下近期的趨勢。」模型會 思考 到需要查詢股價，於是 行動 (呼叫一個股票查詢 API)，得到 觀察 (股價數據)，然後再 思考 如何基於這些數據進行分析。

    - 與外部服務互動：建立一個能幫您預訂餐廳、查詢訂單狀態或是在您的行事曆上新增活動的 AI 助理。模型會根據您的指令，思考需要呼叫哪個 API (行動)，並根據 API 的回傳結果 (觀察) 進行下一步。

    - 自動化研究與報告生成：給定一個主題，ReAct 框架可以讓模型主動上網搜尋資料 (行動)，閱讀並總結搜尋結果 (觀察 & 思考)，最終生成一份結構完整的報告。

> 💡 教學提示：「當您在設計這些不同策略的 Prompt 時，最佳實踐是將它們作為不同的版本保存在您的 Prompt 管理平台（如 LangFuse）中。例如，您可以有一個 baseline 版本、一個 few-shot 版本和一個 CoT 版本，這樣便於後續進行系統化的測試和比較。」

### 1.3 Prompt 測試與 A/B 比較：從實驗室到真實世界的雙層次評估法
一個好的 Prompt 不是憑感覺想出來的，而是需要經過科學驗證。這個驗證過程就像新藥上市，需要經過兩個核心階段：實驗室裡的嚴格品管，和真實世界裡的臨床試驗。

#### 1.3.1 層次一：上線前的「品質檢驗」(The "Lab Coat" Phase)

在這個階段，我們像科學家一樣在控制環境中做測試。我們的目標不是看它能不能賺錢，而是回答一個更基本的問題：「這個 Prompt 安全、穩定、正確嗎？」

- 由誰主導？ 主要由工程師和 AI 團隊主導。

- 測試方法：黃金測試集 (Golden Set)

    - 我們會準備一個包含數十到數百個「考題」的測試集 (e.g., 不同的產品名稱、用戶問題)。

    - 用這個測試集去執行我們的 Prompt 新版本，確保它不會產生有害內容、不會跑出奇怪的格式、並且答案品質在一個基本水準之上。

- 評估指標： 這時我們關注的是客觀指標和主觀指標。

    - 格式符合率：輸出的 JSON 是否總是合法？

    - 安全性：是否會生成不當言論？

    - 準確率：對於某些任務，回答是否正確？

- 核心工具： 這就是 LangFuse, Pezzo, LangSmith 這類 LLMOps 平台發揮巨大價值的地方。它們能幫助我們管理黃金測試集，自動化地運行測試，並記錄下每個 Prompt 版本在這些品質指標上的表現（例如成本、延遲、評分）。

一句話總結： 在這個階段，我們淘汰掉那些「不及格」的 Prompt，篩選出幾個品質優良的「候選版本」進入下一輪。

#### 1.3.2 層次二：上線後的「市場驗證」(The "In the Wild" Phase)

當一個 Prompt 通過了品質檢驗，證明它是一個「好」的 Prompt 後，我們還需要將它放到真實的市場中去驗證，回答一個商業問題：「這個 Prompt 能為我們帶來商業價值嗎？」

- 由誰主導？ 主要由產品經理和行銷團隊主導。

- 測試方法：A/B 測試 (A/B Testing)

    - 我們將通過品質檢驗的「候選版本」（例如，舊的穩定版 vs. 新的優化版）同時推送給線上用戶。

    - 透過隨機分流，讓一部分用戶看到 A 版本，另一部分看到 B 版本。

- 評估指標： 這時我們關注的是業務指標。

    - 轉換率：哪個版本的文案帶來了更多的銷售？

    - 用戶滿意度：哪個版本的客服機器人收到了更多的好評？

- 核心工具組合：

    1. 功能旗標服務 (LaunchDarkly/Flagsmith)：用它強大的後台來設定流量分配規則（例如 50% vs 50%），並可以隨時調整或關閉實驗，而不需要工程師修改程式碼。

    2. 數據分析工具 (Google Analytics, Mixpanel 等)：用來衡量和比較不同組別在業務指標上的表現。

一句話總結： 在這個階段，我們用真實的市場數據，從幾個「候選版本」中找出最終的「冠軍版本」。

*實作 Lab：模擬一個從開發、品管到線上測試的完整工作流程*
現在，讓我們透過為「Aura 智慧香氛機」設計行銷文案這個場景，來走一遍上面提到的完整流程。

*情境*
我們的目標是為新產品找到最佳的社群媒體文案。團隊已經腦力激盪出三版 Prompts，現在需要透過科學的方法來決定最終要用哪一版。

步驟 1：開發與版本定義 (Development & Version Definition)

這一步對應的是 Prompt 的開發階段。在真實世界中，這三版 Prompt 會被儲存在 LangFuse 或 Pezzo 中進行集中管理，並賦予明確的版本號。

1. v1_baseline (基礎版)

- Prompt: "為「Aura 智慧香氛機」寫一段社群媒體的行銷文案。"

- 目的：這是一個最簡單、最直接的 prompt，作為我們比較的基準。

2. v2_few_shot (範例學習版)

- Prompt:

為產品寫一段吸引人的社群媒體行銷文案。

```
範例1：
產品：SleekPods 無線藍牙耳機
文案：🎶 告別纏線，擁抱自由！SleekPods 帶來沉浸式音質與全天候舒適配戴感。#無線自由 #音樂隨行

範例2：
產品：Zenith 手沖咖啡壺
文案：☕️ 每個早晨，都值得一杯完美的咖啡。Zenith 手沖咖啡壺，精準溫控，釋放咖啡豆的極致風味。#手沖咖啡 #生活美學

現在，請為「Aura 智慧香氛機」寫一段社群媒體的行銷文案。
```

- 目的：使用 Few-shot learning，提供範例來引導模型生成包含 Emoji 和 Hashtag 的、更符合社群媒體風格的文案。

3. v3_optimized (優化版)

- Prompt:

```
你是一位頂尖的社群行銷專家。你的任務是為新產品「Aura 智慧香氛機」撰寫一段引人入勝、不超過150字的 Instagram 貼文。

**產品核心賣點**:
1.  可透過 App 遠端遙控，並設定排程。
2.  能與智慧家庭系統（如 Google Home）連動。
3.  使用天然植物精油，安靜運作。

**寫作要求**:
- 開頭要能立刻抓住眼球。
- 文案中需自然地融入至少兩個核心賣點。
- 語氣需溫暖、有質感，能喚起放鬆和科技感。
- 結尾必須包含一個行動呼籲 (Call to Action)。
- 最後附上 3-4 個相關的 Hashtag。
```

- 目的：這是一個更精緻的 prompt，明確定義了角色 (Persona)、上下文 (Context)、任務細節 (Task Specifics) 和輸出格式 (Output Format)，期望能生成質量最高的文案。

步驟 2：(模擬) 品質檢驗 (Simulated QA)
在真實場景中，工程師會用一個包含 50 個不同產品名稱的「黃金測試集」來測試這三版 Prompt。他們可能會得出以下結論：

- v1_baseline：輸出品質普通，但格式非常穩定。(通過品管)

- v2_few_shot：風格討喜，但偶爾會模仿範例到忘記產品本身，導致文案不相關。(品管不通過)

- v3_optimized：輸出品質高，且嚴格遵循所有寫作要求，表現最穩定。(通過品管)

基於品管結果，團隊決定淘汰 v2，讓表現穩定的 v1 和 v3 進入下一階段的真人 A/B 測試，來看看用戶到底更喜歡哪一種。

步驟 3：用 Feature Flag 實現線上 A/B 測試 (Implementation)

我們將使用 Python 偽代碼來模擬這個過程。

```Python
import random

# --- 步驟 1: 在 Prompt 管理平台 (如 LangFuse/Pezzo) 中定義好的 Prompts ---
# 在真實應用中，這些內容是透過 API 從 LangFuse/Pezzo 獲取的，而不是寫死在本地。
PROMPT_REPOSITORY = {
    "v1_baseline": "為「Aura 智慧香氛機」寫一段社群媒體的行銷文案。",
    "v3_optimized": """你是一位頂尖的社群行銷專家... (省略如上) ..."""
}

# --- 步驟 2: 連接 Feature Flag 服務 (如 LaunchDarkly) 來決定使用哪個版本 ---
# 產品經理可以在 LaunchDarkly 的儀表板上設定 A/B 測試，例如各分配 50% 的流量。
def get_prompt_version_from_feature_flag(user_id):
    """
    模擬從 LaunchDarkly 獲取決策。
    這個函式決定了特定用戶應該看到哪個實驗版本。
    """
    # 簡單的 50/50 分流邏輯
    if user_id % 2 == 0:
        return "v3_optimized"  # 偶數 ID 的用戶看到優化版
    else:
        return "v1_baseline"   # 奇數 ID 的用戶看到基礎版

# --- 應用程式主邏輯 ---
def generate_product_copy(product_name, user_id):
    """
    一個完整的請求流程：
    1. 詢問 Feature Flag 我該用哪個版本。
    2. 根據版本號去 Prompt 平台拿對應的 Prompt。
    3. 執行 Prompt 並返回結果。
    """
    # 1. 從 LaunchDarkly 獲取版本決策
    version_to_use = get_prompt_version_from_feature_flag(user_id)
    
    # 2. 從 LangFuse/Pezzo 獲取對應的 Prompt 內容
    prompt_template = PROMPT_REPOSITORY.get(version_to_use)
    
    print(f"--- User {user_id} 正在參與 A/B 測試，分配到的版本是: {version_to_use} ---")
    
    # 3. (模擬) 呼叫 LLM API
    # response = call_llm_api(prompt_template.format(product_name=product_name))
    # print(response)
    
    return f"用戶看到的文案將由 {version_to_use} 版本生成。"


# --- 模擬兩個不同用戶的請求 ---
user_a = 101  # 奇數 ID
user_b = 202  # 偶數 ID

print("處理 User A 的請求:")
generate_product_copy("Aura 智慧香氛機", user_a)

print("\n" + "="*50 + "\n")

print("處理 User B 的請求:")
generate_product_copy("Aura 智慧香氛機", user_b)

```

Lab 總結與討論

完成這個 Lab 後，請思考：

1. 我們為什麼在 A/B 測試前要先進行「品質檢驗」？如果直接把 v2_few_shot 版本也加入 A/B 測試，可能會帶來什麼風險？

2. 在 A/B 測試進行中，產品經理發現 v3_optimized 版本的轉換率遠超預期。她應該如何使用 Feature Flag 系統來結束實驗，並將 100% 的流量都切換到 v3 版本？這個過程需要工程師介入嗎？

3. 請描述在這個完整的流程中，LangFuse (或 Pezzo) 和 LaunchDarkly 是如何協同工作的？它們分別解決了哪個階段的核心問題？

---

## 📘 模組 2：LLM API 與 UX 整合（第 2 週）

### 🎯 學習目標
本模組將帶領學員跨越單純的 Prompt Engineering，進入應用開發的實務層面。學員將學習如何根據不同的使用者情境，選擇最適合的 API 呼叫模式，並設計出符合使用者直覺的對話體驗。最後，我們將探討如何建立一道「信任層」，確保從 LLM 獲得的輸出是可控、可靠且能被後端系統使用的。

### 📚 核心內容

### **2.1 LLM API 呼叫模式 (API Call Patterns)**

**核心問題：** 我們該如何與 LLM API 溝通？不同的溝通方式會如何影響應⽤的效能和使⽤者體驗？

#### **2.1.1 Completion 模式（一次性完成模式）**

- **定義：** 這是最基礎的互動模式。你送出一個完整的提示 (Prompt)，API 回傳一個完整的答案。這是一種「一次性請求-回應」的無狀態 (Stateless) 互動。
    
- **比喻：** 就像寫一封 Email，你把所有問題寫完寄出，然後等待對方完整的回覆。
    
- **優點：**
    
    - 實作簡單、直觀。
        
    - 適合不需要上下文記憶的單次任務。
        
- **適用情境：**
    
    - **內容生成：** 寫一篇文章、產生一首詩、草擬一封郵件。
        
    - **文本摘要：** 給定一篇文章，生成其摘要。
        
    - **簡單問答：** 「法國的首都是哪裡？」
        
- **範例程式碼 (概念)：**
       
    ```python
    response = llm.complete(
        prompt="請為一家新開的咖啡店想三個有創意的名字。"
    )
    print(response.text)
    # >> 1. 啡語流光 2. 城市焙思 3. 滴答手沖
    ```
    

#### **2.1.2 Chat 模式（對話模式）**

- **定義：** 專為多輪對話設計的模式。API 不只接收當前的問題，還會接收過去的對話歷史紀錄。這是一種有狀態 (Stateful) 的互動。
    
- **比喻：** 就像使用 LINE 或 Messenger，每一句話都建立在前文的基礎上，形成連續的對話。
    
- **關鍵概念：**
    
    - **角色 (Roles)：** 通常包含 `system` (設定 AI 的行為準則)、`user` (使用者輸入)、`assistant` (AI 的回覆)。
        
    - **對話歷史 (History)：** 每次請求都必須附上之前的對話內容，AI 才能「記住」上下文。
        
- **優點：**
    
    - 能處理複雜、有上下文關聯的任務。
        
    - 提供更自然、更具互動性的使用者體驗。
        
- **適用情境：**
    
    - **聊天機器人 (Chatbot)：** 客服、個人助理。
        
    - **逐步引導任務：** 「幫我規劃一趟去東京的五日遊。」 -> 「第一天想去哪些地方？」
        
    - **程式碼協作：** 「寫一個 Python 函式...」 -> 「不錯，現在幫我加上錯誤處理。」
        
- **範例程式碼 (概念)：**
    
    ```python
    messages = [
        {"role": "system", "content": "你是一位友善的旅遊規劃助理。"},
        {"role": "user", "content": "我想去一個溫暖的海島國家，有推薦嗎？"},
        {"role": "assistant", "content": "當然！泰國的普吉島和印尼的峇里島都是很棒的選擇，您對哪個比較感興趣呢？"},
        {"role": "user", "content": "那先介紹一下峇里島吧。"}
    ]
    response = llm.chat(messages=messages)
    print(response.text)
    ```
    

#### **2.1.3 Streaming 模式（串流模式）**

- **定義：** API 不會等所有內容生成完畢才一次性回傳，而是以「字詞」或「Token」為單位，即時、逐字地將結果串流回客戶端。
    
- **比喻：** 像是觀看 YouTube 直播，畫面是即時傳送過來的，而不是等整部影片下載完才播放。也像看著真人在打字，一個字一個字出現。
    
- **優點：**
    
    - **大幅降低感知延遲 (Perceived Latency)：** 使用者幾乎立刻就能看到回應，避免了漫長的等待焦慮。
        
    - **提升互動感：** 感覺更像在與一個「正在思考並回應」的對象交流。
        
- **適用情境：**
    
    - 所有需要即時互動的 Chat 應用。
        
    - 生成長文或程式碼時，讓使用者能先看到開頭部分。
        
- **挑戰：** 前端需要實作對應的邏輯來接收和拼接這些串流的字塊。
    

|模式|核心概念|使用者體驗|適用場景|
|---|---|---|---|
|**Completion**|一問一答，無記憶|快速直接，但生硬|內容總結、文案生成|
|**Chat**|多輪對話，有記憶|自然流暢，具互動性|客服、虛擬助理、教學|
|**Streaming**|即時逐字回傳|低延遲、高互動感|所有對話式應用|

---

### **2.2 三種對話 UX 模式 (Conversational UX Patterns)**

**核心問題：** 我們該如何設計對話的「流程」，讓使用者覺得 AI 是聰明且有幫助的？

#### **2.2.1 Command 模式（命令模式）**

- **定義：** 使用者給出明確、具體的指令，期望 AI 直接執行並給出結果。
    
- **互動心態：** 使用者把 AI 當作一個「高效率的工具」。
    
- **設計重點：**
    
    - **意圖辨識要精準：** AI 必須準確理解使用者的指令。
        
    - **回應要簡潔：** 直接給出答案或確認執行的結果，不多廢話。
        
- **範例：**
    
    - 使用者：「幫我設定明天早上 7 點的鬧鐘。」 -> AI：「好的，已設定明天早上 7 點的鬧鐘。」
        
    - 使用者：「查詢訂單 `A12345678` 的狀態。」 -> AI：「訂單 `A12345678` 目前狀態為：已出貨。」
        

#### **2.2.2 Clarification 模式（澄清模式）**

- **定義：** 當使用者的指令模糊或資訊不足時，AI 主動提出問題，以澄清意圖、補足資訊。
    
- **互動心態：** 使用者把 AI 當作一個「細心的夥伴」。
    
- **設計重點：**
    
    - **識別模糊地帶：** 知道何時該問問題，而不是猜測或直接拒絕。
        
    - **提問要聰明：** 提出的問題應該是關鍵性的，最好提供選項。
        
- **範例：**
    
    - 使用者：「幫我訂一張去東京的機票。」
        
    - AI (澄清)：「好的，請問您預計的出發日期是什麼時候呢？」
        
    - 使用者：「我想退貨。」
        
    - AI (澄清)：「沒問題，請問您想退貨的是哪一筆訂單呢？可以提供訂單編號嗎？」
        

#### **2.2.3 Suggestion 模式（建議模式）**

- **定義：** AI 在完成使用者的指令後，基於上下文，主動提供相關的、可能有幫助的建議或下一步操作。
    
- **互動心態：** 使用者把 AI 當作一個「有遠見的專家」。
    
- **設計重點：**
    
    - **建議要相關：** 提供的建議必須與當前對話高度相關。
        
    - **時機要恰當：** 在使用者完成一個主要任務節點後提出，避免打斷流程。
        
    - **不要強迫：** 以提議的口吻呈現，讓使用者可以輕鬆接受或拒絕。
        
- **範例：**
    
    - AI：「您的訂單 `A12345678` 已成功取消並辦理退款。」
        
    - AI (建議)：「很高興為您服務。順帶一提，您上次購買的《原子習慣》同系列作者有本新書《高效人士的七個習慣》，需要為您介紹嗎？」
        
    - 使用者：「幫我查一下台北今天的天氣。」
        
    - AI：「台北今天天氣晴朗，氣溫 28-34 度。」
        
    - AI (建議)：「天氣炎熱，提醒您多補充水分。需要幫您查詢附近的飲料店嗎？」
        

---

### **2.3 API 輸出信任層 (API Output Trust Layer)**

**核心問題：** LLM 的輸出是不確定的，有時甚至是「胡說八道」或格式錯誤。如何確保我們從 API 得到的結果是結構化、可靠、且能被程式其他部分安全使用的？

這一層是介於「LLM 原始輸出」和「你的應用程式邏輯」之間的**防護網**和**格式轉換器**。

#### **2.3.1 正則表達式 (Regular Expressions)**

- **用途：** 當你需要從 LLM 回傳的一段自然語言文字中，**抽取**出特定格式的簡單資訊時。
    
- **範例：**
    
    - **任務：** 從「我的訂單編號是 `ORD-2025-08-23-001`，請幫我查詢。」中提取訂單編號。
        
    - **作法：** 使用正則表達式 `ORD-\d{4}-\d{2}-\d{2}-\d{3}` 來匹配並抽取出 `ORD-2025-08-23-001`。
        
- **優點：** 快速、輕量，適合處理格式固定的簡單字串。
    
- **缺點：** 對於複雜的巢狀結構（如 JSON）無能為力，且規則寫起來較為繁瑣。
    

#### **2.3.2 Schema Validation (結構化驗證)**

- **用途：** 當你要求 LLM 輸出一種**複雜的、結構化的資料**（最常見的是 JSON），用以驗證其格式是否完全符合預期。
    
- **核心流程：**
    
    1. **定義 Schema：** 在 Prompt 中，明確指示 LLM 必須以 JSON 格式回覆，並提供 JSON 的結構範例。
        
    2. **LLM 輸出：** LLM 嘗試生成符合要求的 JSON 字串。
        
    3. **解析與驗證：** 你的程式碼先嘗試將 LLM 回傳的字串 `JSON.parse()`。成功後，再用一個預先定義好的 Schema（例如 Pydantic、JSON Schema）去驗證這個 JSON 物件的欄位、型別是否都正確。
        
    4. **重試機制：** 如果驗證失敗，可以將錯誤訊息連同原始請求再次發送給 LLM，要求它「修正錯誤並重新輸出」。
        
- **範例：**
    
    - **任務：** 從「幫我預約一位叫 John Doe 的客戶，電話是 0912-345-678，時間是明天下午 3 點」中提取預約資訊。
        
    - **Schema (Pydantic)：**
        
        ```python
        from pydantic import BaseModel
        class Appointment(BaseModel):
            name: str
            phone: str
            time: str
        ```
        
    - **驗證：** 程式會檢查 LLM 的輸出是否包含 `name`, `phone`, `time` 三個欄位，且其值是否為字串。
        
- **進階概念：** 現代的 LLM API（如 OpenAI 的 Function Calling）已經內建了類似的功能，能更可靠地強制 LLM 輸出符合特定結構的 JSON，這是 Schema Validation 的更穩健實踐。
    

---

### **Lab：做一個客服 Bot，測試三種 UX 模式**

**目標：** 綜合運用本模組所學，為一個虛構的電商平台「智慧小舖」打造一個多功能客服 Bot。

**場景設定：**

1. **Command 模式實作：查詢訂單**
    
    - **使用者輸入：** 「幫我查訂單 `S20250823001`」
        
    - **Bot 任務：**
        
        1. 使用**正則表達式**從使用者輸入中準確提取訂單編號。
            
        2. 呼叫一個假的後端函式 `getOrderStatus(orderId)`。
            
        3. 回傳訂單狀態，例如：「您好，訂單 `S20250823001` 的狀態是【已出貨】，預計 2 天內送達。」
            
    - **學習重點：** 指令理解、資訊抽取 (正則)。
        
2. **Clarification 模式實作：申請退貨**
    
    - **使用者輸入：** 「我想要退貨。」
        
    - **Bot 任務：**
        
        1. 辨識到使用者的意圖是「退貨」，但缺少關鍵資訊「訂單編號」。
            
        2. 進入 **Chat 模式**，發起澄清式提問：「好的，請問您要申請退貨的訂單編號是？」。
            
        3. 使用者提供編號後，Bot 再繼續後續流程。
            
    - **學習重點：** 模糊意圖處理、多輪對話 (Chat 模式)、狀態管理。
        
3. **Suggestion 模式實作：智慧推薦**
    
    - **情境接續：** 在使用者成功查詢完一筆「已完成」的訂單後（例如訂單內容是購買了一本《AI 技術的未來》）。
        
    - **Bot 任務：**
        
        1. 完成主要任務：「您的訂單 `S20250823001` 已於 8 月 20 日順利送達。」
            
        2. 觸發建議模式：「看到您對 AI 主題感興趣，我們最近剛上架一本新書《大型語言模型入門與實踐》，許多讀者反應熱烈，需要為您加入購物車嗎？」
            
    - **學習重點：** 上下文感知、主動服務、提升使用者價值。
        

**進階挑戰 (Bonus)：**

- 將 Bot 的所有回覆都改為 **Streaming 模式**，讓使用者能即時看到 Bot 的打字效果，提升互動體驗。
    
- 在處理使用者退貨請求時，要求 LLM 將退貨原因和訂單編號整理成一個固定的 JSON 格式，並用 **Schema Validation** 進行驗證，驗證通過後才模擬呼叫後端 API。

---

## 📘 模組 3：Context Engineering 必修基礎（第 3 週）

### 🎯 學習目標
在本單元中，我們將深入探討大型語言模型（LLM）的「大腦」是如何運作的——也就是 **Context（上下文）**。Context 是我們與 LLM 溝通時提供給它的所有資訊，它直接決定了模型的回應品質。學會如何設計與管理 Context，是打造一個聰明、可靠 AI 應用的基礎。我們將學習如何區分不同類型的 Prompt，理解模型的記憶限制，並掌握管理長期與短期記憶的關鍵策略。

### 📚 核心內容

#### **3.1 System Prompt vs User Prompt vs Memory**

要讓 LLM 表現得像個專家，我們必須學會扮演好「導演」的角色。提供給模型的 Context 主要由這三部分組成，它們各自扮演著不同的角色：

- **System Prompt（系統提示詞）：AI 的「角色設定」與「核心指令」**
    
    - **定義**：這是我們在對話開始前，預先給予 LLM 的最高層級指令。它定義了 AI 的 **人格、角色、行為準則、技能、以及最終目標**。你可以把它想像成是為一位演員寫的「角色劇本」，或者是一個程式的「核心設定檔」。
        
    - **作用**：
        
        - **設定人設**：例如，「你是一位資深的天文學家，請用淺顯易懂的方式回答問題。」
            
        - **規範行為**：例如，「你的回答必須保持客觀中立，絕不帶有個人情感。」、「你的回答長度不能超過 200 字。」
            
        - **提供專有知識**：可以放入特定的背景資料或指南，讓 AI 參考。
            
    - **重要性**：一個好的 System Prompt 能讓模型在整個對話中保持一致性與專業度，是決定 AI 應用成敗的關鍵第一步。它就像船的舵，決定了航行的方向。
        
- **User Prompt（使用者提示詞）：當下的「對話」與「任務」**
    
    - **定義**：這是使用者在每一次互動中，輸入的具體問題或指令。它代表了使用者當前的意圖。
        
    - **作用**：驅動 AI 針對特定問題產生回應。例如，「請解釋什麼是黑洞？」或「幫我把這段文字翻譯成英文。」
        
    - **與 System Prompt 的關係**：User Prompt 是演員（AI）在舞台上接到的「即時指令」。AI 會根據它的核心角色設定（System Prompt）來回應這個即時指令。如果 System Prompt 設定 AI 是天文學家，那麼它回答「什麼是黑洞」時，就會使用專業但易懂的口吻。
        
- **Memory（記憶）：對話的「歷史紀錄」**
    
    - **定義**：LLM 本身是「無狀態（Stateless）」的，它不會自動記得之前的對話。為了讓對話能夠持續，我們必須在每一次發送 User Prompt 時，**手動附上** 過去的對話紀錄。這段歷史紀錄就是所謂的「記憶」。
        
    - **作用**：
        
        - **維持對話連貫性**：讓 AI 知道「我們剛剛聊到哪裡」，能夠理解「他」、「那個」等代名詞的指涉對象。
            
        - **避免重複資訊**：AI 知道使用者已經提供過的資訊，不會重複詢問。
            
    - **運作方式**：最簡單的記憶模式，就是將 `[歷史對話紀錄] + [最新的 User Prompt]` 一起打包，作為完整的 Context 傳送給模型。
        

#### **3.2 Context 長度限制 & 壓縮策略**

模型的記憶力不是無限的，這是在開發應用時必須面對的現實。

- **Context 長度限制（Context Window）**
    
    - **定義**：每個 LLM 都有一個最大的 Context 處理長度，稱為「Context Window」或「Context Length」，通常以 **Token** 為單位計算（一個 Token 約等於幾個英文字母或一個中文字）。常見的長度有 4K, 8K, 32K, 甚至 128K Token。
        
    - **問題**：當整個對話（包含 System Prompt、歷史紀錄、最新問題）的總 Token 數超過這個上限時，模型就必須「忘掉」一部分資訊。最常見的情況是，它會忘掉 **最早的對話內容**，導致「失憶」。
        
    - **比喻**：這就像一個人的短期記憶容量有限，當你不斷告訴他新資訊時，他會漸漸忘記一開始聽到的內容。
        
- **壓縮策略：如何在有限的記憶中塞入更多智慧**
    
    - 當對話越來越長，我們不能無限制地把所有歷史紀錄都塞進去。這時就需要「壓縮」策略，來保留最重要的資訊。
        
    - **Summarization（摘要法）**：
        
        - **方法**：當對話歷史快要超過長度限制時，呼叫 LLM 另一個獨立任務，要求它「總結」目前為止的對話重點。
            
        - **實作**：將 `[舊的對話摘要] + [最近幾輪的對話原文] + [最新的 User Prompt]` 組合成新的 Context。這樣既保留了長期的對話精華，又確保了近期的對話細節沒有遺失。這是最常用且效果最好的策略之一。
            
    - **Sliding Window（滑動窗口法）**：
        
        - **方法**：只保留最近的 N 輪對話。這是一種簡單粗暴的方法。
            
        - **優點**：實作簡單，計算成本低。
            
        - **缺點**：會完全遺忘早期的重要資訊，例如使用者一開始設定的目標。
            
    - **Filtering（過濾法）**：
        
        - **方法**：保留 System Prompt 和使用者最重要的幾輪對話，捨棄中間的閒聊或不重要的互動。
            
        - **缺點**：需要額外的邏輯來判斷哪些對話是「重要」的。
            

#### **3.3 短期 / 長期記憶的管理模式**

一個好的 AI 應用，不僅要能記得「剛剛說了什麼」，還要能記得「很久以前的偏好」。

- **短期記憶（Short-term Memory）**
    
    - **定義**：指在 **單次連續對話** 中，AI 能夠記住的內容。
        
    - **管理**：主要就是透過我們前面提到的 **Context 壓縮策略** 來管理。目標是在不超過 Context Window 的前提下，盡可能保留對當前對話有用的資訊。Summarization 是管理短期記憶的黃金標準。
        
- **長期記憶（Long-term Memory）**
    
    - **定義**：指 AI 能夠 **跨越不同對話、不同時間** 記住的資訊。例如，記住你的名字、偏好的語言風格、或你公司的業務範圍。
        
    - **挑戰**：LLM 的 Context Window 是為短期記憶設計的，無法直接實現長期記憶。
        
    - **管理模式（RAG - Retrieval-Augmented Generation）**：
        
        1. **外部儲存**：將需要長期記住的資訊（例如，使用者個人資料、產品文件、歷史對札記）儲存在一個外部的資料庫，最常見的就是 **向量資料庫（Vector Database）**。
            
        2. **資訊檢索**：當使用者提出新問題時，系統先根據問題的關鍵字，去向量資料庫中「搜尋」最相關的幾筆長期記憶。
            
        3. **動態注入**：將搜尋到的「相關長期記憶」片段，動態地注入到當前的 System Prompt 或 Context 中，再一起傳送給 LLM。
            
    - **比喻**：這就像 AI 在回答你問題前，會先去旁邊的「圖書館」（向量資料庫）快速查閱最相關的幾頁資料，然後再根據這些資料來回答你。
        

#### **Lab：用 Summarization 管理多輪對話**

實驗目標：

親手打造一個不會輕易「失憶」的聊天機器人。我們將實作一個自動摘要機制，讓它在對話變長時，能自動總結之前的內容，以維持長時間的連貫對話。

**實驗步驟**：

1. **設定場景**：
    
    - 建立一個基本的對話迴圈，接收使用者輸入。
        
    - 設定一個較小的 Context 上限，例如 1000 Token，以利於快速觸發摘要機制。
        
    - 設計一個 System Prompt，例如：「你是一位樂於助人的 AI 助理。」
        
2. **對話歷史管理**：
    
    - 用一個列表（List/Array）來儲存每一輪的對話（使用者說的、AI 回的）。
        
3. **觸發摘要的時機**：
    
    - 在每一次發送請求給 LLM **之前**，先計算目前對話歷史的總 Token 數。
        
    - **判斷**：如果 `總 Token 數 > 上限 * 0.8`（設定一個緩衝區，例如 80%），則啟動摘要程序。
        
4. **執行摘要**：
    
    - **呼叫 LLM**：發起一個獨立的 API 請求給 LLM。
        
    - **摘要專用 Prompt**：給予 LLM 一個專門用於摘要的指令，例如：`「請將以下對話紀錄濃縮成一段精簡的摘要，保留關鍵的人物、事件與結論：[將前 80% 的對話歷史貼在這裡]」`
        
    - **取得摘要結果**：接收 LLM 回傳的摘要文字。
        
5. **重建 Context**：
    
    - **清空舊歷史**：清空儲存對話歷史的列表。
        
    - **建立新歷史**：將 `[摘要結果]` 和 `[保留的後 20% 對話歷史]` 重新放入列表中。
        
    - 現在，你的對話歷史變得既簡短又保留了精華！
        
6. **繼續對話**：
    
    - 將新的、經過壓縮的對話歷史，連同使用者最新的問題，一起發送給 LLM，取得回應。
        
    - 重複步驟 2-6。
        

預期成果：

學員將能觀察到，即使進行了 20、30 輪以上的對話，AI 依然能大致記得一開始的對話內容，因為關鍵資訊都被保存在不斷更新的摘要中。這將讓他們深刻體會到 Context 管理的強大之處。

---

## 📘 模組 4：知識掛載與 RAG 基礎（第 4 週）

### 🎯 學習目標
在本模組中，學員將學習如何讓大型語言模型 (LLM) 連接外部知識庫，以回答特定領域的問題，克服 LLM 本身的知識限制。我們將從頭開始建構一個基本的 RAG 流程，並特別著重於處理在真實世界中常見的「失敗情境」，確保建立的系統更加穩健可靠。

### 📚 核心內容

#### **4.1 向量檢索 & Chunking 策略（簡化版）**

教學目標：

理解為什麼我們需要將文件「向量化」，以及如何透過基本的「Chunking」策略來處理長文件。

**4.1.1 為什麼需要「知識掛載」？— RAG 的動機**

- **LLM 的天生限制：**
    
    - **知識斷層 (Knowledge Cutoff)：** 就像一本2023年出版的百科全書，LLM 不知道之後發生的新事件或新知識。
        
    - **無法存取私有資料：** LLM 對於您公司的內部文件、個人筆記或任何不在其訓練資料中的內容一無所知。
        
    - **幻覺 (Hallucination)：** 當 LLM 不確定答案時，有時會「一本正經地胡說八道」，編造看似合理卻錯誤的資訊。
        
- **RAG 的核心思想：**
    
    - **給 LLM 一本「開卷考試」的參考書。** 我們不要求 LLM「背誦」所有知識，而是在它回答問題前，先幫它從我們提供的資料庫（參考書）中，找出最相關的幾頁，讓它「參考」著回答。
        
    - 這樣不僅能提供最新、最準確的資訊，還能大幅減少幻覺的發生。
        

**4.1.2 如何讓機器理解文字？— 向量嵌入 (Vector Embeddings)**

- **概念比喻：** 想像一個巨大的圖書館，每本書都有一個獨特的座標。語意上相似的書（例如「科幻小說」和「奇幻文學」）在圖書館中的位置會非常接近，而與「食譜」這本書的距離則會非常遙遠。
    
- **技術解釋：**
    
    - **Embedding Model：** 這是一個特殊的 AI 模型，它的工作就是將文字（單詞、句子、段落）轉換成一串數字，這串數字就是「向量 (Vector)」。
        
    - **語意相似性 (Semantic Similarity)：** 這些向量的神奇之處在於，它們抓住了文字的「意義」。在向量空間中，意義相近的文字，其向量也比較接近。
        
    - **向量檢索：** 當使用者提出一個問題時，我們也將問題轉換成一個向量，然後在我們的「向量資料庫」中，尋找與問題向量「距離最近」的文件向量。這個過程就像在圖書館中找到與你手上紙條描述最相似的書。
        

**4.1.3 如何處理長篇大論？— Chunking 策略**

- **為什麼需要 Chunking？**
    
    - **Context Window 限制：** LLM 一次能處理的文字長度有限（就像人的短期記憶有限），你不能把一本 300 頁的書全部塞給它。
        
    - **精準度：** 將長文切成小塊（Chunks），可以讓檢索到的內容更聚焦、更相關，避免無關資訊的干擾。如果只檢索到相關的那個段落，而不是整份文件，LLM 的回答會更精準。
        
- **簡化的 Chunking 策略介紹：**
    
    - **固定大小 (Fixed-size Chunking)：** 這是最簡單粗暴的方法。例如，設定每個 Chunk 為 500 個字元，並可以設定一些重疊 (Overlap) 的字元，確保語意不會在切割點被完全斷開。
        
        - **優點：** 實作簡單快速。
            
        - **缺點：** 可能會粗暴地將一個完整的句子或段落從中間切斷。
            
    - **（可選，進階提及）基於標點符號的分割：** 例如，以句號、問號或段落換行符作為切割點，更能保留語意的完整性。
        

---

#### **4.2 基本 RAG Pipeline（Retrieval → Augmentation → Generation）**

教學目標：

清晰地拆解 RAG 的三個核心步驟，並理解資訊是如何在其中流動的。

RAG 工作流程圖：

使用者問題 → [1. Retrieval] → 相關文件片段 → [2. Augmentation] → 增強後的 Prompt → [3. Generation (LLM)] → 最終答案

**4.2.1 檢索 (Retrieval)**

- **任務：** 根據使用者的問題，從向量資料庫中找出最相關的 N 個文件片段 (Chunks)。
    
- **流程：**
    
    1. 將使用者問題透過 Embedding Model 轉換為「查詢向量」。
        
    2. 在向量資料庫中進行相似度搜索（例如，餘弦相似度計算）。
        
    3. 返回分數最高的前 K 個文件片段。
        

**4.2.2 增強 (Augmentation)**

- **任務：** 將檢索到的文件片段與原始問題「組合」成一個新的、資訊更豐富的 Prompt。這是整個流程的精髓。
    
- **Prompt 模板範例：**
    
    ```
    請根據以下提供的上下文資訊來回答問題。如果上下文中沒有相關資訊，請回答「根據我所擁有的資料，我找不到相關答案」。
    
    [上下文資訊開始]
    {retrieved_chunk_1}
    ---
    {retrieved_chunk_2}
    ---
    ...
    [上下文資訊結束]
    
    使用者的問題是：
    {user_question}
    ```
    
- **重點：** 這一步驟是「指示」LLM 如何行動的關鍵。我們明確地告訴它，答案的範圍應該被限制在我們提供的「上下文資訊」中。
    

**4.2.3 生成 (Generation)**

- **任務：** 將這個「增強後的 Prompt」發送給 LLM，讓它生成最終的答案。
    
- **結果：** 因為 LLM 拿到的 Prompt 已經包含了它回答問題所需的所有參考資料，所以它能夠：
    
    - 生成基於事實的、準確的回答。
        
    - 引用資料來源（如果 Prompt 設計得當）。
        
    - 在資料不足時，誠實地承認，而不是胡亂編造。
        

---

#### **4.3 RAG 失敗處理**

教學目標：

認識到完美的 RAG 系統並不存在，並學習如何設計策略來應對最常見的三種失敗情境。這部分是從「能用」到「好用」的關鍵。

**情境一：找不到文件 (Low Relevance / No Document Found)**

- **原因：**
    
    - 使用者的問題與知識庫內容完全無關。
        
    - 檢索演算法不夠好，或者相似度分數太低，表示找到的內容可能也是不相關的。
        
- **判斷方法：**
    
    - 設定一個「相似度分數閾值」。如果在檢索步驟中，所有文件片段的分數都低於這個閾值，我們就判定為「找不到相關文件」。
        
- **處理策略：**
    
    - **直接回應：** 不要將低品質的內容傳遞給 LLM。系統應直接回應：「抱歉，在我的知識庫中找不到與『...』相關的資訊。您可以換個方式提問嗎？」
        
    - **禁止幻覺：** 這是避免 LLM 在沒有資料的情況下自由發揮的關鍵第一道防線。
        

**情境二：找到多個答案（可能互相衝突）**

- **原因：**
    
    - 知識庫中存在過時的舊文件和更新後的新文件（例如，公司政策的兩個版本）。
        
    - 不同的文件從不同角度描述同一件事，導致看似矛盾。
        
- **處理策略：**
    
    - **策略 A (簡單呈現)：** 在 Augmentation 階段，將所有檢索到的（可能衝突的）資訊都提供給 LLM，並在 Prompt 中引導它去總結或指出差異。
        
        - **Prompt 範例：** `"請綜合以下多個來源的資訊來回答。如果資訊之間存在衝突，請一併指出。"`
            
        - **優點：** 簡單，讓 LLM 自行處理複雜性。
            
        - **缺點：** 對 LLM 的推理能力要求較高，結果可能不穩定。
            
    - **策略 B (呈現給使用者)：** 系統直接將找到的多個相關片段呈現給使用者，並附上來源，讓使用者自行判斷。
        
        - **回應範例：** `"我找到了以下幾則相關資訊，它們之間可能存在差異，請您參考：\n1. [來源A]: ...\n2. [來源B]: ..."`
            

**情境三：回退到通用 LLM (Fallback to General LLM)**

- **時機：**
    
    - 當觸發了「找不到文件」的狀況後，系統可以提供一個選項。
        
    - 當問題明顯是「閒聊」或「通用知識」類型，而非知識庫內的專業問題時（例如，使用者問 FAQ Bot "今天天氣如何？"）。
        
- **處理策略：**
    
    - **設計一個決策層：** 在 RAG 流程之前或之後，判斷是否需要繞過 RAG。
        
    - **明確告知使用者：** 當系統決定使用通用 LLM 回答時，必須讓使用者知道。
        
        - **回應範例：** `"這個問題超出了我的專業知識庫範圍。不過，根據我的通用知識，我可以告訴您... 這樣的回答可以嗎？"`
            
    - **為什麼要告知？** 為了管理使用者的期望。使用者需要知道現在的答案是來自於「可信的內部文件」還是「LLM 的通用訓練資料」。
        

---

#### **Lab：做一個 FAQ Bot，模擬查不到答案 / 答案衝突**

Lab 核心目標：

親手搭建一個迷你的 RAG Pipeline，並在其中實作對「找不到答案」和「答案衝突」這兩種失敗情境的處理邏輯。

**1. 準備知識庫 (一個 `faq.txt` 文件)**

- **創建衝突內容：**
    
    ```
    Q: 退貨期限是多久？
    A: 我們的標準退貨期限是購買後的 7 天內。 (來源: 舊版政策)
    ...
    Q: VIP 會員的退貨政策是什麼？
    A: VIP 會員享有延長至 14 天的退貨期限。 (來源: 會員福利)
    ```
    
- **創建明確答案：**
    
    ```
    Q: 如何聯繫客服？
    A: 您可以在上班時間撥打 0800-123-456 聯繫我們的客服團隊。
    ```
    
- **故意留白：** 文件中不要包含任何關於「運費」或「海外配送」的資訊。
    

**2. Lab 步驟**

1. **環境設定：** 安裝必要的函式庫 (如 `langchain`, `transformers`, `faiss-cpu`, `openai` 等)。
    
2. **載入與切割文件 (Load & Chunk)：**
    
    - 讀取 `faq.txt` 文件。
        
    - 使用 `RecursiveCharacterTextSplitter` 或類似工具，將文件切割成小的 Chunks。
        
3. **向量化與儲存 (Embed & Store)：**
    
    - 選擇一個 Embedding 模型 (例如 `HuggingFaceEmbeddings`)。
        
    - 將 Chunks 轉換為向量，並儲存在一個本地的向量資料庫中 (例如 `FAISS`)。
        
4. **建立檢索器 (Retriever)：**
    
    - 設定檢索器，讓它可以執行相似度搜索，並設定 `k=2`，表示一次找回最相關的 2 個文件。
        
5. **核心邏輯：實作失敗處理**
    
    - **主函式 `ask_faq_bot(query)`：**
        
        - 接收使用者問題 `query`。
            
        - **執行檢索：** 使用檢索器不僅獲取文件 (documents)，還要獲取它們的相似度分數 (scores)。`retriever.get_relevant_documents_with_scores(query)`
            
        - **情境判斷：**
            
            - **IF** `scores` 為空，或 `scores[0]` 低於你設定的閾值 (例如 0.7)，**THEN** return `"抱歉，我找不到相關資訊。"` (模擬找不到答案)
                
            - **ELSE** 進入 RAG 的 Augmentation 和 Generation 步驟。
                
6. **建立完整的 RAG Chain**
    
    - 設計你的 Prompt 模板，特別引導 LLM 處理可能的衝突。
        
    - 將檢索器、Prompt 模板和 LLM (例如 OpenAI 的 `GPT-3.5-turbo`) 串聯起來。
        

**3. 測試與驗證**

- **測試 1 (正常情況)：**
    
    - **提問：** `"請問如何聯絡客服？"`
        
    - **預期結果：** Bot 應該能準確回答 0800 電話號碼。
        
- **測試 2 (找不到答案)：**
    
    - **提問：** `"海外運費是多少？"`
        
    - **預期結果：** 你的程式碼應該在檢索階段就因為分數過低而中止，並直接返回「找不到資訊」的回應。
        
- **測試 3 (答案衝突)：**
    
    - **提問：** `"退貨期限是幾天？"`
        
    - **預期結果：**
        
        - 檢索器應該會同時找到「7 天」和「14 天」這兩個 Chunks。
            
        - LLM 應該會根據你的 Prompt 指示，生成一個綜合性的答案，例如：「標準退貨期限是 7 天，但如果您是 VIP 會員，則可以延長至 14 天。」
            

---

**總結與學習重點回顧：**

- RAG 讓我們能為 LLM 外掛一個「大腦硬碟」，使其知識可以即時更新。
    
- Chunking 和 Vectorization 是將外部知識「格式化」以便機器檢索的關鍵前置步驟。
    
- 一個穩健的 RAG 系統不僅要會「回答」，更要會「拒絕回答」和「處理模糊地帶」。
    
- 透過 Lab 的實作，學員應能體會到，真正的挑戰往往在於如何優雅地處理各種預期外的失敗情境。

---

## 📘 模組 5：產品思維 for LLM Apps（第 5 週）

### 🎯 學習目標
培養產品思維，知道何時該用/不該用 LLM

### 📚 核心內容

#### 5.1 LLM 的能力矩陣

| 任務類型 | 適合用 LLM | 不適合用 LLM | 替代方案 |
|---------|-----------|-------------|---------|
| **創意生成** | ✅ 文案、故事、點子 | ❌ 需要事實準確的內容 | 人工創作 + LLM 輔助 |
| **格式轉換** | ✅ JSON↔自然語言 | ❌ 複雜的資料轉換 | 專門的 Parser |
| **分類任務** | ✅ 情感分析、主題分類 | ❌ 需要 100% 準確率 | 傳統 ML 模型 |
| **計算任務** | ✅ 簡單估算 | ❌ 精確計算 | Calculator API |
| **搜尋任務** | ✅ 語義搜尋 | ❌ 精確匹配 | 傳統搜尋引擎 |
| **決策任務** | ✅ 建議、推薦 | ❌ 關鍵決策 | 規則引擎 + 人工 |

#### 5.2 成本-延遲-體驗三角

```python
class ProductDecisionFramework:
    """產品決策框架"""
    
    def __init__(self):
        self.metrics = {
            "cost": {"weight": 0.3, "threshold": 0.1},      # $0.1 per request
            "latency": {"weight": 0.3, "threshold": 2000},  # 2 seconds
            "quality": {"weight": 0.4, "threshold": 0.85}   # 85% satisfaction
        }
    
    def evaluate_llm_fit(self, use_case):
        """評估是否適合用 LLM"""
        score = 0
        reasons = []
        
        # 正面因素
        if use_case["requires_creativity"]:
            score += 30
            reasons.append("✅ 需要創意生成")
        
        if use_case["handles_natural_language"]:
            score += 25
            reasons.append("✅ 處理自然語言")
        
        if use_case["needs_flexibility"]:
            score += 20
            reasons.append("✅ 需要靈活性")
        
        # 負面因素
        if use_case["requires_deterministic"]:
            score -= 40
            reasons.append("❌ 需要確定性結果")
        
        if use_case["cost_sensitive"]:
            score -= 30
            reasons.append("❌ 成本敏感")
        
        if use_case["latency_critical"]:
            score -= 25
            reasons.append("❌ 延遲關鍵")
        
        # 決策
        recommendation = "USE_LLM" if score > 0 else "AVOID_LLM"
        
        return {
            "score": score,
            "recommendation": recommendation,
            "reasons": reasons,
            "alternative": self.suggest_alternative(use_case) if score < 0 else None
        }
    
    def suggest_alternative(self, use_case):
        """建議替代方案"""
        if use_case["requires_deterministic"]:
            return "使用規則引擎或傳統演算法"
        elif use_case["cost_sensitive"]:
            return "使用快取層 + 小模型"
        elif use_case["latency_critical"]:
            return "預生成 + Edge 部署"
        else:
            return "混合方案：關鍵路徑用傳統方法，輔助功能用 LLM"
```

#### 5.3 優雅降級策略

```python
class GracefulDegradation:
    """優雅降級系統"""
    
    def __init__(self):
        self.strategies = [
            self.try_primary,      # 主要策略：完整 LLM
            self.try_cache,        # 快取策略
            self.try_simple_model, # 簡單模型
            self.try_template,     # 模板回覆
            self.try_human         # 人工介入
        ]
        
        self.cache = ResponseCache()
        self.template_engine = TemplateEngine()
    
    async def handle_request(self, request):
        """多層降級處理"""
        start_time = time.time()
        
        for i, strategy in enumerate(self.strategies):
            try:
                # 設定超時
                timeout = self.calculate_timeout(i)
                
                result = await asyncio.wait_for(
                    strategy(request),
                    timeout=timeout
                )
                
                if result["success"]:
                    # 記錄使用了哪個策略
                    result["strategy_used"] = strategy.__name__
                    result["degradation_level"] = i
                    result["total_time"] = time.time() - start_time
                    
                    return result
                    
            except asyncio.TimeoutError:
                print(f"Strategy {strategy.__name__} timeout")
                continue
            except Exception as e:
                print(f"Strategy {strategy.__name__} failed: {e}")
                continue
        
        # 所有策略都失敗
        return self.final_fallback(request)
    
    async def try_primary(self, request):
        """主要策略：完整 LLM 處理"""
        response = await llm.complete(
            request["prompt"],
            model="gpt-4",
            temperature=0.7
        )
        
        return {
            "success": True,
            "response": response,
            "quality": "high"
        }
    
    async def try_cache(self, request):
        """快取策略"""
        # 語義相似度快取
        cached = self.cache.get_similar(request["prompt"], threshold=0.95)
        
        if cached:
            return {
                "success": True,
                "response": cached["response"],
                "quality": "cached",
                "cache_hit": True
            }
        
        return {"success": False}
    
    async def try_simple_model(self, request):
        """使用更簡單的模型"""
        response = await llm.complete(
            request["prompt"],
            model="gpt-3.5-turbo",  # 降級到更快更便宜的模型
            temperature=0.5,
            max_tokens=150
        )
        
        return {
            "success": True,
            "response": response,
            "quality": "medium"
        }
    
    async def try_template(self, request):
        """模板回覆"""
        # 意圖識別
        intent = self.classify_intent(request["prompt"])
        
        if intent in self.template_engine.templates:
            response = self.template_engine.generate(intent, request)
            return {
                "success": True,
                "response": response,
                "quality": "template"
            }
        
        return {"success": False}
    
    async def try_human(self, request):
        """轉人工"""
        # 創建工單
        ticket = self.create_support_ticket(request)
        
        response = f"""
        您的問題較為複雜，我已經為您創建了工單 #{ticket['id']}。
        客服人員將在 30 分鐘內與您聯繫。
        
        您也可以直接撥打客服熱線：400-XXX-XXXX
        """
        
        return {
            "success": True,
            "response": response,
            "quality": "human_escalation",
            "ticket": ticket
        }
    
    def calculate_timeout(self, level):
        """根據降級等級計算超時時間"""
        timeouts = [3.0, 1.0, 0.5, 0.2, 0.1]  # 逐級降低
        return timeouts[min(level, len(timeouts) - 1)]
```

### 🔬 Lab 實作：降級策略設計與測試

```python
# Lab: 設計並測試降級策略
class DegradationTestBench:
    """降級策略測試平台"""
    
    def __init__(self):
        self.degradation = GracefulDegradation()
        self.metrics = {
            "success_rate": [],
            "response_time": [],
            "quality_score": [],
            "cost": []
        }
    
    async def simulate_load(self, requests_per_second=10, duration=60):
        """模擬負載測試"""
        tasks = []
        
        for i in range(duration):
            for j in range(requests_per_second):
                # 模擬不同類型的請求
                request = self.generate_request()
                
                # 隨機注入故障
                if random.random() < 0.1:  # 10% 故障率
                    request["inject_failure"] = True
                
                task = self.process_request(request)
                tasks.append(task)
            
            await asyncio.sleep(1)
        
        results = await asyncio.gather(*tasks)
        self.analyze_results(results)
    
    async def process_request(self, request):
        """處理單個請求"""
        start = time.time()
        
        # 模擬網路問題
        if request.get("inject_failure"):
            await asyncio.sleep(random.uniform(5, 10))  # 超時
        
        result = await self.degradation.handle_request(request)
        
        # 記錄 metrics
        self.metrics["response_time"].append(time.time() - start)
        self.metrics["success_rate"].append(1 if result["success"] else 0)
        self.metrics["quality_score"].append(
            self.calculate_quality_score(result)
        )
        self.metrics["cost"].append(
            self.calculate_cost(result)
        )
        
        return result
    
    def calculate_quality_score(self, result):
        """計算品質分數"""
        quality_map = {
            "high": 1.0,
            "cached": 0.9,
            "medium": 0.7,
            "template": 0.5,
            "human_escalation": 0.3
        }
        return quality_map.get(result.get("quality", "unknown"), 0)
    
    def calculate_cost(self, result):
        """計算成本"""
        cost_map = {
            "try_primary": 0.03,      # GPT-4
            "try_cache": 0.0001,      # 幾乎免費
            "try_simple_model": 0.002, # GPT-3.5
            "try_template": 0,         # 免費
            "try_human": 5.0          # 人工成本
        }
        
        strategy = result.get("strategy_used", "unknown")
        return cost_map.get(strategy, 0)
    
    def analyze_results(self, results):
        """分析測試結果"""
        print("\n=== 降級策略測試報告 ===\n")
        
        # 成功率
        success_rate = sum(self.metrics["success_rate"]) / len(self.metrics["success_rate"])
        print(f"✅ 成功率: {success_rate:.2%}")
        
        # 響應時間
        avg_response = sum(self.metrics["response_time"]) / len(self.metrics["response_time"])
        p95_response = sorted(self.metrics["response_time"])[int(len(self.metrics["response_time"]) * 0.95)]
        print(f"⏱️ 平均響應時間: {avg_response:.2f}秒")
        print(f"⏱️ P95 響應時間: {p95_response:.2f}秒")
        
        # 品質分數
        avg_quality = sum(self.metrics["quality_score"]) / len(self.metrics["quality_score"])
        print(f"⭐ 平均品質分數: {avg_quality:.2f}/1.0")
        
        # 成本
        total_cost = sum(self.metrics["cost"])
        avg_cost = total_cost / len(self.metrics["cost"])
        print(f"💰 總成本: ${total_cost:.2f}")
        print(f"💰 平均成本: ${avg_cost:.4f}/request")
        
        # 策略使用分布
        strategy_counts = {}
        for result in results:
            strategy = result.get("strategy_used", "unknown")
            strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1
        
        print("\n📊 策略使用分布:")
        for strategy, count in sorted(strategy_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(results) * 100
            print(f"  - {strategy}: {count} ({percentage:.1f}%)")
        
        # 成本效益分析
        cost_per_quality = total_cost / (avg_quality * len(results))
        print(f"\n💡 成本效益比: ${cost_per_quality:.4f}/quality point")
        
        # 建議
        print("\n🎯 優化建議:")
        if avg_response > 2:
            print("  - 響應時間過長，考慮增加快取或使用更快的模型")
        if avg_quality < 0.7:
            print("  - 品質分數偏低，考慮調整降級閾值")
        if avg_cost > 0.01:
            print("  - 成本偏高，增加快取命中率或使用更多模板")

# 執行測試
test_bench = DegradationTestBench()
await test_bench.simulate_load(requests_per_second=5, duration=30)
```

### 💡 實戰心得
> 🎯 **黃金法則**：永遠不要讓 LLM 成為單點故障。每個 LLM 呼叫都應該有至少 2 層 fallback。

---

## 📘 模組 6：應用級 Workflow（第 6 週）

### 🎯 學習目標
設計多步驟任務流程，處理真實世界的髒資料

### 📚 核心內容

#### 6.1 Chain 思維設計模式

```python
class WorkflowChain:
    """工作流鏈設計"""
    
    def __init__(self):
        self.steps = []
        self.error_handlers = {}
        self.validators = {}
    
    def add_step(self, name, function, validator=None, error_handler=None):
        """添加步驟"""
        self.steps.append({
            "name": name,
            "function": function
        })
        
        if validator:
            self.validators[name] = validator
        if error_handler:
            self.error_handlers[name] = error_handler
        
        return self  # 支援鏈式呼叫
    
    async def execute(self, input_data):
        """執行工作流"""
        context = {"input": input_data, "steps": {}}
        
        for step in self.steps:
            step_name = step["name"]
            print(f"執行步驟: {step_name}")
            
            try:
                # 執行步驟
                result = await step["function"](context)
                
                # 驗證結果
                if step_name in self.validators:
                    is_valid = self.validators[step_name](result)
                    if not is_valid:
                        raise ValueError(f"Validation failed for {step_name}")
                
                # 儲存結果
                context["steps"][step_name] = result
                
            except Exception as e:
                # 錯誤處理
                if step_name in self.error_handlers:
                    result = self.error_handlers[step_name](e, context)
                    context["steps"][step_name] = result
                else:
                    raise
        
        return context
```

#### 6.2 髒資料處理大全

```python
class DirtyDataProcessor:
    """髒資料處理專家"""
    
    def __init__(self):
        self.cleaners = {
            "typo": self.fix_typos,
            "mixed_language": self.handle_mixed_language,
            "emotional": self.handle_emotional_input,
            "incomplete": self.handle_incomplete,
            "contradictory": self.handle_contradictory
        }
    
    def clean(self, dirty_input):
        """清理髒資料"""
        cleaned = dirty_input
        issues_found = []
        
        # 檢測並修復各種問題
        for issue_type, cleaner in self.cleaners.items():
            if self.detect_issue(cleaned, issue_type):
                cleaned = cleaner(cleaned)
                issues_found.append(issue_type)
        
        return {
            "original": dirty_input,
            "cleaned": cleaned,
            "issues_found": issues_found,
            "confidence": self.calculate_confidence(issues_found)
        }
    
    def fix_typos(self, text):
        """修正錯字"""
        # 簡單的錯字修正
        common_typos = {
            "teh": "the",
            "recieve": "receive",
            "occured": "occurred",
            "refund": "refund",  
            "cancle": "cancel"
        }
        
        for typo, correct in common_typos.items():
            text = text.replace(typo, correct)
        
        # 使用 LLM 修正更複雜的錯字
        if self.has_potential_typos(text):
            prompt = f"""
            修正以下文字中的錯字，但保持原意：
            原文：{text}
            修正後：
            """
            text = llm.complete(prompt, temperature=0.1)
        
        return text
    
    def handle_mixed_language(self, text):
        """處理混合語言"""
        # 檢測語言
        languages = self.detect_languages(text)
        
        if len(languages) > 1:
            # 策略 1：翻譯成主要語言
            primary_lang = max(languages.items(), key=lambda x: x[1])[0]
            
            prompt = f"""
            將以下混合語言文字統一翻譯成{primary_lang}：
            {text}
            """
            
            return llm.complete(prompt)
        
        return text
    
    def handle_emotional_input(self, text):
        """處理情緒化輸入"""
        emotion_score = self.detect_emotion(text)
        
        if emotion_score > 0.7:  # 高度情緒化
            # 提取核心訴求
            prompt = f"""
            用戶情緒激動地說：{text}
            
            請提取用戶的核心訴求（忽略情緒化表達）：
            """
            
            core_request = llm.complete(prompt)
            
            return {
                "original": text,
                "emotion_level": "high",
                "core_request": core_request,
                "suggested_response_tone": "empathetic"
            }
        
        return text
    
    def handle_incomplete(self, text):
        """處理不完整輸入"""
        if len(text.split()) < 3:  # 太短
            return {
                "original": text,
                "issue": "incomplete",
                "clarification_needed": True,
                "suggested_questions": [
                    "您能詳細說明一下嗎？",
                    "請問具體是什麼問題？",
                    "能提供更多資訊嗎？"
                ]
            }
        
        return text
```

#### 6.3 Agent-like Flow 設計

```python
class SimpleAgent:
    """簡單 Agent 流程"""
    
    def __init__(self):
        self.tools = {
            "search_order": self.search_order,
            "check_inventory": self.check_inventory,
            "calculate_shipping": self.calculate_shipping,
            "process_refund": self.process_refund
        }
        
        self.planner = TaskPlanner()
        self.executor = TaskExecutor()
    
    async def process(self, user_request):
        """處理用戶請求"""
        
        # 1. 理解意圖
        intent = await self.understand_intent(user_request)
        
        # 2. 規劃步驟
        plan = await self.planner.create_plan(intent, self.tools.keys())
        
        # 3. 執行計劃
        results = []
        for step in plan["steps"]:
            result = await self.execute_step(step)
            results.append(result)
            
            # 動態調整計劃
            if result.get("requires_replanning"):
                plan = await self.planner.replan(plan, results)
        
        # 4. 綜合結果
        final_response = await self.synthesize_response(results)
        
        return final_response
    
    async def understand_intent(self, request):
        """理解用戶意圖"""
        prompt = f"""
        分析用戶請求並識別意圖：
        
        用戶說：{request}
        
        可能的意圖：
        - ORDER_STATUS: 查詢訂單狀態
        - REFUND_REQUEST: 申請退款
        - PRODUCT_INQUIRY: 產品諮詢
        - SHIPPING_INFO: 運送資訊
        - COMPLAINT: 投訴
        - OTHER: 其他
        
        返回格式：
        {{
            "primary_intent": "...",
            "entities": {{...}},
            "confidence": 0.X
        }}
        """
        
        response = await llm.complete(prompt)
        return json.loads(response)
    
    async def execute_step(self, step):
        """執行單個步驟"""
        tool_name = step["tool"]
        params = step["params"]
        
        if tool_name not in self.tools:
            return {"error": f"Tool {tool_name} not found"}
        
        try:
            result = await self.tools[tool_name](**params)
            return {"success": True, "data": result}
        except Exception as e:
            return {"success": False, "error": str(e)}
```

### 🔬 Lab 實作：訂單查詢助理

```python
# Lab: 能處理混合語言和模糊輸入的訂單助理
class SmartOrderAssistant:
    def __init__(self):
        self.workflow = WorkflowChain()
        self.data_processor = DirtyDataProcessor()
        self.agent = SimpleAgent()
        
        # 設定工作流
        self.setup_workflow()
    
    def setup_workflow(self):
        """設定工作流程"""
        self.workflow \
            .add_step("clean_input", self.clean_input) \
            .add_step("extract_info", self.extract_order_info) \
            .add_step("search_order", self.search_order) \
            .add_step("check_status", self.check_order_status) \
            .add_step("generate_response", self.generate_response)
    
    async def clean_input(self, context):
        """步驟 1：清理輸入"""
        raw_input = context["input"]
        
        # 處理髒資料
        cleaned = self.data_processor.clean(raw_input)
        
        # 處理特殊情況
        if "mixed_language" in cleaned["issues_found"]:
            # 統一語言
            cleaned["cleaned"] = await self.translate_to_primary(cleaned["cleaned"])
        
        if "emotional" in cleaned["issues_found"]:
            # 記錄情緒狀態
            context["emotional_state"] = "high"
            context["response_tone"] = "empathetic"
        
        return cleaned
    
    async def extract_order_info(self, context):
        """步驟 2：提取訂單資訊"""
        cleaned_input = context["steps"]["clean_input"]["cleaned"]
        
        prompt = f"""
        從以下文字中提取訂單相關資訊：
        {cleaned_input}
        
        提取：
        - order_id: 訂單號（如果有）
        - product_name: 產品名稱
        - issue: 問題描述
        - date: 相關日期
        
        如果資訊不完整，標記 needs_clarification: true
        """
        
        extracted = await llm.complete(prompt, response_format={"type": "json"})
        
        # 驗證提取結果
        if not extracted.get("order_id"):
            # 嘗試模糊匹配
            extracted["possible_orders"] = await self.fuzzy_search_orders(cleaned_input)
        
        return extracted
    
    async def search_order(self, context):
        """步驟 3：搜尋訂單"""
        order_info = context["steps"]["extract_info"]
        
        if order_info.get("order_id"):
            # 精確搜尋
            order = await self.db.find_order(order_info["order_id"])
        elif order_info.get("possible_orders"):
            # 讓用戶確認
            return {
                "found_multiple": True,
                "orders": order_info["possible_orders"],
                "needs_confirmation": True
            }
        else:
            # 根據其他資訊搜尋
            orders = await self.db.search_orders(
                customer_id=context.get("customer_id"),
                product=order_info.get("product_name"),
                date_range=self.parse_date_range(order_info.get("date"))
            )
            
            if len(orders) == 1:
                order = orders[0]
            elif len(orders) > 1:
                return {"found_multiple": True, "orders": orders}
            else:
                return {"found": False}
        
        return {"found": True, "order": order}
    
    async def generate_response(self, context):
        """步驟 5：生成回應"""
        order_result = context["steps"]["search_order"]
        
        # 根據不同情況生成回應
        if not order_result.get("found"):
            response = "抱歉，我找不到相關訂單。請確認訂單號或提供更多資訊。"
        elif order_result.get("found_multiple"):
            response = self.format_multiple_orders(order_result["orders"])
        else:
            order = order_result["order"]
            status = context["steps"]["check_status"]
            response = self.format_order_status(order, status)
        
        # 根據情緒狀態調整語氣
        if context.get("emotional_state") == "high":
            response = self.add_empathy(response)
        
        return response
    
    async def handle_user_input(self, user_input):
        """主入口：處理用戶輸入"""
        
        # 測試各種髒輸入
        test_inputs = [
            "我的訂單（訂單號：ORD12345）到哪了？？？",  # 正常
            "Check my order status ORDER12345 謝謝",      # 混合語言
            "wtf 我的東西呢！！！都三天了！！！",         # 情緒化
            "訂單",                                        # 不完整
            "我上週買的藍牙耳機怎麼還沒到",                # 模糊
            "ORD1234... 不對是 ORD12346",                 # 矛盾
        ]
        
        results = []
        for test_input in test_inputs:
            print(f"\n處理輸入: {test_input}")
            
            try:
                result = await self.workflow.execute(test_input)
                response = result["steps"]["generate_response"]
                
                print(f"清理後: {result['steps']['clean_input']['cleaned']}")
                print(f"提取資訊: {result['steps']['extract_info']}")
                print(f"回應: {response}")
                
                results.append({
                    "input": test_input,
                    "success": True,
                    "response": response
                })
                
            except Exception as e:
                print(f"錯誤: {e}")
                results.append({
                    "input": test_input,
                    "success": False,
                    "error": str(e)
                })
        
        # 統計成功率
        success_rate = sum(1 for r in results if r["success"]) / len(results)
        print(f"\n成功率: {success_rate:.1%}")
        
        return results

# 執行測試
assistant = SmartOrderAssistant()
results = await assistant.handle_user_input("測試輸入")
```

### 💡 實戰心得
> 🔧 **髒資料是常態，不是例外**。一個產品級的 LLM 應用，50% 的程式碼都在處理各種邊界情況。

---

## 📘 模組 7：可靠性與可觀測性（第 7 週）

### 🎯 學習目標
建立監控體系，優化成本與性能

### 📚 核心內容

#### 7.1 成本工程實戰

```python
class CostEngineering:
    """成本工程系統"""
    
    def __init__(self):
        self.token_prices = {
            "gpt-4": {"input": 0.03, "output": 0.06},      # per 1K tokens
            "gpt-3.5-turbo": {"input": 0.001, "output": 0.002},
            "claude-3-opus": {"input": 0.015, "output": 0.075},
            "claude-3-sonnet": {"input": 0.003, "output": 0.015}
        }
        
        self.optimizers = {
            "batching": BatchingOptimizer(),
            "caching": CachingOptimizer(),
            "routing": ModelRouter(),
            "compression": PromptCompressor()
        }
    
    def optimize_request(self, request):
        """優化單個請求"""
        original_cost = self.estimate_cost(request)
        optimized = request.copy()
        
        # 1. 壓縮 Prompt
        optimized["prompt"] = self.optimizers["compression"].compress(
            request["prompt"]
        )
        
        # 2. 選擇最佳模型
        optimized["model"] = self.optimizers["routing"].select_model(
            task_type=request["task_type"],
            quality_requirement=request.get("quality", 0.8),
            budget=request.get("budget", float('inf'))
        )
        
        # 3. 檢查快取
        cached = self.optimizers["caching"].get(optimized["prompt"])
        if cached:
            return {
                "response": cached,
                "cost": 0,
                "source": "cache",
                "savings": original_cost
            }
        
        # 4. 批次處理
        if self.optimizers["batching"].should_batch(optimized):
            batch_id = self.optimizers["batching"].add_to_batch(optimized)
            return {
                "batch_id": batch_id,
                "estimated_wait": self.optimizers["batching"].estimate_wait(),
                "estimated_savings": original_cost * 0.3
            }
        
        final_cost = self.estimate_cost(optimized)
        
        return {
            "optimized_request": optimized,
            "original_cost": original_cost,
            "optimized_cost": final_cost,
            "savings": original_cost - final_cost,
            "savings_percentage": (original_cost - final_cost) / original_cost * 100
        }
    
    def estimate_cost(self, request):
        """估算成本"""
        model = request["model"]
        prompt_tokens = self.count_tokens(request["prompt"])
        
        # 估算輸出 tokens（經驗值）
        estimated_output = prompt_tokens * 0.8
        
        input_cost = (prompt_tokens / 1000) * self.token_prices[model]["input"]
        output_cost = (estimated_output / 1000) * self.token_prices[model]["output"]
        
        return input_cost + output_cost
```

#### 7.2 可觀測性架構

```python
class ObservabilitySystem:
    """可觀測性系統"""
    
    def __init__(self):
        self.metrics = MetricsCollector()
        self.tracer = DistributedTracer()
        self.logger = StructuredLogger()
        self.profiler = PerformanceProfiler()
        
        # 關鍵指標
        self.key_metrics = {
            "latency": {"threshold": 2000, "unit": "ms"},
            "error_rate": {"threshold": 0.01, "unit": "%"},
            "token_cost": {"threshold": 0.1, "unit": "$"},
            "user_satisfaction": {"threshold": 0.85, "unit": "score"}
        }
    
    def trace_request(self, request_id):
        """追蹤請求全流程"""
        
        @self.tracer.span("llm_request")
        async def traced_execution():
            span = self.tracer.current_span()
            
            # 記錄請求屬性
            span.set_attributes({
                "request.id": request_id,
                "request.model": request.get("model"),
                "request.prompt_length": len(request.get("prompt", "")),
                "request.timestamp": datetime.now().isoformat()
            })
            
            try:
                # Prompt 處理階段
                with self.tracer.span("prompt_processing"):
                    prompt = await self.process_prompt(request)
                    span.set_attribute("prompt.tokens", self.count_tokens(prompt))
                
                # LLM 呼叫階段
                with self.tracer.span("llm_call"):
                    start_time = time.time()
                    response = await self.call_llm(prompt)
                    latency = (time.time() - start_time) * 1000
                    
                    span.set_attributes({
                        "llm.latency_ms": latency,
                        "llm.response_tokens": self.count_tokens(response),
                        "llm.total_tokens": self.count_tokens(prompt + response)
                    })
                
                # 後處理階段
                with self.tracer.span("post_processing"):
                    final_response = await self.post_process(response)
                
                # 記錄成功metrics
                self.metrics.record("request_success", 1)
                self.metrics.record("request_latency", latency)
                
                return final_response
                
            except Exception as e:
                # 記錄錯誤
                span.set_status("ERROR")
                span.set_attribute("error.message", str(e))
                
                self.metrics.record("request_error", 1)
                self.logger.error(f"Request failed: {e}", extra={
                    "request_id": request_id,
                    "error_type": type(e).__name__
                })
                
                raise
        
        return await traced_execution()
```

#### 7.3 A/B 測試框架

```python
class ABTestingFramework:
    """A/B 測試框架"""
    
    def __init__(self):
        self.experiments = {}
        self.results = defaultdict(lambda: {
            "impressions": 0,
            "conversions": 0,
            "total_latency": 0,
            "total_cost": 0,
            "errors": 0
        })
    
    def create_experiment(self, name, variants):
        """創建實驗"""
        self.experiments[name] = {
            "variants": variants,
            "traffic_split": self.calculate_traffic_split(len(variants)),
            "created_at": datetime.now(),
            "status": "running"
        }
    
    async def run_variant(self, experiment_name, user_id):
        """執行變體"""
        experiment = self.experiments[experiment_name]
        
        # 分配變體
        variant = self.assign_variant(user_id, experiment)
        variant_config = experiment["variants"][variant]
        
        # 執行並追蹤
        start_time = time.time()
        try:
            # 根據變體配置執行
            result = await self.execute_variant(variant_config)
            
            # 記錄成功metrics
            self.results[f"{experiment_name}_{variant}"]["impressions"] += 1
            self.results[f"{experiment_name}_{variant}"]["total_latency"] += (time.time() - start_time)
            
            # 計算成本
            cost = self.calculate_cost(variant_config, result)
            self.results[f"{experiment_name}_{variant}"]["total_cost"] += cost
            
            return result
            
        except Exception as e:
            self.results[f"{experiment_name}_{variant}"]["errors"] += 1
            raise
    
    def analyze_experiment(self, experiment_name):
        """分析實驗結果"""
        experiment = self.experiments[experiment_name]
        analysis = {}
        
        for variant in experiment["variants"]:
            key = f"{experiment_name}_{variant}"
            data = self.results[key]
            
            if data["impressions"] > 0:
                analysis[variant] = {
                    "conversion_rate": data["conversions"] / data["impressions"],
                    "avg_latency": data["total_latency"] / data["impressions"],
                    "avg_cost": data["total_cost"] / data["impressions"],
                    "error_rate": data["errors"] / data["impressions"],
                    "sample_size": data["impressions"]
                }
                
                # 計算統計顯著性
                if len(analysis) > 1:
                    analysis[variant]["statistical_significance"] = \
                        self.calculate_significance(analysis)
        
        # 推薦獲勝者
        winner = self.recommend_winner(analysis)
        
        return {
            "analysis": analysis,
            "winner": winner,
            "confidence": self.calculate_confidence(analysis),
            "recommendation": self.generate_recommendation(analysis, winner)
        }
```

### 🔬 Lab 實作：成本優化實驗

```python
# Lab: Streaming vs Batch 實驗
class StreamingVsBatchExperiment:
    def __init__(self):
        self.ab_test = ABTestingFramework()
        self.cost_engine = CostEngineering()
        self.observability = ObservabilitySystem()
        
        # 定義實驗變體
        self.setup_experiment()
    
    def setup_experiment(self):
        """設定實驗"""
        self.ab_test.create_experiment(
            name="streaming_vs_batch",
            variants={
                "streaming": {
                    "mode": "streaming",
                    "model": "gpt-3.5-turbo",
                    "batch_size": 1,
                    "timeout": 30
                },
                "batch_small": {
                    "mode": "batch",
                    "model": "gpt-3.5-turbo",
                    "batch_size": 5,
                    "timeout": 10
                },
                "batch_large": {
                    "mode": "batch",
                    "model": "gpt-3.5-turbo",
                    "batch_size": 20,
                    "timeout": 5
                }
            }
        )
    
    async def run_test(self, num_requests=100):
        """執行測試"""
        tasks = []
        
        for i in range(num_requests):
            user_id = f"user_{i % 10}"  # 模擬 10 個用戶
            
            # 創建測試請求
            request = {
                "prompt": f"Test prompt {i}: Explain quantum computing",
                "user_id": user_id,
                "timestamp": datetime.now()
            }
            
            # 執行實驗
            task = self.process_with_monitoring(request, user_id)
            tasks.append(task)
            
            # 模擬真實流量
            await asyncio.sleep(random.uniform(0.1, 0.5))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # 分析結果
        self.analyze_results(results)
    
    async def process_with_monitoring(self, request, user_id):
        """帶監控的處理"""
        
        # 開始追蹤
        with self.observability.tracer.span("experiment_request") as span:
            span.set_attribute("user_id", user_id)
            
            try:
                # 執行變體
                result = await self.ab_test.run_variant(
                    "streaming_vs_batch",
                    user_id
                )
                
                # 記錄詳細metrics
                self.observability.metrics.record("request_success", 1, {
                    "variant": result.get("variant"),
                    "user_id": user_id
                })
                
                # 模擬用戶行為（轉換）
                if self.simulate_user_satisfaction(result):
                    variant_key = f"streaming_vs_batch_{result['variant']}"
                    self.ab_test.results[variant_key]["conversions"] += 1
                
                return result
                
            except Exception as e:
                self.observability.logger.error(f"Experiment failed: {e}")
                raise
    
    def simulate_user_satisfaction(self, result):
        """模擬用戶滿意度"""
        # 基於延遲和品質計算滿意度
        latency_score = 1.0 if result.get("latency", 0) < 2000 else 0.5
        quality_score = result.get("quality", 0.8)
        
        satisfaction = (latency_score * 0.4 + quality_score * 0.6)
        
        # 隨機決定是否轉換
        return random.random() < satisfaction
    
    def analyze_results(self, results):
        """分析測試結果"""
        analysis = self.ab_test.analyze_experiment("streaming_vs_batch")
        
        print("\n" + "="*60)
        print("📊 A/B 測試結果分析")
        print("="*60)
        
        for variant, data in analysis["analysis"].items():
            print(f"\n🔹 變體: {variant}")
            print(f"   轉換率: {data['conversion_rate']:.2%}")
            print(f"   平均延遲: {data['avg_latency']:.2f}秒")
            print(f"   平均成本: ${data['avg_cost']:.4f}")
            print(f"   錯誤率: {data['error_rate']:.2%}")
            print(f"   樣本數: {data['sample_size']}")
        
        print(f"\n🏆 獲勝者: {analysis['winner']}")
        print(f"📈 信心度: {analysis['confidence']:.2%}")
        print(f"\n💡 建議: {analysis['recommendation']}")
        
        # 成本節省分析
        self.calculate_cost_savings(analysis)
    
    def calculate_cost_savings(self, analysis):
        """計算成本節省"""
        baseline = analysis["analysis"].get("streaming", {})
        
        print("\n💰 成本節省分析:")
        for variant, data in analysis["analysis"].items():
            if variant != "streaming":
                savings = baseline.get("avg_cost", 0) - data["avg_cost"]
                savings_pct = (savings / baseline.get("avg_cost", 1)) * 100
                
                print(f"   {variant}: 節省 ${savings:.4f}/request ({savings_pct:.1f}%)")
                
                # 月度預估
                monthly_requests = 100000
                monthly_savings = savings * monthly_requests
                print(f"      預估月節省: ${monthly_savings:,.2f}")

# 執行實驗
experiment = StreamingVsBatchExperiment()
await experiment.run_test(num_requests=100)
```

### 💡 實戰心得
> 📊 **數據驅動決策**：不要憑感覺優化，要用 A/B 測試證明。我們曾經以為 streaming 一定更好，結果發現批次處理在某些場景下成本降低 70%。

---

## 📘 模組 8：應用落地與 UX 強化（第 8 週）

### 🎯 學習目標
將應用推向生產環境，加強用戶體驗

### 📚 核心內容

#### 8.1 產品指標體系設計

```python
class ProductMetrics:
    """產品指標系統"""
    
    def __init__(self):
        # 技術指標
        self.technical_metrics = {
            "latency_p50": {"target": 1000, "unit": "ms", "weight": 0.2},
            "latency_p95": {"target": 3000, "unit": "ms", "weight": 0.1},
            "error_rate": {"target": 0.01, "unit": "%", "weight": 0.2},
            "token_cost_per_request": {"target": 0.05, "unit": "$", "weight": 0.2}
        }
        
        # 業務指標
        self.business_metrics = {
            "csat": {"target": 0.85, "unit": "score", "weight": 0.3},  # 客戶滿意度
            "resolution_rate": {"target": 0.80, "unit": "%", "weight": 0.3},  # 解決率
            "fallback_rate": {"target": 0.10, "unit": "%", "weight": 0.2},  # 降級率
            "escalation_rate": {"target": 0.05, "unit": "%", "weight": 0.2}  # 轉人工率
        }
        
        # 綜合健康分數
        self.health_score_calculator = HealthScoreCalculator()
    
    def calculate_health_score(self, current_metrics):
        """計算健康分數"""
        technical_score = 0
        business_score = 0
        
        # 計算技術分數
        for metric, config in self.technical_metrics.items():
            current = current_metrics.get(metric, 0)
            target = config["target"]
            
            # 反向指標（越低越好）
            if metric in ["latency_p50", "latency_p95", "error_rate", "token_cost_per_request"]:
                score = min(1.0, target / max(current, 0.001))
            else:
                score = min(1.0, current / target)
            
            technical_score += score * config["weight"]
        
        # 計算業務分數
        for metric, config in self.business_metrics.items():
            current = current_metrics.get(metric, 0)
            target = config["target"]
            
            # 反向指標
            if metric in ["fallback_rate", "escalation_rate"]:
                score = min(1.0, target / max(current, 0.001))
            else:
                score = min(1.0, current / target)
            
            business_score += score * config["weight"]
        
        # 綜合分數
        overall_score = technical_score * 0.4 + business_score * 0.6
        
        return {
            "overall": overall_score,
            "technical": technical_score,
            "business": business_score,
            "status": self.get_status(overall_score),
            "recommendations": self.generate_recommendations(current_metrics)
        }
    
    def get_status(self, score):
        """獲取狀態"""
        if score >= 0.9:
            return "🟢 Excellent"
        elif score >= 0.7:
            return "🟡 Good"
        elif score >= 0.5:
            return "🟠 Needs Improvement"
        else:
            return "🔴 Critical"
    
    def generate_recommendations(self, metrics):
        """生成優化建議"""
        recommendations = []
        
        # 延遲問題
        if metrics.get("latency_p95", 0) > 5000:
            recommendations.append("⚡ 考慮使用更快的模型或增加快取")
        
        # 成本問題
        if metrics.get("token_cost_per_request", 0) > 0.1:
            recommendations.append("💰 優化 Prompt 長度或使用更便宜的模型")
        
        # 滿意度問題
        if metrics.get("csat", 1) < 0.7:
            recommendations.append("😊 改善回應品質，考慮增加人工審核")
        
        # 降級率問題
        if metrics.get("fallback_rate", 0) > 0.2:
            recommendations.append("🔧 檢查主要服務穩定性，優化降級策略")
        
        return recommendations
```

#### 8.2 LLM 特有的 UX 模式

```python
class LLMSpecificUX:
    """LLM 特有的 UX 元件"""
    
    def __init__(self):
        self.components = {
            "thinking_indicator": ThinkingIndicator(),
            "confidence_display": ConfidenceDisplay(),
            "quick_fix": QuickFixComponent(),
            "streaming_renderer": StreamingRenderer()
        }
    
    def create_thinking_indicator(self):
        """思考過程可視化"""
        return {
            "type": "thinking_indicator",
            "stages": [
                {"id": "understanding", "label": "理解問題", "duration": 500},
                {"id": "searching", "label": "搜尋資料", "duration": 1000},
                {"id": "analyzing", "label": "分析中", "duration": 1500},
                {"id": "generating", "label": "生成回答", "duration": 800}
            ],
            "animations": {
                "dots": "...",
                "spinner": "⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏",
                "progress": "████░░░░░░"
            }
        }
    
    def create_confidence_display(self, response, confidence_score):
        """信心度顯示"""
        if confidence_score > 0.9:
            indicator = "✅ 高信心度"
            color = "green"
            disclaimer = None
        elif confidence_score > 0.7:
            indicator = "⚠️ 中等信心度"
            color = "yellow"
            disclaimer = "此回答可能不完全準確，建議進一步確認"
        else:
            indicator = "⚡ 低信心度"
            color = "red"
            disclaimer = "此回答僅供參考，強烈建議人工確認"
        
        return {
            "type": "confidence_display",
            "score": confidence_score,
            "indicator": indicator,
            "color": color,
            "disclaimer": disclaimer,
            "show_sources": confidence_score < 0.8  # 低信心時顯示來源
        }
    
    def create_quick_fix_component(self, response):
        """快速修正元件"""
        return {
            "type": "quick_fix",
            "actions": [
                {
                    "id": "regenerate",
                    "label": "🔄 重新生成",
                    "hotkey": "Ctrl+R",
                    "action": "regenerate_response"
                },
                {
                    "id": "report",
                    "label": "🚩 回報問題",
                    "hotkey": "Ctrl+F",
                    "action": "report_issue"
                },
                {
                    "id": "edit",
                    "label": "✏️ 編輯回答",
                    "hotkey": "Ctrl+E",
                    "action": "edit_response"
                }
            ],
            "feedback_options": [
                "不準確",
                "不完整",
                "不相關",
                "語氣不當"
            ]
        }
    
    def create_streaming_ux(self):
        """串流 UX 設計"""
        return {
            "type": "streaming",
            "features": {
                "typing_effect": {
                    "enabled": True,
                    "speed": 50,  # 字/秒
                    "variable_speed": True  # 模擬真人打字節奏
                },
                "partial_render": {
                    "enabled": True,
                    "chunk_size": 10,  # 每 10 個字渲染一次
                    "markdown_preview": True
                },
                "interrupt": {
                    "enabled": True,
                    "hotkey": "Esc",
                    "show_stop_button": True
                },
                "progress": {
                    "show_token_count": True,
                    "show_estimated_time": True,
                    "show_cost": False  # 生產環境不顯示成本
                }
            }
        }
```

#### 8.3 部署檢查清單

```python
class DeploymentChecklist:
    """部署檢查清單"""
    
    def __init__(self):
        self.checks = {
            "security": [
                ("prompt_injection_protection", self.check_prompt_injection),
                ("api_key_management", self.check_api_keys),
                ("rate_limiting", self.check_rate_limits),
                ("data_privacy", self.check_data_privacy)
            ],
            "reliability": [
                ("fallback_strategy", self.check_fallback),
                ("error_handling", self.check_error_handling),
                ("timeout_configuration", self.check_timeouts),
                ("retry_logic", self.check_retry_logic)
            ],
            "performance": [
                ("response_time", self.check_response_time),
                ("concurrent_requests", self.check_concurrency),
                ("cache_configuration", self.check_cache),
                ("model_optimization", self.check_model_optimization)
            ],
            "monitoring": [
                ("logging_setup", self.check_logging),
                ("metrics_collection", self.check_metrics),
                ("alerting_rules", self.check_alerts),
                ("dashboard_setup", self.check_dashboards)
            ],
            "cost": [
                ("budget_limits", self.check_budget_limits),
                ("cost_tracking", self.check_cost_tracking),
                ("optimization_rules", self.check_optimizations),
                ("billing_alerts", self.check_billing_alerts)
            ]
        }
    
    def run_all_checks(self):
        """執行所有檢查"""
        results = {}
        total_passed = 0
        total_checks = 0
        
        for category, checks in self.checks.items():
            results[category] = {}
            
            for check_name, check_func in checks:
                result = check_func()
                results[category][check_name] = result
                
                if result["passed"]:
                    total_passed += 1
                total_checks += 1
        
        # 生成報告
        return self.generate_report(results, total_passed, total_checks)
    
    def generate_report(self, results, passed, total):
        """生成部署報告"""
        score = passed / total * 100
        
        report = f"""
        ========================================
        📋 部署就緒檢查報告
        ========================================
        
        總分: {score:.1f}/100
        狀態: {'✅ 可以部署' if score > 80 else '❌ 需要改進'}
        
        詳細結果:
        """
        
        for category, checks in results.items():
            report += f"\n{category.upper()}:\n"
            for check, result in checks.items():
                status = "✅" if result["passed"] else "❌"
                report += f"  {status} {check}: {result['message']}\n"
        
        # 關鍵問題
        critical_issues = self.find_critical_issues(results)
        if critical_issues:
            report += "\n⚠️ 關鍵問題:\n"
            for issue in critical_issues:
                report += f"  - {issue}\n"
        
        return report
```

### 🔬 Lab 實作：完整客服 Bot 部署

```python
# Lab: 部署一個生產級客服 Bot
class ProductionCustomerBot:
    def __init__(self):
        self.metrics = ProductMetrics()
        self.ux = LLMSpecificUX()
        self.deployment = DeploymentChecklist()
        
        # 初始化各個組件
        self.setup_components()
    
    def setup_components(self):
        """設定所有組件"""
        # 核心功能
        self.llm_handler = LLMHandler()
        self.fallback_system = GracefulDegradation()
        self.cache = ResponseCache()
        
        # 監控
        self.monitoring = ObservabilitySystem()
        self.analytics = AnalyticsTracker()
        
        # UX 元件
        self.ui_components = {
            "thinking": self.ux.create_thinking_indicator(),
            "confidence": None,  # 動態生成
            "quick_fix": self.ux.create_quick_fix_component(None),
            "streaming": self.ux.create_streaming_ux()
        }
    
    async def handle_customer_query(self, query, session_id):
        """處理客戶查詢 - 完整流程"""
        
        # 1. 開始追蹤
        trace_id = self.monitoring.start_trace()
        
        try:
            # 2. 顯示思考指示器
            await self.show_thinking_process()
            
            # 3. 處理查詢（帶降級）
            response_data = await self.fallback_system.handle_request({
                "prompt": query,
                "session_id": session_id,
                "trace_id": trace_id
            })
            
            # 4. 計算信心度
            confidence = self.calculate_confidence(response_data)
            
            # 5. 準備 UX 元件
            ui_response = {
                "message": response_data["response"],
                "confidence": self.ux.create_confidence_display(
                    response_data["response"],
                    confidence
                ),
                "quick_actions": self.ux.create_quick_fix_component(
                    response_data["response"]
                ),
                "metadata": {
                    "strategy_used": response_data.get("strategy_used"),
                    "response_time": response_data.get("total_time"),
                    "trace_id": trace_id
                }
            }
            
            # 6. 記錄 metrics
            await self.record_metrics(response_data, confidence)
            
            # 7. 用戶反饋收集
            ui_response["feedback_widget"] = self.create_feedback_widget(trace_id)
            
            return ui_response
            
        except Exception as e:
            # 錯誤處理
            self.monitoring.record_error(e, trace_id)
            return self.create_error_response(e)
        
        finally:
            # 結束追蹤
            self.monitoring.end_trace(trace_id)
    
    async def show_thinking_process(self):
        """顯示思考過程"""
        stages = self.ui_components["thinking"]["stages"]
        
        for stage in stages:
            # 發送狀態更新到前端
            await self.send_status_update({
                "type": "thinking",
                "stage": stage["id"],
                "label": stage["label"]
            })
            
            # 模擬處理時間
            await asyncio.sleep(stage["duration"] / 1000)
    
    def calculate_confidence(self, response_data):
        """計算回應信心度"""
        factors = {
            "strategy_level": {
                "try_primary": 1.0,
                "try_cache": 0.9,
                "try_simple_model": 0.7,
                "try_template": 0.5,
                "try_human": 0.3
            },
            "response_time": 1.0 if response_data.get("total_time", 0) < 2 else 0.8,
            "cache_hit": 0.9 if response_data.get("cache_hit") else 1.0
        }
        
        strategy = response_data.get("strategy_used", "unknown")
        confidence = factors["strategy_level"].get(strategy, 0.5)
        confidence *= factors["response_time"]
        
        if response_data.get("cache_hit"):
            confidence *= factors["cache_hit"]
        
        return min(confidence, 1.0)
    
    async def record_metrics(self, response_data, confidence):
        """記錄各項指標"""
        metrics = {
            "latency_p50": response_data.get("total_time", 0) * 1000,
            "error_rate": 0 if response_data.get("success") else 1,
            "token_cost_per_request": response_data.get("cost", 0),
            "csat": confidence,  # 暫時用信心度代替
            "resolution_rate": 1 if confidence > 0.7 else 0,
            "fallback_rate": 1 if response_data.get("degradation_level", 0) > 0 else 0,
            "escalation_rate": 1 if response_data.get("strategy_used") == "try_human" else 0
        }
        
        # 記錄到監控系統
        for metric, value in metrics.items():
            self.monitoring.metrics.record(metric, value)
        
        # 計算健康分數
        health = self.metrics.calculate_health_score(metrics)
        
        # 如果健康分數低，發送警報
        if health["overall"] < 0.5:
            await self.send_alert(f"Health score critical: {health['overall']:.2f}")
    
    def create_feedback_widget(self, trace_id):
        """創建反饋小工具"""
        return {
            "type": "feedback",
            "trace_id": trace_id,
            "options": [
                {"emoji": "👍", "value": "helpful"},
                {"emoji": "👎", "value": "not_helpful"},
                {"emoji": "😕", "value": "confusing"},
                {"emoji": "🎯", "value": "accurate"},
                {"emoji": "❌", "value": "wrong"}
            ],
            "allow_text": True,
            "placeholder": "告訴我們如何改進..."
        }
    
    async def run_deployment_checks(self):
        """執行部署前檢查"""
        print("🚀 開始部署前檢查...\n")
        
        report = self.deployment.run_all_checks()
        print(report)
        
        # 模擬一些指標
        test_metrics = {
            "latency_p50": 800,
            "latency_p95": 2500,
            "error_rate": 0.005,
            "token_cost_per_request": 0.03,
            "csat": 0.88,
            "resolution_rate": 0.82,
            "fallback_rate": 0.08,
            "escalation_rate": 0.03
        }
        
        health = self.metrics.calculate_health_score(test_metrics)
        
        print("\n📊 系統健康報告:")
        print(f"  整體分數: {health['overall']:.2f} - {health['status']}")
        print(f"  技術分數: {health['technical']:.2f}")
        print(f"  業務分數: {health['business']:.2f}")
        
        if health["recommendations"]:
            print("\n💡 優化建議:")
            for rec in health["recommendations"]:
                print(f"  {rec}")
        
        return health["overall"] > 0.7

# 測試部署
bot = ProductionCustomerBot()

# 1. 執行部署檢查
ready = await bot.run_deployment_checks()

if ready:
    print("\n✅ 系統已準備就緒，可以部署！")
    
    # 2. 測試一些查詢
    test_queries = [
        "我的訂單什麼時候到？",
        "如何申請退款？",
        "你們的客服電話是多少？"
    ]
    
    for query in test_queries:
        print(f"\n測試查詢: {query}")
        response = await bot.handle_customer_query(query, "test_session")
        print(f"回應: {response['message'][:100]}...")
        print(f"信心度: {response['confidence']['indicator']}")
else:
    print("\n❌ 系統尚未準備好，請解決上述問題後再部署。")
```

### 💡 實戰心得
> 🚀 **UX 決定成敗**：技術再好，用戶體驗差就是失敗。花 50% 的時間在 UX 上不為過。

---

## 🎓 終極專題：Capstone Project

### 🎯 專題目標
整合所有模組知識，開發一個**真正可用的 LLM OS 應用**

### 📝 專題選項

#### Option 1: 智慧客服助手（推薦新手）
- **功能要求**：多輪對話、RAG、情緒管理、工單系統整合
- **技術挑戰**：處理情緒化客戶、多語言支援、降級策略
- **評分重點**：回應品質、處理效率、用戶滿意度

#### Option 2: 程式碼審查 Copilot
- **功能要求**：程式碼分析、bug 檢測、重構建議、最佳實踐檢查
- **技術挑戰**：理解程式碼上下文、提供可執行建議
- **評分重點**：建議準確性、實用性、整合便利性

#### Option 3: 智慧文件助理
- **功能要求**：文件上傳、智慧問答、摘要生成、引用追蹤
- **技術挑戰**：處理大文件、準確引用、多格式支援
- **評分重點**：檢索準確性、回答完整性、引用可靠性

### 📊 評分標準

| 面向 | 權重 | 評分要點 |
|------|------|---------|
| **技術實現** | 40% | API 整合完整性、錯誤處理完善度、效能優化程度、成本控制能力 |
| **產品完成度** | 40% | UX 流暢性、功能完整性、實際解決問題能力、部署就緒程度 |
| **創新性** | 20% | 場景創意、Prompt 設計巧思、UX 差異化、技術亮點 |

### 🏆 優秀專題範例

```python
class ExcellentCapstoneExample:
    """優秀專題範例：智慧 HR 助手"""
    
    def __init__(self):
        # 完整的模組整合
        self.modules = {
            "prompt_manager": PromptManager(),        # 模組 1
            "api_handler": LLMAPIPatterns(),         # 模組 2
            "context_engine": MemoryArchitecture(),  # 模組 3
            "rag_system": SimpleRAG(),               # 模組 4
            "product_logic": ProductDecisionFramework(), # 模組 5
            "workflow": WorkflowChain(),             # 模組 6
            "monitoring": ObservabilitySystem(),     # 模組 7
            "ux_system": LLMSpecificUX()            # 模組 8
        }
        
        # 創新點
        self.innovations = {
            "multi_persona": self.handle_multiple_personas,
            "proactive_suggestions": self.generate_proactive_suggestions,
            "sentiment_routing": self.route_by_sentiment
        }
    
    async def demo_flow(self):
        """展示完整流程"""
        
        # 場景：員工詢問請假政策
        query = "我想請病假，需要什麼手續？最近壓力很大..."
        
        # 1. 情緒檢測與路由（創新點）
        sentiment = await self.detect_sentiment(query)
        if sentiment["stress_level"] > 0.7:
            # 切換到關懷模式
            self.modules["prompt_manager"].switch_mode("empathetic")
        
        # 2. 多角色處理（創新點）
        response = await self.handle_with_persona(
            query,
            persona="hr_counselor"  # 不只是 HR，還有心理關懷
        )
        
        # 3. 主動建議（創新點）
        suggestions = await self.generate_proactive_suggestions(query)
        # 例如：建議 EAP 服務、推薦減壓資源
        
        # 4. 完整的產品化輸出
        return {
            "primary_response": response,
            "care_package": {
                "detected_stress": True,
                "eap_resources": self.get_eap_resources(),
                "anonymous_counseling": self.get_counseling_link()
            },
            "next_steps": suggestions,
            "follow_up_scheduled": self.schedule_follow_up(query)
        }
```

---

## 🚀 學習路徑總結

### 🎯 快速通道（4 週兼職學習）

**Week 1-2: 基礎速成**
- Day 1-3: Prompt Engineering + 版本管理
- Day 4-5: API 模式 + UX 基礎
- Day 6-7: 簡單 RAG + Context 管理
- **產出**: 能對話的 Bot Demo

**Week 3: 核心實戰**
- Day 1-2: 產品思維 + 成本優化
- Day 3-4: Workflow 設計
- Day 5-7: 可靠性基礎
- **產出**: 有 fallback 的穩定 Bot

**Week 4: 專題衝刺**
- Day 1-5: 開發完整專題
- Day 6-7: 部署與優化
- **產出**: 可部署的 MVP

### 🎓 完整通道（8 週深度學習）

按照完整 8 個模組循序漸進，每週一個模組，最後用 2 週完成專題。

---

## 💼 職涯發展建議

### 📈 技能成長路線圖

```
初級（0-6個月）
├── 掌握 Prompt Engineering
├── 熟悉主流 LLM API
└── 能開發簡單 Bot

中級（6-18個月）
├── 精通 RAG 和 Context 管理  
├── 設計複雜 Workflow
├── 優化成本和性能
└── 領導小型專案

高級（18個月+）
├── 架構大型 LLM 系統
├── 制定技術標準
├── 培訓團隊
└── 推動創新

專家（3年+）
├── 定義產品策略
├── 跨團隊協作
├── 開源貢獻
└── 行業影響力
```

## 🎯 立即行動計劃

### 今天就開始（Day 1）

```python
# Step 1: 設定環境
pip install openai langchain pinecone-client

# Step 2: 第一個 Prompt Manager
class MyFirstPromptManager:
    def __init__(self):
        self.prompts = {
            "v1": "You are a helpful assistant.",
            "v2": "You are a helpful and empathetic assistant."
        }
        self.current = "v1"
    
    def get_prompt(self):
        return self.prompts[self.current]

# Step 3: 測試版本差異
manager = MyFirstPromptManager()
for version in ["v1", "v2"]:
    manager.current = version
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": manager.get_prompt()},
            {"role": "user", "content": "I'm feeling stressed"}
        ]
    )
    print(f"{version}: {response.choices[0].message.content}")
```

### 本週目標（Week 1）

- [ ] 完成模組 1-2 的學習
- [ ] 實作 3 個不同的 Prompt 策略
- [ ] 測試 3 種 UX 模式
- [ ] 建立第一個帶驗證的 API 呼叫

### 本月目標（Month 1）

- [ ] 完成模組 1-6
- [ ] 開發一個完整的 FAQ Bot
- [ ] 實現成本優化，降低 50% 開銷
- [ ] 部署到測試環境

---

## 📚 學習資源推薦

### 📖 必讀資源
- [OpenAI Cookbook](https://cookbook.openai.com/) - 官方最佳實踐
- [LangChain Docs](https://docs.langchain.com/) - 應用開發框架
- [Pinecone Learning Center](https://www.pinecone.io/learn/) - 向量資料庫入門

### 🎓 進階課程
- [Building LLM Apps with LangChain.js](https://www.deeplearning.ai/short-courses/building-applications-vector-databases/) - DeepLearning.AI
- [LLM Application Development](https://www.coursera.org/learn/llm-applications) - Coursera
- [Full Stack LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/) - FSDL

### 🛠 開發工具
- **Prompt 測試**: [Promptfoo](https://promptfoo.dev/), [Langfuse](https://langfuse.com/)
- **監控平台**: [Helicone](https://helicone.ai/), [Langsmith](https://smith.langchain.com/)
- **向量資料庫**: [Pinecone](https://pinecone.io/), [Weaviate](https://weaviate.io/)

### 👥 社群資源
- [r/LocalLLaMA](https://reddit.com/r/LocalLLaMA) - Reddit 社群
- [LLM Discord](https://discord.gg/llm) - Discord 討論群
- [AI Builder Club](https://aibuilder.club/) - 開發者社群

---

## 🏁 結語：成為市場需要的 LLM 應用工程師

還記得開頭提到的問題嗎？為什麼 90% 的 LLM 應用都只是「玩具」？

現在你有答案了：**因為缺乏應用工程的系統化思維。**

透過這 8 個模組的學習，你將掌握：
- ✅ 如何管理 Prompt 資產，而不只是寫 Prompt
- ✅ 如何設計降級策略，而不是祈禱 API 不會掛
- ✅ 如何優化成本效益，而不是燒錢做 Demo
- ✅ 如何打造用戶體驗，而不只是技術炫技

**市場現狀**：
- 每家企業都想導入 AI，但缺乏落地能力
- 大量職缺湧現，但合格人才稀缺
- 薪資持續上漲，機會窗口正在開啟

**你的機會**：
- 不需要 PhD，不需要懂模型原理
- 只要 2-3 個月，就能掌握核心技能
- 市場需求是系統工程師的 **5-10 倍**

### 🚀 最後的行動呼籲

**不要再等了。** 每天都有新的 LLM 應用上線，每天都有公司在找 LLM 應用工程師。

現在就開始：
1. **今天**：設定環境，寫第一個 Prompt Manager
2. **本週**：完成模組 1-2，做出 Demo
3. **本月**：開發第一個完整應用
4. **三個月後**：成為合格的 LLM 應用工程師

記住：**在 AI 時代，應用層才是主戰場。**

系統工程師建造引擎，但**應用工程師開車上路**。

問題是——**你準備好上路了嗎？**

---

**最後更新**：2025年8月  
**作者**：Ian Chou  
**聯絡方式**：i@wo94.com

---

### 🔗 相關文章推薦
- [🧭 建造你的 LLM OS：從 Prompt 到 Production 的完整建構指南](/blog/2025-08-llm-os-complete-guide) - LLM OS 系統架構深度指南
- [🚀 AI 編程助手比較：Claude vs ChatGPT vs Gemini 開發者實戰評測](/blog/2025-06-ai-coding-assistant-comparison-claude-chatgpt-gemini-developers) - AI 編程工具對比
- [📊 台灣軟體工程師就業市場分析：2025 年趨勢與機會](/blog/2025-06-taiwan-software-engineer-job-market-analysis) - 職涯發展參考
- [🔧 規格驅動開發：2025 年軟體開發的新典範](/blog/2025-07-specification-driven-development-2025-paradigm) - 新興開發模式