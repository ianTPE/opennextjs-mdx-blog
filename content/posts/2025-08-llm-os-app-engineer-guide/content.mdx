## ğŸ“‘ ç›®éŒ„

- [å‰è¨€ï¼šç‚ºä»€éº¼ 90% çš„ LLM æ‡‰ç”¨éƒ½åªæ˜¯ã€Œç©å…·ã€ï¼Ÿ](#å‰è¨€ç‚ºä»€éº¼-90-çš„-llm-æ‡‰ç”¨éƒ½åªæ˜¯ç©å…·)
- [LLM æ‡‰ç”¨å·¥ç¨‹å¸« vs ç³»çµ±å·¥ç¨‹å¸«](#-llm-æ‡‰ç”¨å·¥ç¨‹å¸«-vs-ç³»çµ±å·¥ç¨‹å¸«)
- [å­¸ç¿’ç›®æ¨™èˆ‡èƒ½åŠ›å‡ºå£](#-å­¸ç¿’ç›®æ¨™èˆ‡èƒ½åŠ›å‡ºå£)
- [å®Œæ•´èª²ç¨‹æ¶æ§‹ï¼ˆ8 é€±é€Ÿæˆï¼‰](#-å®Œæ•´èª²ç¨‹æ¶æ§‹8-é€±é€Ÿæˆ)
- [æ¨¡çµ„ 1ï¼šPrompt for Applications](#-æ¨¡çµ„-1prompt-for-applicationsç¬¬-1-é€±)
- [æ¨¡çµ„ 2ï¼šLLM API èˆ‡ UX æ•´åˆ](#-æ¨¡çµ„-2llm-api-èˆ‡-ux-æ•´åˆç¬¬-2-é€±)
- [æ¨¡çµ„ 3ï¼šContext Engineering å¿…ä¿®åŸºç¤](#-æ¨¡çµ„-3context-engineering-å¿…ä¿®åŸºç¤ç¬¬-3-é€±)
- [æ¨¡çµ„ 4ï¼šçŸ¥è­˜æ›è¼‰èˆ‡ RAG åŸºç¤](#-æ¨¡çµ„-4çŸ¥è­˜æ›è¼‰èˆ‡-rag-åŸºç¤ç¬¬-4-é€±)
- [æ¨¡çµ„ 5ï¼šç”¢å“æ€ç¶­ for LLM Apps](#-æ¨¡çµ„-5ç”¢å“æ€ç¶­-for-llm-appsç¬¬-5-é€±)
- [æ¨¡çµ„ 6ï¼šæ‡‰ç”¨ç´š Workflow](#-æ¨¡çµ„-6æ‡‰ç”¨ç´š-workflowç¬¬-6-é€±)
- [æ¨¡çµ„ 7ï¼šå¯é æ€§èˆ‡å¯è§€æ¸¬æ€§](#-æ¨¡çµ„-7å¯é æ€§èˆ‡å¯è§€æ¸¬æ€§ç¬¬-7-é€±)
- [æ¨¡çµ„ 8ï¼šæ‡‰ç”¨è½åœ°èˆ‡ UX å¼·åŒ–](#-æ¨¡çµ„-8æ‡‰ç”¨è½åœ°èˆ‡-ux-å¼·åŒ–ç¬¬-8-é€±)
- [çµ‚æ¥µå°ˆé¡Œï¼šCapstone Project](#-çµ‚æ¥µå°ˆé¡Œcapstone-project)
- [å­¸ç¿’è·¯å¾‘ç¸½çµ](#-å­¸ç¿’è·¯å¾‘ç¸½çµ)
- [è·æ¶¯ç™¼å±•å»ºè­°](#-è·æ¶¯ç™¼å±•å»ºè­°)
- [ç«‹å³è¡Œå‹•è¨ˆåŠƒ](#-ç«‹å³è¡Œå‹•è¨ˆåŠƒ)
- [å­¸ç¿’è³‡æºæ¨è–¦](#-å­¸ç¿’è³‡æºæ¨è–¦)

---

## å‰è¨€ï¼š90% çš„ LLM å°ˆæ¡ˆç‚ºä½•åœåœ¨ POC éšæ®µï¼Ÿ

æ ¹æ“š Gartner èˆ‡ McKinsey çš„è§€å¯Ÿï¼Œè¶…éå…«æˆçš„ LLM å°ˆæ¡ˆåœç•™åœ¨æ¦‚å¿µé©—è­‰ï¼ˆPOCï¼‰ï¼Œå¾æœªé€²å…¥æ­£å¼ç”Ÿç”¢ç’°å¢ƒã€‚

åŸå› ä¸åœ¨æŠ€è¡“ä¸å¯è¡Œï¼Œè€Œåœ¨æ–¼ï¼š

- ç¼ºä¹ç©©å®šæ€§è¨­è¨ˆï¼ˆnon-deterministic outputï¼‰
- æˆæœ¬ä¸å¯é æ¸¬ï¼ˆtoken usage æ³¢å‹•å¤§ï¼‰
- ç„¡æ³•èˆ‡ç¾æœ‰ç³»çµ±æ•´åˆï¼ˆdata pipeline, auth, audit logï¼‰
- ç¼ºå°‘ç›£æ§èˆ‡é™¤éŒ¯å·¥å…·

æ›å¥è©±èªªï¼šæˆ‘å€‘æ“…é•·ã€Œå±•ç¤ºèƒ½åŠ›ã€ï¼Œä½†ä¸æ“…é•·ã€Œå·¥ç¨‹åŒ–å»ºæ§‹å¾Œäº¤ä»˜ã€ã€‚

é€™é–€èª²çš„ç›®æ¨™ï¼Œæ˜¯å¡«è£œé€™é“é´»æºâ€”â€”
è®“ä½ å¾ã€Œæœƒç”¨ Token API çš„é–‹ç™¼è€…ã€ï¼Œè½‰å‹ç‚ºã€Œèƒ½äº¤ä»˜ç©©å®š LLM OS æ‡‰ç”¨çš„ç³»çµ±è¨­è¨ˆè€…ã€ã€‚

> ğŸ’¡ **å®šä½æ˜ç¢º**ï¼šä½ ä¸éœ€è¦ç†è§£ Transformer çš„æ•¸å­¸åŸç†ï¼Œä¹Ÿä¸éœ€è¦æœƒè¨“ç·´æ¨¡å‹ã€‚ä½ éœ€è¦çš„æ˜¯æŠŠ LLM OS ç•¶ä½œã€Œæ–°æ™‚ä»£çš„ APIã€ï¼Œä¸¦ç”¨å·¥ç¨‹åŒ–çš„æ–¹å¼æ‰“é€ æ‡‰ç”¨ã€‚

---

## ğŸ“Š LLM æ‡‰ç”¨å·¥ç¨‹å¸« vs ç³»çµ±å·¥ç¨‹å¸«

| é¢å‘ | ç³»çµ±å·¥ç¨‹å¸« | **æ‡‰ç”¨å·¥ç¨‹å¸«ï¼ˆYouï¼‰** | 
|---|---|---|
| **é—œæ³¨é‡é»** | åº•å±¤æ¶æ§‹ã€æ¨¡å‹å„ªåŒ– | ç”¢å“é«”é©—ã€æ¥­å‹™åƒ¹å€¼ |
| **æ ¸å¿ƒæŠ€èƒ½** | RAG åŸç†ã€å‘é‡è³‡æ–™åº«ã€æ¨¡å‹éƒ¨ç½² | Prompt ç‰ˆæœ¬ç®¡ç†ã€UX è¨­è¨ˆã€æˆæœ¬æ§åˆ¶ |
| **å·¥ä½œå…§å®¹** | å»ºæ§‹ LLM åŸºç¤è¨­æ–½ | é–‹ç™¼ Botã€Copilotã€Agent |
| **è–ªè³‡ç¯„åœ** | $150K - $300K | **$100K - $200K** |
| **å¸‚å ´éœ€æ±‚** | æ¯å®¶å…¬å¸ 1-2 ä½ | **æ¯å®¶å…¬å¸ 5-10 ä½** |
| **å­¸ç¿’æ›²ç·š** | é™¡å³­ï¼ˆ6-12 å€‹æœˆï¼‰ | **é©ä¸­ï¼ˆ2-3 å€‹æœˆï¼‰** |

---

## ğŸ¯ å­¸ç¿’ç›®æ¨™èˆ‡èƒ½åŠ›å‡ºå£

å®Œæˆé€™å€‹èª²ç¨‹å¾Œï¼Œä½ å°‡èƒ½å¤ ï¼š

### âœ… æ ¸å¿ƒèƒ½åŠ›
- æŠŠä¸€å€‹ LLM æ‡‰ç”¨å¾**è¨­è¨ˆ â†’ API â†’ UX â†’ ä¸Šç·š**
- è™•ç† **80% çš„ç”¢å“å•é¡Œ**ï¼ˆä¸ç©©å®šã€æˆæœ¬é«˜ã€é«”é©—å·®ï¼‰
- å…·å‚™**ç”¢å“æ€ç¶­**ï¼ŒçŸ¥é“ä½•æ™‚è©²ç”¨/ä¸è©²ç”¨ LLM

### ğŸš€ å¯é–‹ç™¼çš„æ‡‰ç”¨é¡å‹
1. **æ™ºæ…§å®¢æœ Bot**ï¼šèƒ½è™•ç†è¤‡é›œè©¢å•ã€å¤šè¼ªå°è©±ã€æƒ…ç·’ç®¡ç†
2. **æ–‡ä»¶åŠ©æ‰‹**ï¼šæ”¯æ´ RAGã€æ‘˜è¦ã€å•ç­”
3. **Coding Copilot**ï¼šç¨‹å¼ç¢¼ç”Ÿæˆã€è§£é‡‹ã€é‡æ§‹å»ºè­°
4. **Agent-like Apps**ï¼šå¤šæ­¥é©Ÿä»»å‹™ã€å·¥å…·å‘¼å«ã€è‡ªå‹•åŒ–æµç¨‹

### ğŸ“ˆ è·æ¶¯ç™¼å±•è·¯å¾‘
```mermaid
graph LR
    A[åˆç´šé–‹ç™¼è€…] --> B[LLM æ‡‰ç”¨å·¥ç¨‹å¸«]
    B --> C1[ç”¢å“ç¶“ç†]
    B --> C2[AI æ¶æ§‹å¸«]
    B --> C3[å‰µæ¥­/é¡§å•]
```

---

## ğŸ— å®Œæ•´èª²ç¨‹æ¶æ§‹ï¼ˆ8 é€±é€Ÿæˆï¼‰

### ğŸ“š èª²ç¨‹è¨­è¨ˆç†å¿µ
- **70% æ‡‰ç”¨å¯¦ä½œ**ï¼šç›´æ¥å‹•æ‰‹åšç”¢å“
- **30% ç³»çµ±å¿…ä¿®**ï¼šç†è§£å¿…è¦çš„åº•å±¤æ¦‚å¿µ
- **100% å¯¦æˆ°å°å‘**ï¼šæ¯å€‹æ¨¡çµ„éƒ½æœ‰å¯¦éš›ç”¢å‡º

---

## ğŸ“˜ æ¨¡çµ„ 1ï¼šPrompt for Applicationsï¼ˆç¬¬ 1 é€±ï¼‰

### ğŸ¯ å­¸ç¿’ç›®æ¨™
å¾ã€Œå¯« Promptã€å‡ç´šåˆ°ã€Œç®¡ç† Prompt è³‡ç”¢ã€

### ğŸ“š æ ¸å¿ƒå…§å®¹

#### 1.1 Prompt ç‰ˆæœ¬ç®¡ç†
```python
class PromptManager:
    """ä¼æ¥­ç´š Prompt ç®¡ç†ç³»çµ±"""
    def __init__(self):
        self.versions = {}
        self.active_version = "v1.0"
        self.feature_flags = FeatureFlags()
    
    def register_prompt(self, name, version, prompt_template):
        """è¨»å†Šæ–°ç‰ˆæœ¬çš„ Prompt"""
        key = f"{name}_{version}"
        self.versions[key] = {
            "template": prompt_template,
            "created_at": datetime.now(),
            "metrics": {"success_rate": 0, "avg_tokens": 0}
        }
    
    def get_prompt(self, name, user_id=None):
        """æ ¹æ“š Feature Flag è¿”å›å°æ‡‰ç‰ˆæœ¬"""
        if self.feature_flags.is_enabled("new_prompt_v2", user_id):
            return self.versions[f"{name}_v2.0"]["template"]
        return self.versions[f"{name}_{self.active_version}"]["template"]
```

#### 1.2 é€²éš Prompt æŠ€å·§å°æ¯”

| æŠ€å·§ | ä½¿ç”¨å ´æ™¯ | å„ªé» | ç¼ºé» | æˆæœ¬ |
|---|---|---|---|---|
| **Zero-shot** | ç°¡å–®ä»»å‹™ | å¿«é€Ÿã€ä¾¿å®œ | ä¸ç©©å®š | ğŸ’° |
| **Few-shot** | æ ¼å¼åŒ–è¼¸å‡º | ç©©å®šæ€§æå‡ | Token æ¶ˆè€—å¤š | ğŸ’°ğŸ’° |
| **Chain-of-Thought** | è¤‡é›œæ¨ç† | æº–ç¢ºåº¦é«˜ | å»¶é²å¢åŠ  | ğŸ’°ğŸ’°ğŸ’° |
| **ReAct** | å¤šæ­¥é©Ÿä»»å‹™ | å¯è§£é‡‹æ€§å¼· | å¯¦ä½œè¤‡é›œ | ğŸ’°ğŸ’°ğŸ’°ğŸ’° |

### ğŸ”¬ Lab å¯¦ä½œï¼šä¸‰ç‰ˆ Prompt æ¯”è¼ƒ

```python
# Lab: A/B/C æ¸¬è©¦ä¸åŒ Prompt ç­–ç•¥
class PromptExperiment:
    def __init__(self):
        self.prompts = {
            "baseline": """
                åˆ†æé€™å‰‡å®¢æˆ¶è©•è«–çš„æƒ…ç·’ï¼š{review}
                è¼¸å‡ºï¼špositive/negative/neutral
            """,
            
            "few_shot": """
                åˆ†æå®¢æˆ¶è©•è«–çš„æƒ…ç·’ã€‚
                
                ç¯„ä¾‹ï¼š
                - "ç”¢å“å¾ˆæ£’ï¼Œç‰©è¶…æ‰€å€¼ï¼" â†’ positive
                - "å®Œå…¨ä¸å€¼é€™å€‹åƒ¹éŒ¢" â†’ negative
                - "æ™®é€šï¼Œæ²’ä»€éº¼ç‰¹åˆ¥" â†’ neutral
                
                ç¾åœ¨åˆ†æï¼š{review}
                è¼¸å‡ºï¼špositive/negative/neutral
            """,
            
            "cot_optimized": """
                åˆ†æé€™å‰‡å®¢æˆ¶è©•è«–ï¼š{review}
                
                æ€è€ƒæ­¥é©Ÿï¼š
                1. è­˜åˆ¥æƒ…ç·’è©å½™ï¼ˆæ­£é¢/è² é¢/ä¸­æ€§ï¼‰
                2. è©•ä¼°æ•´é«”èªæ°£
                3. è€ƒæ…®ä¸Šä¸‹æ–‡å’Œåè«·
                4. ç¶œåˆåˆ¤æ–·
                
                åŸºæ–¼ä»¥ä¸Šåˆ†æï¼Œæƒ…ç·’æ˜¯ï¼špositive/negative/neutral
            """
        }
        
        self.feature_flags = FeatureFlags()
    
    def run_experiment(self, review, user_id):
        # æ ¹æ“š Feature Flag é¸æ“‡ç‰ˆæœ¬
        version = self.feature_flags.get_variant("sentiment_prompt", user_id)
        prompt = self.prompts[version].format(review=review)
        
        # åŸ·è¡Œä¸¦è¨˜éŒ„metrics
        start_time = time.time()
        result = llm.complete(prompt)
        latency = time.time() - start_time
        
        # è¨˜éŒ„å¯¦é©—æ•¸æ“š
        self.log_metrics(version, result, latency)
        return result

# å¯¦éš›æ¸¬è©¦
experiment = PromptExperiment()
reviews = [
    "çˆ›é€äº†ï¼Œå®Œå…¨æ˜¯è©é¨™ï¼",
    "é‚„è¡Œå§ï¼Œä¸éåƒ¹æ ¼æœ‰é»è²´",
    "è¶…ç´šæ¨è–¦ï¼æ”¹è®Šäº†æˆ‘çš„ç”Ÿæ´»ï¼"
]

for review in reviews:
    results = {}
    for version in ["baseline", "few_shot", "cot_optimized"]:
        results[version] = experiment.run_experiment(review, f"test_user_{version}")
    
    print(f"Review: {review}")
    print(f"Results: {results}")
    print(f"ä¸€è‡´æ€§: {len(set(results.values())) == 1}")
```

### ğŸ’¡ å¯¦æˆ°å¿ƒå¾—
> âš ï¸ **å¸¸è¦‹éŒ¯èª¤**ï¼šéåº¦å„ªåŒ– Promptï¼Œå¿½ç•¥äº†æˆæœ¬å’Œå»¶é²ã€‚è¨˜ä½ï¼šFew-shot ä¸ä¸€å®šæ¯” Zero-shot å¥½ï¼Œè¦çœ‹å…·é«”å ´æ™¯ã€‚

---

## ğŸ“˜ æ¨¡çµ„ 2ï¼šLLM API èˆ‡ UX æ•´åˆï¼ˆç¬¬ 2 é€±ï¼‰

### ğŸ¯ å­¸ç¿’ç›®æ¨™
æŒæ¡ä¸‰ç¨® API æ¨¡å¼å’Œå°æ‡‰çš„ UX è¨­è¨ˆæ¨¡å¼

### ğŸ“š æ ¸å¿ƒå…§å®¹

#### 2.1 API å‘¼å«æ¨¡å¼æ·±åº¦è§£æ

```python
class LLMAPIPatterns:
    """ä¸‰ç¨® API æ¨¡å¼çš„æœ€ä½³å¯¦è¸"""
    
    @staticmethod
    def completion_mode(prompt):
        """è£œå…¨æ¨¡å¼ï¼šé©åˆå–®æ¬¡ç”Ÿæˆ"""
        response = openai.Completion.create(
            model="text-davinci-003",
            prompt=prompt,
            max_tokens=150,
            temperature=0.7
        )
        return response.choices[0].text
    
    @staticmethod
    def chat_mode(messages):
        """å°è©±æ¨¡å¼ï¼šé©åˆå¤šè¼ªäº¤äº’"""
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=messages,  # [{"role": "user", "content": "..."}]
            temperature=0.7
        )
        return response.choices[0].message.content
    
    @staticmethod
    async def streaming_mode(prompt, callback):
        """ä¸²æµæ¨¡å¼ï¼šé©åˆå³æ™‚å›é¥‹"""
        stream = await openai.ChatCompletion.acreate(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            stream=True
        )
        
        full_response = ""
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_response += content
                await callback(content)  # å³æ™‚é¡¯ç¤º
        
        return full_response
```

#### 2.2 ä¸‰ç¨®å°è©± UX æ¨¡å¼å¯¦æˆ°

```python
class ConversationUX:
    """å°è©± UX çš„ä¸‰ç¨®æ ¸å¿ƒæ¨¡å¼"""
    
    def command_mode(self, user_input):
        """
        Command æ¨¡å¼ï¼šç”¨æˆ¶ä¸‹æŒ‡ä»¤ï¼Œç³»çµ±åŸ·è¡Œ
        é©åˆï¼šæ˜ç¢ºä»»å‹™ã€å–®æ¬¡æ“ä½œ
        """
        # ç¯„ä¾‹ï¼š/translate <text> to <language>
        if user_input.startswith("/translate"):
            _, text, _, language = user_input.split()
            return self.execute_translation(text, language)
    
    def clarification_mode(self, user_input, context):
        """
        Clarification æ¨¡å¼ï¼šç³»çµ±ä¸»å‹•è©¢å•ï¼Œæ¾„æ¸…éœ€æ±‚
        é©åˆï¼šè¤‡é›œä»»å‹™ã€æ¨¡ç³ŠæŒ‡ä»¤
        """
        ambiguity_score = self.detect_ambiguity(user_input)
        
        if ambiguity_score > 0.7:
            clarifying_questions = [
                "æ‚¨æ˜¯æŒ‡ A é‚„æ˜¯ Bï¼Ÿ",
                "éœ€è¦åŒ…å« X å—ï¼Ÿ",
                "æ™‚é–“ç¯„åœæ˜¯ï¼Ÿ"
            ]
            return {"needs_clarification": True, "questions": clarifying_questions}
        
        return {"needs_clarification": False, "response": self.process(user_input)}
    
    def suggestion_mode(self, user_input, history):
        """
        Suggestion æ¨¡å¼ï¼šç³»çµ±æä¾›å»ºè­°é¸é …
        é©åˆï¼šæ¢ç´¢æ€§ä»»å‹™ã€æ–°æ‰‹å¼•å°
        """
        response = self.generate_response(user_input)
        
        # æ™ºæ…§å»ºè­°ä¸‹ä¸€æ­¥
        suggestions = [
            "ğŸ“Š æŸ¥çœ‹è©³ç´°æ•¸æ“š",
            "ğŸ“§ ç™¼é€å ±å‘Š",
            "ğŸ”„ ä¿®æ”¹åƒæ•¸é‡è©¦"
        ]
        
        return {
            "response": response,
            "suggestions": suggestions,
            "quick_actions": self.generate_quick_actions(context=history)
        }
```

#### 2.3 ä¿¡ä»»å±¤è¨­è¨ˆï¼ˆé—œéµä½†å¸¸è¢«å¿½ç•¥ï¼‰

```python
class OutputValidator:
    """LLM è¼¸å‡ºçš„ä¿¡ä»»å±¤"""
    
    def __init__(self):
        self.validators = {
            "email": re.compile(r'^[\w\.-]+@[\w\.-]+\.\w+$'),
            "phone": re.compile(r'^\+?1?\d{9,15}$'),
            "json": self.validate_json,
            "sql": self.validate_sql_safety
        }
    
    def validate_json(self, output):
        """JSON æ ¼å¼é©—è­‰"""
        try:
            parsed = json.loads(output)
            # é¡å¤–çš„ schema é©—è­‰
            return jsonschema.validate(parsed, self.schema)
        except:
            return False
    
    def validate_sql_safety(self, sql):
        """SQL å®‰å…¨æ€§æª¢æŸ¥"""
        dangerous_keywords = ['DROP', 'DELETE', 'TRUNCATE', 'ALTER']
        sql_upper = sql.upper()
        
        for keyword in dangerous_keywords:
            if keyword in sql_upper:
                raise SecurityError(f"Dangerous SQL detected: {keyword}")
        
        return True
    
    def validate_and_fix(self, output, output_type):
        """é©—è­‰ä¸¦å˜—è©¦ä¿®å¾©"""
        if output_type not in self.validators:
            return output
        
        validator = self.validators[output_type]
        
        if isinstance(validator, re.Pattern):
            if not validator.match(output):
                # å˜—è©¦ä¿®å¾©ï¼ˆä¾‹å¦‚ï¼šæå– emailï¼‰
                fixed = self.extract_pattern(output, validator)
                if fixed:
                    return fixed
                raise ValidationError(f"Invalid {output_type}: {output}")
        else:
            # å‡½æ•¸é©—è­‰å™¨
            if not validator(output):
                raise ValidationError(f"Validation failed for {output_type}")
        
        return output
```

### ğŸ”¬ Lab å¯¦ä½œï¼šæ™ºæ…§å®¢æœ Bot ä¸‰ç¨®æ¨¡å¼

```python
# Lab: å¯¦ä½œä¸€å€‹æ”¯æ´ä¸‰ç¨® UX æ¨¡å¼çš„å®¢æœ Bot
class SmartCustomerServiceBot:
    def __init__(self):
        self.mode = "auto"  # auto, command, clarify, suggest
        self.validator = OutputValidator()
        self.context_manager = ContextManager()
    
    async def handle_message(self, message, user_id):
        # 1. æ¨¡å¼åˆ¤æ–·
        if message.startswith("/"):
            return await self.handle_command(message)
        
        # 2. æ¨¡ç³Šåº¦åˆ†æ
        ambiguity = self.analyze_ambiguity(message)
        if ambiguity > 0.6:
            return await self.handle_clarification(message)
        
        # 3. ä¸€èˆ¬è™•ç† + å»ºè­°
        response = await self.generate_response(message)
        suggestions = self.generate_suggestions(message, response)
        
        return {
            "mode": "suggestion",
            "response": response,
            "suggestions": suggestions,
            "typing_time": self.calculate_typing_time(response)
        }
    
    async def handle_command(self, command):
        """Command æ¨¡å¼è™•ç†"""
        parts = command.split()
        cmd = parts[0]
        
        commands = {
            "/status": self.check_order_status,
            "/refund": self.process_refund,
            "/escalate": self.escalate_to_human
        }
        
        if cmd in commands:
            return await commands[cmd](parts[1:])
        
        return "Unknown command. Type /help for available commands."
    
    async def handle_clarification(self, message):
        """Clarification æ¨¡å¼è™•ç†"""
        # æ™ºæ…§å•é¡Œç”Ÿæˆ
        questions = self.generate_clarifying_questions(message)
        
        return {
            "mode": "clarification",
            "message": "æˆ‘éœ€è¦æ›´å¤šè³‡è¨Šä¾†å¹«åŠ©æ‚¨ï¼š",
            "questions": questions,
            "quick_replies": [q["short"] for q in questions]
        }
    
    def calculate_typing_time(self, response):
        """æ¨¡æ“¬çœŸäººæ‰“å­—æ™‚é–“"""
        words = len(response.split())
        # å‡è¨­æ‰“å­—é€Ÿåº¦ï¼š40 words/min
        return min(max(words / 40 * 60 * 1000, 1000), 5000)  # 1-5ç§’

# å¯¦éš›æ¸¬è©¦
bot = SmartCustomerServiceBot()

test_messages = [
    "/status ORDER123",  # Command mode
    "æˆ‘çš„è¨‚å–®æ€éº¼äº†",     # Clarification mode
    "æˆ‘è¦é€€è²¨ï¼Œç”¢å“æœ‰å•é¡Œ" # Suggestion mode
]

for msg in test_messages:
    result = await bot.handle_message(msg, "user_123")
    print(f"Input: {msg}")
    print(f"Mode: {result.get('mode', 'default')}")
    print(f"Response: {result}")
    print("-" * 50)
```

### ğŸ’¡ å¯¦æˆ°å¿ƒå¾—
> ğŸ¯ **Pro Tip**ï¼šä¸è¦ä¸€é–‹å§‹å°±ç”¨ Streamingã€‚å…ˆç”¨ Completion æ¨¡å¼é©—è­‰é‚è¼¯ï¼Œç©©å®šå¾Œå†å‡ç´šåˆ° Streaming æ”¹å–„é«”é©—ã€‚

---

## ğŸ“˜ æ¨¡çµ„ 3ï¼šContext Engineering å¿…ä¿®åŸºç¤ï¼ˆç¬¬ 3 é€±ï¼‰

### ğŸ¯ å­¸ç¿’ç›®æ¨™
ç†è§£ä¸¦ç®¡ç† LLM çš„ã€Œè¨˜æ†¶ç³»çµ±ã€â€”â€”é€™æ˜¯ç³»çµ±å·¥ç¨‹å¸«çš„å¿…ä¿®ç°¡åŒ–ç‰ˆ

### ğŸ“š æ ¸å¿ƒå…§å®¹

#### 3.1 ä¸‰å±¤è¨˜æ†¶æ¶æ§‹

```python
class MemoryArchitecture:
    """LLM æ‡‰ç”¨çš„è¨˜æ†¶ç®¡ç†ç³»çµ±"""
    
    def __init__(self):
        # ä¸‰å±¤è¨˜æ†¶é«”ç³»
        self.system_prompt = SystemPrompt()      # é•·æœŸè¨˜æ†¶ï¼ˆä¸è®Šï¼‰
        self.user_context = UserContext()        # ä¸­æœŸè¨˜æ†¶ï¼ˆsessionï¼‰
        self.conversation = ConversationBuffer() # çŸ­æœŸè¨˜æ†¶ï¼ˆå°è©±ï¼‰
        
        # Token é ç®—ç®¡ç†
        self.token_budget = {
            "system": 1000,      # ç³»çµ±æç¤º
            "context": 2000,     # ç”¨æˆ¶ä¸Šä¸‹æ–‡
            "conversation": 5000, # å°è©±æ­·å²
            "response": 1000     # é ç•™çµ¦å›æ‡‰
        }
    
    def build_context(self, user_input):
        """æ§‹å»ºå®Œæ•´ä¸Šä¸‹æ–‡"""
        # 1. è¨ˆç®—å„éƒ¨åˆ† token
        system_tokens = self.count_tokens(self.system_prompt.get())
        
        # 2. å‹•æ…‹èª¿æ•´ç©ºé–“åˆ†é…
        remaining = 8000 - system_tokens - 1000  # ç¸½å…± 8Kï¼Œé ç•™ 1K
        
        # 3. æ™ºæ…§å£“ç¸®
        context = {
            "system": self.system_prompt.get(),
            "user_context": self.compress_context(
                self.user_context.get(), 
                max_tokens=remaining * 0.3
            ),
            "conversation": self.compress_conversation(
                self.conversation.get(),
                max_tokens=remaining * 0.7
            ),
            "current_input": user_input
        }
        
        return self.format_context(context)
```

#### 3.2 Context å£“ç¸®ç­–ç•¥

```python
class ContextCompressor:
    """ä¸Šä¸‹æ–‡å£“ç¸®å™¨"""
    
    def __init__(self):
        self.strategies = {
            "summarization": self.summarize,
            "sliding_window": self.sliding_window,
            "importance_sampling": self.importance_sampling
        }
    
    def summarize(self, messages, max_tokens):
        """æ‘˜è¦å£“ç¸®ï¼šé©åˆé•·å°è©±"""
        if self.count_tokens(messages) <= max_tokens:
            return messages
        
        # åˆ†çµ„æ‘˜è¦
        chunks = self.chunk_messages(messages, chunk_size=10)
        summaries = []
        
        for chunk in chunks[:-1]:  # ä¿ç•™æœ€æ–°çš„ä¸æ‘˜è¦
            summary = llm.complete(f"Summarize: {chunk}", max_tokens=100)
            summaries.append({
                "role": "system",
                "content": f"[Earlier conversation summary]: {summary}"
            })
        
        # ä¿ç•™æœ€æ–°æ¶ˆæ¯åŸæ–‡
        return summaries + chunks[-1]
    
    def sliding_window(self, messages, max_tokens):
        """æ»‘å‹•çª—å£ï¼šä¿ç•™æœ€è¿‘ N æ¢"""
        total_tokens = 0
        kept_messages = []
        
        # å¾æœ€æ–°çš„é–‹å§‹ä¿ç•™
        for msg in reversed(messages):
            msg_tokens = self.count_tokens(msg)
            if total_tokens + msg_tokens <= max_tokens:
                kept_messages.insert(0, msg)
                total_tokens += msg_tokens
            else:
                break
        
        # åŠ å…¥æ‘˜è¦æç¤º
        if len(kept_messages) < len(messages):
            kept_messages.insert(0, {
                "role": "system",
                "content": f"[{len(messages) - len(kept_messages)} earlier messages omitted]"
            })
        
        return kept_messages
    
    def importance_sampling(self, messages, max_tokens):
        """é‡è¦æ€§æ¡æ¨£ï¼šä¿ç•™é—œéµè¨Šæ¯"""
        # è¨ˆç®—æ¯æ¢è¨Šæ¯çš„é‡è¦æ€§åˆ†æ•¸
        scored_messages = []
        for msg in messages:
            score = self.calculate_importance(msg)
            scored_messages.append((score, msg))
        
        # æ’åºä¸¦é¸æ“‡
        scored_messages.sort(reverse=True)
        selected = []
        total_tokens = 0
        
        for score, msg in scored_messages:
            msg_tokens = self.count_tokens(msg)
            if total_tokens + msg_tokens <= max_tokens:
                selected.append(msg)
                total_tokens += msg_tokens
        
        # ä¿æŒæ™‚é–“é †åº
        return sorted(selected, key=lambda m: messages.index(m))
    
    def calculate_importance(self, message):
        """è¨ˆç®—è¨Šæ¯é‡è¦æ€§"""
        score = 0.0
        
        # é—œéµè©æ¬Šé‡
        keywords = ["æ±ºå®š", "é‡è¦", "å•é¡Œ", "éŒ¯èª¤", "ç¢ºèª"]
        for keyword in keywords:
            if keyword in message.get("content", ""):
                score += 0.2
        
        # ç”¨æˆ¶è¨Šæ¯æ¬Šé‡æ›´é«˜
        if message.get("role") == "user":
            score += 0.3
        
        # é•·åº¦æ‡²ç½°ï¼ˆå¤ªé•·çš„å¯èƒ½æ˜¯å»¢è©±ï¼‰
        length = len(message.get("content", ""))
        if length > 500:
            score -= 0.1
        
        # æ™‚é–“è¡°æ¸›
        # score *= time_decay_factor
        
        return score
```

#### 3.3 è¨˜æ†¶ç®¡ç†å¯¦æˆ°

```python
class ConversationMemoryManager:
    """å°è©±è¨˜æ†¶ç®¡ç†å™¨"""
    
    def __init__(self, model="gpt-4", max_tokens=8000):
        self.model = model
        self.max_tokens = max_tokens
        self.compressor = ContextCompressor()
        
        # åˆ†å±¤è¨˜æ†¶
        self.memories = {
            "episodic": [],      # äº‹ä»¶è¨˜æ†¶
            "semantic": {},      # èªç¾©è¨˜æ†¶
            "working": []        # å·¥ä½œè¨˜æ†¶
        }
    
    def add_turn(self, user_input, assistant_response):
        """æ·»åŠ ä¸€è¼ªå°è©±"""
        turn = {
            "user": user_input,
            "assistant": assistant_response,
            "timestamp": datetime.now(),
            "tokens": self.count_tokens(user_input + assistant_response)
        }
        
        # åŠ å…¥å·¥ä½œè¨˜æ†¶
        self.memories["working"].append(turn)
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦å£“ç¸®
        if self.get_total_tokens() > self.max_tokens * 0.8:
            self.compress()
        
        # æå–é‡è¦è³‡è¨Šåˆ°èªç¾©è¨˜æ†¶
        self.extract_semantic_memory(turn)
    
    def compress(self):
        """å£“ç¸®è¨˜æ†¶"""
        # å°‡èˆŠçš„å·¥ä½œè¨˜æ†¶è½‰ç‚ºäº‹ä»¶è¨˜æ†¶
        if len(self.memories["working"]) > 10:
            # æ‘˜è¦å‰ 5 è¼ª
            to_compress = self.memories["working"][:5]
            summary = self.create_summary(to_compress)
            
            self.memories["episodic"].append({
                "summary": summary,
                "original_turns": len(to_compress),
                "timestamp": to_compress[0]["timestamp"]
            })
            
            # ç§»é™¤å·²å£“ç¸®çš„
            self.memories["working"] = self.memories["working"][5:]
    
    def extract_semantic_memory(self, turn):
        """æå–èªç¾©è¨˜æ†¶ï¼ˆå¯¦é«”ã€é—œä¿‚ã€äº‹å¯¦ï¼‰"""
        # ç°¡åŒ–ç‰ˆå¯¦é«”æå–
        entities = self.extract_entities(turn["user"])
        for entity in entities:
            if entity not in self.memories["semantic"]:
                self.memories["semantic"][entity] = []
            
            self.memories["semantic"][entity].append({
                "context": turn["assistant"][:100],
                "timestamp": turn["timestamp"]
            })
    
    def get_relevant_context(self, query):
        """ç²å–ç›¸é—œä¸Šä¸‹æ–‡"""
        context_parts = []
        
        # 1. ç›¸é—œçš„èªç¾©è¨˜æ†¶
        relevant_entities = self.extract_entities(query)
        for entity in relevant_entities:
            if entity in self.memories["semantic"]:
                context_parts.append(
                    f"é—œæ–¼{entity}ï¼š{self.memories['semantic'][entity][-1]['context']}"
                )
        
        # 2. æœ€è¿‘çš„äº‹ä»¶è¨˜æ†¶
        if self.memories["episodic"]:
            latest_episode = self.memories["episodic"][-1]
            context_parts.append(f"æ—©æœŸå°è©±æ‘˜è¦ï¼š{latest_episode['summary']}")
        
        # 3. å®Œæ•´çš„å·¥ä½œè¨˜æ†¶
        for turn in self.memories["working"][-5:]:  # æœ€è¿‘ 5 è¼ª
            context_parts.append(f"User: {turn['user']}")
            context_parts.append(f"Assistant: {turn['assistant']}")
        
        return "\n".join(context_parts)
```

### ğŸ”¬ Lab å¯¦ä½œï¼šå¤šè¼ªå°è©±æ‘˜è¦ç³»çµ±

```python
# Lab: å¯¦ä½œä¸€å€‹æ™ºæ…§å°è©±æ‘˜è¦ç³»çµ±
class SmartConversationManager:
    def __init__(self):
        self.memory_manager = ConversationMemoryManager()
        self.token_limit = 4000
        
    async def chat(self, user_input, session_id):
        # 1. è¼‰å…¥ session è¨˜æ†¶
        context = self.memory_manager.get_relevant_context(user_input)
        
        # 2. æª¢æŸ¥ token æ•¸é‡
        context_tokens = self.count_tokens(context)
        if context_tokens > self.token_limit:
            # å‹•æ…‹å£“ç¸®
            context = self.compress_context(context)
        
        # 3. æ§‹å»º prompt
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æ™ºæ…§åŠ©ç†ã€‚"},
            {"role": "system", "content": f"[Context]: {context}"},
            {"role": "user", "content": user_input}
        ]
        
        # 4. ç”Ÿæˆå›æ‡‰
        response = await self.generate_response(messages)
        
        # 5. æ›´æ–°è¨˜æ†¶
        self.memory_manager.add_turn(user_input, response)
        
        # 6. é¡¯ç¤ºè¨˜æ†¶é«”ä½¿ç”¨ç‹€æ³
        memory_stats = {
            "working_memory_turns": len(self.memory_manager.memories["working"]),
            "episodic_memories": len(self.memory_manager.memories["episodic"]),
            "semantic_entities": len(self.memory_manager.memories["semantic"]),
            "total_tokens": self.memory_manager.get_total_tokens(),
            "compression_ratio": self.calculate_compression_ratio()
        }
        
        return {
            "response": response,
            "memory_stats": memory_stats
        }
    
    def compress_context(self, context):
        """æ™ºæ…§å£“ç¸®ä¸Šä¸‹æ–‡"""
        strategies = [
            ("summarization", 0.5),
            ("sliding_window", 0.3),
            ("importance_sampling", 0.2)
        ]
        
        # æ ¹æ“šå…§å®¹ç‰¹æ€§é¸æ“‡ç­–ç•¥
        if self.is_technical_discussion(context):
            # æŠ€è¡“è¨è«–ä¿ç•™æ›´å¤šç´°ç¯€
            return self.memory_manager.compressor.sliding_window(
                context, self.token_limit
            )
        else:
            # ä¸€èˆ¬å°è©±å¯ä»¥æ‘˜è¦
            return self.memory_manager.compressor.summarize(
                context, self.token_limit
            )

# æ¸¬è©¦é•·å°è©±
manager = SmartConversationManager()
session_id = "test_session_001"

conversation = [
    "æˆ‘æƒ³äº†è§£ä½ å€‘çš„é€€è²¨æ”¿ç­–",
    "æˆ‘ä¸Šé€±è²·äº†ä¸€å€‹ç”¢å“ï¼Œä½†æ˜¯æœ‰å•é¡Œ",
    "è¨‚å–®è™Ÿæ˜¯ ORD-12345",
    "ç”¢å“æ˜¯è—è‰²çš„è—ç‰™è€³æ©Ÿ",
    "å•é¡Œæ˜¯å·¦è€³å¸¸å¸¸æ–·ç·š",
    "æˆ‘å·²ç¶“è©¦éé‡ç½®äº†ï¼Œæ²’æœ‰ç”¨",
    "å¯ä»¥æ›è²¨å—ï¼Ÿ",
    "å¦‚æœä¸èƒ½æ›è²¨ï¼Œå¯ä»¥é€€æ¬¾å—ï¼Ÿ",
    "é€€æ¬¾éœ€è¦å¤šä¹…ï¼Ÿ",
    "éœ€è¦æ”¯ä»˜é‹è²»å—ï¼Ÿ"
]

for i, msg in enumerate(conversation):
    print(f"\n=== Turn {i+1} ===")
    result = await manager.chat(msg, session_id)
    print(f"User: {msg}")
    print(f"Assistant: {result['response'][:100]}...")
    print(f"Memory Stats: {result['memory_stats']}")
    
    # æ¨¡æ“¬è¨˜æ†¶é«”å£“åŠ›
    if i == 4:
        print("\n[è§¸ç™¼è¨˜æ†¶å£“ç¸®...]")
```

### ğŸ’¡ å¯¦æˆ°å¿ƒå¾—
> ğŸ’¾ **Memory is Money**ï¼šæ¯å€‹ token éƒ½æ˜¯æˆæœ¬ã€‚å­¸æœƒå£“ç¸®æ˜¯çœéŒ¢çš„é—œéµã€‚æˆ‘å€‘æ›¾ç¶“æŠŠä¸€å€‹å®¢æœç³»çµ±çš„æˆæœ¬é™ä½ 60%ï¼Œå°±æ˜¯å„ªåŒ–äº†è¨˜æ†¶ç®¡ç†ã€‚

---

## ğŸ“˜ æ¨¡çµ„ 4ï¼šçŸ¥è­˜æ›è¼‰èˆ‡ RAG åŸºç¤ï¼ˆç¬¬ 4 é€±ï¼‰

### ğŸ¯ å­¸ç¿’ç›®æ¨™
æŒæ¡ RAG çš„æ ¸å¿ƒæ¦‚å¿µå’Œå¤±æ•—è™•ç†â€”â€”ä¸éœ€è¦æ‡‚å‘é‡è³‡æ–™åº«åŸç†

### ğŸ“š æ ¸å¿ƒå…§å®¹

#### 4.1 ç°¡åŒ–ç‰ˆ RAG Pipeline

```python
class SimpleRAG:
    """æ‡‰ç”¨å·¥ç¨‹å¸«ç‰ˆçš„ RAG ç³»çµ±"""
    
    def __init__(self):
        # ä½¿ç”¨ç¾æˆçš„å‘é‡è³‡æ–™åº«æœå‹™
        self.vector_db = Pinecone(api_key="your-key")
        self.embedder = OpenAIEmbeddings()
        
        # å¤±æ•—è™•ç†ç­–ç•¥
        self.fallback_strategies = {
            "no_results": self.handle_no_results,
            "conflicting": self.handle_conflicts,
            "low_confidence": self.handle_low_confidence
        }
    
    def search(self, query, k=5):
        """åŸºç¤æª¢ç´¢"""
        # 1. ç”Ÿæˆ embeddingï¼ˆä¸éœ€è¦æ‡‚åŸç†ï¼‰
        query_embedding = self.embedder.embed(query)
        
        # 2. å‘é‡æª¢ç´¢
        results = self.vector_db.query(
            vector=query_embedding,
            top_k=k,
            include_metadata=True
        )
        
        # 3. è™•ç†æª¢ç´¢å¤±æ•—
        if not results or results[0].score < 0.7:
            return self.fallback_strategies["no_results"](query)
        
        # 4. æª¢æŸ¥è¡çª
        if self.has_conflicts(results):
            return self.fallback_strategies["conflicting"](results)
        
        return results
    
    def has_conflicts(self, results):
        """æª¢æ¸¬çµæœæ˜¯å¦è¡çª"""
        # ç°¡å–®è¦å‰‡ï¼šå¦‚æœå‰å…©å€‹çµæœçš„å…§å®¹çŸ›ç›¾
        if len(results) >= 2:
            doc1 = results[0].metadata['content']
            doc2 = results[1].metadata['content']
            
            # ç”¨ LLM æª¢æ¸¬çŸ›ç›¾
            prompt = f"""
            Document 1: {doc1}
            Document 2: {doc2}
            
            Do these documents contradict each other? (Yes/No)
            """
            
            response = llm.complete(prompt)
            return "yes" in response.lower()
        
        return False
```

#### 4.2 RAG å¤±æ•—è™•ç†çš„è—è¡“

```python
class RAGFailureHandler:
    """RAG å¤±æ•—è™•ç†å°ˆå®¶"""
    
    def handle_no_results(self, query):
        """æ‰¾ä¸åˆ°ç›¸é—œæ–‡ä»¶"""
        strategies = [
            self.try_rephrase,        # æ”¹å¯«æŸ¥è©¢
            self.try_decompose,       # æ‹†è§£æŸ¥è©¢
            self.try_generalize,      # æ³›åŒ–æŸ¥è©¢
            self.fallback_to_llm      # ç›´æ¥ç”¨ LLM
        ]
        
        for strategy in strategies:
            result = strategy(query)
            if result["success"]:
                return result["data"]
        
        # å„ªé›…å¤±æ•—
        return {
            "found": False,
            "message": "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è­˜åº«ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šã€‚è®“æˆ‘ç”¨ä¸€èˆ¬çŸ¥è­˜ä¾†å›ç­”...",
            "fallback_response": self.generate_general_response(query)
        }
    
    def try_rephrase(self, query):
        """æ”¹å¯«æŸ¥è©¢è©"""
        rephrases = [
            f"æ›å¥è©±èªªï¼š{query}",
            f"ç›¸é—œæ–¼ï¼š{query}",
            f"é—œæ–¼{self.extract_keywords(query)}"
        ]
        
        for rephrase in rephrases:
            results = self.search(rephrase)
            if results:
                return {"success": True, "data": results}
        
        return {"success": False}
    
    def handle_conflicts(self, conflicting_results):
        """è™•ç†çŸ›ç›¾çš„çµæœ"""
        # ç­–ç•¥ 1ï¼šé¡¯ç¤ºæ‰€æœ‰è§€é»
        response = "æˆ‘æ‰¾åˆ°äº†ä¸åŒçš„è³‡è¨Šï¼š\n"
        for i, result in enumerate(conflicting_results[:2]):
            response += f"\nè§€é» {i+1}ï¼š{result.metadata['content'][:200]}...\n"
        response += "\nè«‹æ³¨æ„é€™äº›è³‡è¨Šå¯èƒ½æœ‰æ‰€å·®ç•°ã€‚"
        
        # ç­–ç•¥ 2ï¼šè®“ LLM ç¶œåˆåˆ¤æ–·
        synthesis_prompt = f"""
        ä»¥ä¸‹æ˜¯é—œæ–¼åŒä¸€å•é¡Œçš„ä¸åŒè³‡æ–™ï¼š
        {[r.metadata['content'] for r in conflicting_results]}
        
        è«‹ç¶œåˆé€™äº›è³‡è¨Šï¼Œçµ¦å‡ºå¹³è¡¡çš„ç­”æ¡ˆã€‚
        """
        
        synthesized = llm.complete(synthesis_prompt)
        
        return {
            "has_conflicts": True,
            "all_views": response,
            "synthesis": synthesized
        }
    
    def handle_low_confidence(self, results):
        """è™•ç†ä½ä¿¡å¿ƒåº¦çµæœ"""
        disclaimer = "âš ï¸ ä»¥ä¸‹è³‡è¨Šå¯èƒ½ä¸å®Œå…¨æº–ç¢ºï¼Œåƒ…ä¾›åƒè€ƒï¼š\n\n"
        
        # åŠ å…¥ä¿¡å¿ƒåº¦æŒ‡ç¤º
        enriched_response = ""
        for result in results:
            confidence = result.score
            if confidence > 0.9:
                confidence_indicator = "âœ… é«˜åº¦ç›¸é—œ"
            elif confidence > 0.7:
                confidence_indicator = "âš ï¸ éƒ¨åˆ†ç›¸é—œ"
            else:
                confidence_indicator = "âŒ ç›¸é—œæ€§è¼ƒä½"
            
            enriched_response += f"{confidence_indicator}: {result.metadata['content']}\n\n"
        
        return disclaimer + enriched_response
```

#### 4.3 Chunking ç­–ç•¥ï¼ˆç°¡åŒ–ç‰ˆï¼‰

```python
class SimpleChunker:
    """æ–‡ä»¶åˆ‡å¡Šå™¨ - æ‡‰ç”¨å±¤é¢å°±å¤ äº†"""
    
    def __init__(self, chunk_size=500, overlap=50):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk_document(self, document):
        """åŸºç¤åˆ‡å¡Šç­–ç•¥"""
        chunks = []
        
        # ç­–ç•¥ 1ï¼šæŒ‰æ®µè½åˆ‡
        paragraphs = document.split('\n\n')
        
        current_chunk = ""
        for para in paragraphs:
            if len(current_chunk) + len(para) < self.chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # åŠ å…¥ overlap
        chunks_with_overlap = []
        for i, chunk in enumerate(chunks):
            if i > 0:
                # åŠ å…¥å‰ä¸€å¡Šçš„çµå°¾
                prev_end = chunks[i-1][-self.overlap:] if len(chunks[i-1]) > self.overlap else chunks[i-1]
                chunk = prev_end + "\n...\n" + chunk
            
            if i < len(chunks) - 1:
                # åŠ å…¥ä¸‹ä¸€å¡Šçš„é–‹é ­
                next_start = chunks[i+1][:self.overlap] if len(chunks[i+1]) > self.overlap else chunks[i+1]
                chunk = chunk + "\n...\n" + next_start
            
            chunks_with_overlap.append(chunk)
        
        return chunks_with_overlap
```

### ğŸ”¬ Lab å¯¦ä½œï¼šFAQ Bot with Fallback

```python
# Lab: å»ºç«‹ä¸€å€‹æœ‰å®Œå–„ fallback çš„ FAQ Bot
class SmartFAQBot:
    def __init__(self):
        self.rag = SimpleRAG()
        self.failure_handler = RAGFailureHandler()
        self.response_cache = {}
        
        # é è¼‰å…¥ FAQ
        self.load_faqs()
    
    def load_faqs(self):
        """è¼‰å…¥å¸¸è¦‹å•é¡Œ"""
        faqs = [
            {
                "question": "å¦‚ä½•é€€è²¨ï¼Ÿ",
                "answer": "æ‚¨å¯ä»¥åœ¨æ”¶åˆ°å•†å“å¾Œ 7 å¤©å…§ç”³è«‹é€€è²¨...",
                "keywords": ["é€€è²¨", "é€€æ¬¾", "return"]
            },
            {
                "question": "é‹è²»æ€éº¼ç®—ï¼Ÿ",
                "answer": "é‹è²»æ ¹æ“šåœ°å€å’Œé‡é‡è¨ˆç®—...",
                "keywords": ["é‹è²»", "é…é€", "shipping"]
            }
        ]
        
        for faq in faqs:
            # å‘é‡åŒ–ä¸¦å­˜å…¥è³‡æ–™åº«
            embedding = self.rag.embedder.embed(faq["question"])
            self.rag.vector_db.upsert(
                id=f"faq_{hash(faq['question'])}",
                vector=embedding,
                metadata=faq
            )
    
    async def answer(self, question):
        # 1. æª¢æŸ¥ cache
        if question in self.response_cache:
            return self.response_cache[question]
        
        # 2. RAG æª¢ç´¢
        results = self.rag.search(question, k=3)
        
        # 3. è™•ç†å„ç¨®æƒ…æ³
        if isinstance(results, dict):
            if not results.get("found", True):
                # æ²’æ‰¾åˆ°ï¼šä½¿ç”¨ fallback
                response = await self.generate_fallback_response(question)
            elif results.get("has_conflicts"):
                # æœ‰è¡çªï¼šé¡¯ç¤ºå¤šå€‹è§€é»
                response = self.format_conflicting_response(results)
            else:
                # ä½ä¿¡å¿ƒåº¦
                response = results
        else:
            # æ­£å¸¸æƒ…æ³ï¼šåŸºæ–¼æª¢ç´¢çµæœç”Ÿæˆç­”æ¡ˆ
            response = await self.generate_answer(question, results)
        
        # 4. å¿«å–çµæœ
        self.response_cache[question] = response
        
        # 5. è¨˜éŒ„ metrics
        self.log_metrics(question, results, response)
        
        return response
    
    async def generate_answer(self, question, search_results):
        """åŸºæ–¼æª¢ç´¢çµæœç”Ÿæˆç­”æ¡ˆ"""
        context = "\n".join([r.metadata['answer'] for r in search_results[:2]])
        
        prompt = f"""
        åŸºæ–¼ä»¥ä¸‹è³‡è¨Šå›ç­”ç”¨æˆ¶å•é¡Œï¼š
        
        è³‡è¨Šï¼š
        {context}
        
        ç”¨æˆ¶å•é¡Œï¼š{question}
        
        è¦æ±‚ï¼š
        1. å¦‚æœè³‡è¨Šè¶³å¤ ï¼Œç›´æ¥å›ç­”
        2. å¦‚æœè³‡è¨Šä¸è¶³ï¼Œèªªæ˜å“ªéƒ¨åˆ†ç„¡æ³•å›ç­”
        3. ä¿æŒå‹å–„å°ˆæ¥­çš„èªæ°£
        """
        
        return await llm.complete(prompt)
    
    async def generate_fallback_response(self, question):
        """ç”Ÿæˆ fallback å›æ‡‰"""
        # å¤šå±¤ fallback
        fallbacks = [
            # Level 1: å˜—è©¦ç”¨é€šç”¨çŸ¥è­˜
            lambda: llm.complete(f"ç”¨ä¸€èˆ¬çŸ¥è­˜å›ç­”ï¼š{question}"),
            
            # Level 2: æä¾›ç›¸é—œé€£çµ
            lambda: f"æˆ‘ç„¡æ³•ç›´æ¥å›ç­”æ‚¨çš„å•é¡Œï¼Œä½†æ‚¨å¯ä»¥æŸ¥çœ‹æˆ‘å€‘çš„å¹«åŠ©ä¸­å¿ƒï¼šhttps://help.example.com",
            
            # Level 3: è½‰äººå·¥
            lambda: "é€™å€‹å•é¡Œè¼ƒç‚ºè¤‡é›œï¼Œå»ºè­°æ‚¨è¯ç¹«å®¢æœäººå“¡ã€‚æ˜¯å¦éœ€è¦æˆ‘å¹«æ‚¨è½‰æ¥ï¼Ÿ"
        ]
        
        for fallback in fallbacks:
            try:
                return fallback()
            except:
                continue
        
        return "æŠ±æ­‰ï¼Œæˆ‘æš«æ™‚ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€‚"

# æ¸¬è©¦å„ç¨®å ´æ™¯
bot = SmartFAQBot()

test_cases = [
    "å¦‚ä½•é€€è²¨ï¼Ÿ",           # æ­£å¸¸åŒ¹é…
    "é€€è²¨æ”¿ç­–æ˜¯ä»€éº¼ï¼Ÿ",     # éœ€è¦æ”¹å¯«
    "ä½ å€‘è³£ç«ç®­å—ï¼Ÿ",       # å®Œå…¨ç„¡é—œ
    "é€€è²¨å’Œæ›è²¨çš„å€åˆ¥ï¼Ÿ",   # éƒ¨åˆ†åŒ¹é…
    "Can I return this?",   # å…¶ä»–èªè¨€
]

for question in test_cases:
    print(f"\nå•é¡Œï¼š{question}")
    answer = await bot.answer(question)
    print(f"å›ç­”ï¼š{answer[:200]}...")
    print("-" * 50)
```

### ğŸ’¡ å¯¦æˆ°å¿ƒå¾—
> ğŸ¯ **RAG çš„ç²¾é«“ä¸æ˜¯æª¢ç´¢ï¼Œè€Œæ˜¯å¤±æ•—è™•ç†**ã€‚ä¸€å€‹å¥½çš„ RAG ç³»çµ±ï¼Œ80% çš„ç¨‹å¼ç¢¼éƒ½åœ¨è™•ç†ã€Œæ²’æ‰¾åˆ°ã€æˆ–ã€Œæ‰¾éŒ¯äº†ã€çš„æƒ…æ³ã€‚

---

## ğŸ“˜ æ¨¡çµ„ 5ï¼šç”¢å“æ€ç¶­ for LLM Appsï¼ˆç¬¬ 5 é€±ï¼‰

### ğŸ¯ å­¸ç¿’ç›®æ¨™
åŸ¹é¤Šç”¢å“æ€ç¶­ï¼ŒçŸ¥é“ä½•æ™‚è©²ç”¨/ä¸è©²ç”¨ LLM

### ğŸ“š æ ¸å¿ƒå…§å®¹

#### 5.1 LLM çš„èƒ½åŠ›çŸ©é™£

| ä»»å‹™é¡å‹ | é©åˆç”¨ LLM | ä¸é©åˆç”¨ LLM | æ›¿ä»£æ–¹æ¡ˆ |
|---------|-----------|-------------|---------|
| **å‰µæ„ç”Ÿæˆ** | âœ… æ–‡æ¡ˆã€æ•…äº‹ã€é»å­ | âŒ éœ€è¦äº‹å¯¦æº–ç¢ºçš„å…§å®¹ | äººå·¥å‰µä½œ + LLM è¼”åŠ© |
| **æ ¼å¼è½‰æ›** | âœ… JSONâ†”è‡ªç„¶èªè¨€ | âŒ è¤‡é›œçš„è³‡æ–™è½‰æ› | å°ˆé–€çš„ Parser |
| **åˆ†é¡ä»»å‹™** | âœ… æƒ…æ„Ÿåˆ†æã€ä¸»é¡Œåˆ†é¡ | âŒ éœ€è¦ 100% æº–ç¢ºç‡ | å‚³çµ± ML æ¨¡å‹ |
| **è¨ˆç®—ä»»å‹™** | âœ… ç°¡å–®ä¼°ç®— | âŒ ç²¾ç¢ºè¨ˆç®— | Calculator API |
| **æœå°‹ä»»å‹™** | âœ… èªç¾©æœå°‹ | âŒ ç²¾ç¢ºåŒ¹é… | å‚³çµ±æœå°‹å¼•æ“ |
| **æ±ºç­–ä»»å‹™** | âœ… å»ºè­°ã€æ¨è–¦ | âŒ é—œéµæ±ºç­– | è¦å‰‡å¼•æ“ + äººå·¥ |

#### 5.2 æˆæœ¬-å»¶é²-é«”é©—ä¸‰è§’

```python
class ProductDecisionFramework:
    """ç”¢å“æ±ºç­–æ¡†æ¶"""
    
    def __init__(self):
        self.metrics = {
            "cost": {"weight": 0.3, "threshold": 0.1},      # $0.1 per request
            "latency": {"weight": 0.3, "threshold": 2000},  # 2 seconds
            "quality": {"weight": 0.4, "threshold": 0.85}   # 85% satisfaction
        }
    
    def evaluate_llm_fit(self, use_case):
        """è©•ä¼°æ˜¯å¦é©åˆç”¨ LLM"""
        score = 0
        reasons = []
        
        # æ­£é¢å› ç´ 
        if use_case["requires_creativity"]:
            score += 30
            reasons.append("âœ… éœ€è¦å‰µæ„ç”Ÿæˆ")
        
        if use_case["handles_natural_language"]:
            score += 25
            reasons.append("âœ… è™•ç†è‡ªç„¶èªè¨€")
        
        if use_case["needs_flexibility"]:
            score += 20
            reasons.append("âœ… éœ€è¦éˆæ´»æ€§")
        
        # è² é¢å› ç´ 
        if use_case["requires_deterministic"]:
            score -= 40
            reasons.append("âŒ éœ€è¦ç¢ºå®šæ€§çµæœ")
        
        if use_case["cost_sensitive"]:
            score -= 30
            reasons.append("âŒ æˆæœ¬æ•æ„Ÿ")
        
        if use_case["latency_critical"]:
            score -= 25
            reasons.append("âŒ å»¶é²é—œéµ")
        
        # æ±ºç­–
        recommendation = "USE_LLM" if score > 0 else "AVOID_LLM"
        
        return {
            "score": score,
            "recommendation": recommendation,
            "reasons": reasons,
            "alternative": self.suggest_alternative(use_case) if score < 0 else None
        }
    
    def suggest_alternative(self, use_case):
        """å»ºè­°æ›¿ä»£æ–¹æ¡ˆ"""
        if use_case["requires_deterministic"]:
            return "ä½¿ç”¨è¦å‰‡å¼•æ“æˆ–å‚³çµ±æ¼”ç®—æ³•"
        elif use_case["cost_sensitive"]:
            return "ä½¿ç”¨å¿«å–å±¤ + å°æ¨¡å‹"
        elif use_case["latency_critical"]:
            return "é ç”Ÿæˆ + Edge éƒ¨ç½²"
        else:
            return "æ··åˆæ–¹æ¡ˆï¼šé—œéµè·¯å¾‘ç”¨å‚³çµ±æ–¹æ³•ï¼Œè¼”åŠ©åŠŸèƒ½ç”¨ LLM"
```

#### 5.3 å„ªé›…é™ç´šç­–ç•¥

```python
class GracefulDegradation:
    """å„ªé›…é™ç´šç³»çµ±"""
    
    def __init__(self):
        self.strategies = [
            self.try_primary,      # ä¸»è¦ç­–ç•¥ï¼šå®Œæ•´ LLM
            self.try_cache,        # å¿«å–ç­–ç•¥
            self.try_simple_model, # ç°¡å–®æ¨¡å‹
            self.try_template,     # æ¨¡æ¿å›è¦†
            self.try_human         # äººå·¥ä»‹å…¥
        ]
        
        self.cache = ResponseCache()
        self.template_engine = TemplateEngine()
    
    async def handle_request(self, request):
        """å¤šå±¤é™ç´šè™•ç†"""
        start_time = time.time()
        
        for i, strategy in enumerate(self.strategies):
            try:
                # è¨­å®šè¶…æ™‚
                timeout = self.calculate_timeout(i)
                
                result = await asyncio.wait_for(
                    strategy(request),
                    timeout=timeout
                )
                
                if result["success"]:
                    # è¨˜éŒ„ä½¿ç”¨äº†å“ªå€‹ç­–ç•¥
                    result["strategy_used"] = strategy.__name__
                    result["degradation_level"] = i
                    result["total_time"] = time.time() - start_time
                    
                    return result
                    
            except asyncio.TimeoutError:
                print(f"Strategy {strategy.__name__} timeout")
                continue
            except Exception as e:
                print(f"Strategy {strategy.__name__} failed: {e}")
                continue
        
        # æ‰€æœ‰ç­–ç•¥éƒ½å¤±æ•—
        return self.final_fallback(request)
    
    async def try_primary(self, request):
        """ä¸»è¦ç­–ç•¥ï¼šå®Œæ•´ LLM è™•ç†"""
        response = await llm.complete(
            request["prompt"],
            model="gpt-4",
            temperature=0.7
        )
        
        return {
            "success": True,
            "response": response,
            "quality": "high"
        }
    
    async def try_cache(self, request):
        """å¿«å–ç­–ç•¥"""
        # èªç¾©ç›¸ä¼¼åº¦å¿«å–
        cached = self.cache.get_similar(request["prompt"], threshold=0.95)
        
        if cached:
            return {
                "success": True,
                "response": cached["response"],
                "quality": "cached",
                "cache_hit": True
            }
        
        return {"success": False}
    
    async def try_simple_model(self, request):
        """ä½¿ç”¨æ›´ç°¡å–®çš„æ¨¡å‹"""
        response = await llm.complete(
            request["prompt"],
            model="gpt-3.5-turbo",  # é™ç´šåˆ°æ›´å¿«æ›´ä¾¿å®œçš„æ¨¡å‹
            temperature=0.5,
            max_tokens=150
        )
        
        return {
            "success": True,
            "response": response,
            "quality": "medium"
        }
    
    async def try_template(self, request):
        """æ¨¡æ¿å›è¦†"""
        # æ„åœ–è­˜åˆ¥
        intent = self.classify_intent(request["prompt"])
        
        if intent in self.template_engine.templates:
            response = self.template_engine.generate(intent, request)
            return {
                "success": True,
                "response": response,
                "quality": "template"
            }
        
        return {"success": False}
    
    async def try_human(self, request):
        """è½‰äººå·¥"""
        # å‰µå»ºå·¥å–®
        ticket = self.create_support_ticket(request)
        
        response = f"""
        æ‚¨çš„å•é¡Œè¼ƒç‚ºè¤‡é›œï¼Œæˆ‘å·²ç¶“ç‚ºæ‚¨å‰µå»ºäº†å·¥å–® #{ticket['id']}ã€‚
        å®¢æœäººå“¡å°‡åœ¨ 30 åˆ†é˜å…§èˆ‡æ‚¨è¯ç¹«ã€‚
        
        æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥æ’¥æ‰“å®¢æœç†±ç·šï¼š400-XXX-XXXX
        """
        
        return {
            "success": True,
            "response": response,
            "quality": "human_escalation",
            "ticket": ticket
        }
    
    def calculate_timeout(self, level):
        """æ ¹æ“šé™ç´šç­‰ç´šè¨ˆç®—è¶…æ™‚æ™‚é–“"""
        timeouts = [3.0, 1.0, 0.5, 0.2, 0.1]  # é€ç´šé™ä½
        return timeouts[min(level, len(timeouts) - 1)]
```

### ğŸ”¬ Lab å¯¦ä½œï¼šé™ç´šç­–ç•¥è¨­è¨ˆèˆ‡æ¸¬è©¦

```python
# Lab: è¨­è¨ˆä¸¦æ¸¬è©¦é™ç´šç­–ç•¥
class DegradationTestBench:
    """é™ç´šç­–ç•¥æ¸¬è©¦å¹³å°"""
    
    def __init__(self):
        self.degradation = GracefulDegradation()
        self.metrics = {
            "success_rate": [],
            "response_time": [],
            "quality_score": [],
            "cost": []
        }
    
    async def simulate_load(self, requests_per_second=10, duration=60):
        """æ¨¡æ“¬è² è¼‰æ¸¬è©¦"""
        tasks = []
        
        for i in range(duration):
            for j in range(requests_per_second):
                # æ¨¡æ“¬ä¸åŒé¡å‹çš„è«‹æ±‚
                request = self.generate_request()
                
                # éš¨æ©Ÿæ³¨å…¥æ•…éšœ
                if random.random() < 0.1:  # 10% æ•…éšœç‡
                    request["inject_failure"] = True
                
                task = self.process_request(request)
                tasks.append(task)
            
            await asyncio.sleep(1)
        
        results = await asyncio.gather(*tasks)
        self.analyze_results(results)
    
    async def process_request(self, request):
        """è™•ç†å–®å€‹è«‹æ±‚"""
        start = time.time()
        
        # æ¨¡æ“¬ç¶²è·¯å•é¡Œ
        if request.get("inject_failure"):
            await asyncio.sleep(random.uniform(5, 10))  # è¶…æ™‚
        
        result = await self.degradation.handle_request(request)
        
        # è¨˜éŒ„ metrics
        self.metrics["response_time"].append(time.time() - start)
        self.metrics["success_rate"].append(1 if result["success"] else 0)
        self.metrics["quality_score"].append(
            self.calculate_quality_score(result)
        )
        self.metrics["cost"].append(
            self.calculate_cost(result)
        )
        
        return result
    
    def calculate_quality_score(self, result):
        """è¨ˆç®—å“è³ªåˆ†æ•¸"""
        quality_map = {
            "high": 1.0,
            "cached": 0.9,
            "medium": 0.7,
            "template": 0.5,
            "human_escalation": 0.3
        }
        return quality_map.get(result.get("quality", "unknown"), 0)
    
    def calculate_cost(self, result):
        """è¨ˆç®—æˆæœ¬"""
        cost_map = {
            "try_primary": 0.03,      # GPT-4
            "try_cache": 0.0001,      # å¹¾ä¹å…è²»
            "try_simple_model": 0.002, # GPT-3.5
            "try_template": 0,         # å…è²»
            "try_human": 5.0          # äººå·¥æˆæœ¬
        }
        
        strategy = result.get("strategy_used", "unknown")
        return cost_map.get(strategy, 0)
    
    def analyze_results(self, results):
        """åˆ†ææ¸¬è©¦çµæœ"""
        print("\n=== é™ç´šç­–ç•¥æ¸¬è©¦å ±å‘Š ===\n")
        
        # æˆåŠŸç‡
        success_rate = sum(self.metrics["success_rate"]) / len(self.metrics["success_rate"])
        print(f"âœ… æˆåŠŸç‡: {success_rate:.2%}")
        
        # éŸ¿æ‡‰æ™‚é–“
        avg_response = sum(self.metrics["response_time"]) / len(self.metrics["response_time"])
        p95_response = sorted(self.metrics["response_time"])[int(len(self.metrics["response_time"]) * 0.95)]
        print(f"â±ï¸ å¹³å‡éŸ¿æ‡‰æ™‚é–“: {avg_response:.2f}ç§’")
        print(f"â±ï¸ P95 éŸ¿æ‡‰æ™‚é–“: {p95_response:.2f}ç§’")
        
        # å“è³ªåˆ†æ•¸
        avg_quality = sum(self.metrics["quality_score"]) / len(self.metrics["quality_score"])
        print(f"â­ å¹³å‡å“è³ªåˆ†æ•¸: {avg_quality:.2f}/1.0")
        
        # æˆæœ¬
        total_cost = sum(self.metrics["cost"])
        avg_cost = total_cost / len(self.metrics["cost"])
        print(f"ğŸ’° ç¸½æˆæœ¬: ${total_cost:.2f}")
        print(f"ğŸ’° å¹³å‡æˆæœ¬: ${avg_cost:.4f}/request")
        
        # ç­–ç•¥ä½¿ç”¨åˆ†å¸ƒ
        strategy_counts = {}
        for result in results:
            strategy = result.get("strategy_used", "unknown")
            strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1
        
        print("\nğŸ“Š ç­–ç•¥ä½¿ç”¨åˆ†å¸ƒ:")
        for strategy, count in sorted(strategy_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(results) * 100
            print(f"  - {strategy}: {count} ({percentage:.1f}%)")
        
        # æˆæœ¬æ•ˆç›Šåˆ†æ
        cost_per_quality = total_cost / (avg_quality * len(results))
        print(f"\nğŸ’¡ æˆæœ¬æ•ˆç›Šæ¯”: ${cost_per_quality:.4f}/quality point")
        
        # å»ºè­°
        print("\nğŸ¯ å„ªåŒ–å»ºè­°:")
        if avg_response > 2:
            print("  - éŸ¿æ‡‰æ™‚é–“éé•·ï¼Œè€ƒæ…®å¢åŠ å¿«å–æˆ–ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹")
        if avg_quality < 0.7:
            print("  - å“è³ªåˆ†æ•¸åä½ï¼Œè€ƒæ…®èª¿æ•´é™ç´šé–¾å€¼")
        if avg_cost > 0.01:
            print("  - æˆæœ¬åé«˜ï¼Œå¢åŠ å¿«å–å‘½ä¸­ç‡æˆ–ä½¿ç”¨æ›´å¤šæ¨¡æ¿")

# åŸ·è¡Œæ¸¬è©¦
test_bench = DegradationTestBench()
await test_bench.simulate_load(requests_per_second=5, duration=30)
```

### ğŸ’¡ å¯¦æˆ°å¿ƒå¾—
> ğŸ¯ **é»ƒé‡‘æ³•å‰‡**ï¼šæ°¸é ä¸è¦è®“ LLM æˆç‚ºå–®é»æ•…éšœã€‚æ¯å€‹ LLM å‘¼å«éƒ½æ‡‰è©²æœ‰è‡³å°‘ 2 å±¤ fallbackã€‚

---

## ğŸ“˜ æ¨¡çµ„ 6ï¼šæ‡‰ç”¨ç´š Workflowï¼ˆç¬¬ 6 é€±ï¼‰

### ğŸ¯ å­¸ç¿’ç›®æ¨™
è¨­è¨ˆå¤šæ­¥é©Ÿä»»å‹™æµç¨‹ï¼Œè™•ç†çœŸå¯¦ä¸–ç•Œçš„é«’è³‡æ–™

### ğŸ“š æ ¸å¿ƒå…§å®¹

#### 6.1 Chain æ€ç¶­è¨­è¨ˆæ¨¡å¼

```python
class WorkflowChain:
    """å·¥ä½œæµéˆè¨­è¨ˆ"""
    
    def __init__(self):
        self.steps = []
        self.error_handlers = {}
        self.validators = {}
    
    def add_step(self, name, function, validator=None, error_handler=None):
        """æ·»åŠ æ­¥é©Ÿ"""
        self.steps.append({
            "name": name,
            "function": function
        })
        
        if validator:
            self.validators[name] = validator
        if error_handler:
            self.error_handlers[name] = error_handler
        
        return self  # æ”¯æ´éˆå¼å‘¼å«
    
    async def execute(self, input_data):
        """åŸ·è¡Œå·¥ä½œæµ"""
        context = {"input": input_data, "steps": {}}
        
        for step in self.steps:
            step_name = step["name"]
            print(f"åŸ·è¡Œæ­¥é©Ÿ: {step_name}")
            
            try:
                # åŸ·è¡Œæ­¥é©Ÿ
                result = await step["function"](context)
                
                # é©—è­‰çµæœ
                if step_name in self.validators:
                    is_valid = self.validators[step_name](result)
                    if not is_valid:
                        raise ValueError(f"Validation failed for {step_name}")
                
                # å„²å­˜çµæœ
                context["steps"][step_name] = result
                
            except Exception as e:
                # éŒ¯èª¤è™•ç†
                if step_name in self.error_handlers:
                    result = self.error_handlers[step_name](e, context)
                    context["steps"][step_name] = result
                else:
                    raise
        
        return context
```

#### 6.2 é«’è³‡æ–™è™•ç†å¤§å…¨

```python
class DirtyDataProcessor:
    """é«’è³‡æ–™è™•ç†å°ˆå®¶"""
    
    def __init__(self):
        self.cleaners = {
            "typo": self.fix_typos,
            "mixed_language": self.handle_mixed_language,
            "emotional": self.handle_emotional_input,
            "incomplete": self.handle_incomplete,
            "contradictory": self.handle_contradictory
        }
    
    def clean(self, dirty_input):
        """æ¸…ç†é«’è³‡æ–™"""
        cleaned = dirty_input
        issues_found = []
        
        # æª¢æ¸¬ä¸¦ä¿®å¾©å„ç¨®å•é¡Œ
        for issue_type, cleaner in self.cleaners.items():
            if self.detect_issue(cleaned, issue_type):
                cleaned = cleaner(cleaned)
                issues_found.append(issue_type)
        
        return {
            "original": dirty_input,
            "cleaned": cleaned,
            "issues_found": issues_found,
            "confidence": self.calculate_confidence(issues_found)
        }
    
    def fix_typos(self, text):
        """ä¿®æ­£éŒ¯å­—"""
        # ç°¡å–®çš„éŒ¯å­—ä¿®æ­£
        common_typos = {
            "teh": "the",
            "recieve": "receive",
            "occured": "occurred",
            "refund": "refund",  
            "cancle": "cancel"
        }
        
        for typo, correct in common_typos.items():
            text = text.replace(typo, correct)
        
        # ä½¿ç”¨ LLM ä¿®æ­£æ›´è¤‡é›œçš„éŒ¯å­—
        if self.has_potential_typos(text):
            prompt = f"""
            ä¿®æ­£ä»¥ä¸‹æ–‡å­—ä¸­çš„éŒ¯å­—ï¼Œä½†ä¿æŒåŸæ„ï¼š
            åŸæ–‡ï¼š{text}
            ä¿®æ­£å¾Œï¼š
            """
            text = llm.complete(prompt, temperature=0.1)
        
        return text
    
    def handle_mixed_language(self, text):
        """è™•ç†æ··åˆèªè¨€"""
        # æª¢æ¸¬èªè¨€
        languages = self.detect_languages(text)
        
        if len(languages) > 1:
            # ç­–ç•¥ 1ï¼šç¿»è­¯æˆä¸»è¦èªè¨€
            primary_lang = max(languages.items(), key=lambda x: x[1])[0]
            
            prompt = f"""
            å°‡ä»¥ä¸‹æ··åˆèªè¨€æ–‡å­—çµ±ä¸€ç¿»è­¯æˆ{primary_lang}ï¼š
            {text}
            """
            
            return llm.complete(prompt)
        
        return text
    
    def handle_emotional_input(self, text):
        """è™•ç†æƒ…ç·’åŒ–è¼¸å…¥"""
        emotion_score = self.detect_emotion(text)
        
        if emotion_score > 0.7:  # é«˜åº¦æƒ…ç·’åŒ–
            # æå–æ ¸å¿ƒè¨´æ±‚
            prompt = f"""
            ç”¨æˆ¶æƒ…ç·’æ¿€å‹•åœ°èªªï¼š{text}
            
            è«‹æå–ç”¨æˆ¶çš„æ ¸å¿ƒè¨´æ±‚ï¼ˆå¿½ç•¥æƒ…ç·’åŒ–è¡¨é”ï¼‰ï¼š
            """
            
            core_request = llm.complete(prompt)
            
            return {
                "original": text,
                "emotion_level": "high",
                "core_request": core_request,
                "suggested_response_tone": "empathetic"
            }
        
        return text
    
    def handle_incomplete(self, text):
        """è™•ç†ä¸å®Œæ•´è¼¸å…¥"""
        if len(text.split()) < 3:  # å¤ªçŸ­
            return {
                "original": text,
                "issue": "incomplete",
                "clarification_needed": True,
                "suggested_questions": [
                    "æ‚¨èƒ½è©³ç´°èªªæ˜ä¸€ä¸‹å—ï¼Ÿ",
                    "è«‹å•å…·é«”æ˜¯ä»€éº¼å•é¡Œï¼Ÿ",
                    "èƒ½æä¾›æ›´å¤šè³‡è¨Šå—ï¼Ÿ"
                ]
            }
        
        return text
```

#### 6.3 Agent-like Flow è¨­è¨ˆ

```python
class SimpleAgent:
    """ç°¡å–® Agent æµç¨‹"""
    
    def __init__(self):
        self.tools = {
            "search_order": self.search_order,
            "check_inventory": self.check_inventory,
            "calculate_shipping": self.calculate_shipping,
            "process_refund": self.process_refund
        }
        
        self.planner = TaskPlanner()
        self.executor = TaskExecutor()
    
    async def process(self, user_request):
        """è™•ç†ç”¨æˆ¶è«‹æ±‚"""
        
        # 1. ç†è§£æ„åœ–
        intent = await self.understand_intent(user_request)
        
        # 2. è¦åŠƒæ­¥é©Ÿ
        plan = await self.planner.create_plan(intent, self.tools.keys())
        
        # 3. åŸ·è¡Œè¨ˆåŠƒ
        results = []
        for step in plan["steps"]:
            result = await self.execute_step(step)
            results.append(result)
            
            # å‹•æ…‹èª¿æ•´è¨ˆåŠƒ
            if result.get("requires_replanning"):
                plan = await self.planner.replan(plan, results)
        
        # 4. ç¶œåˆçµæœ
        final_response = await self.synthesize_response(results)
        
        return final_response
    
    async def understand_intent(self, request):
        """ç†è§£ç”¨æˆ¶æ„åœ–"""
        prompt = f"""
        åˆ†æç”¨æˆ¶è«‹æ±‚ä¸¦è­˜åˆ¥æ„åœ–ï¼š
        
        ç”¨æˆ¶èªªï¼š{request}
        
        å¯èƒ½çš„æ„åœ–ï¼š
        - ORDER_STATUS: æŸ¥è©¢è¨‚å–®ç‹€æ…‹
        - REFUND_REQUEST: ç”³è«‹é€€æ¬¾
        - PRODUCT_INQUIRY: ç”¢å“è«®è©¢
        - SHIPPING_INFO: é‹é€è³‡è¨Š
        - COMPLAINT: æŠ•è¨´
        - OTHER: å…¶ä»–
        
        è¿”å›æ ¼å¼ï¼š
        {{
            "primary_intent": "...",
            "entities": {{...}},
            "confidence": 0.X
        }}
        """
        
        response = await llm.complete(prompt)
        return json.loads(response)
    
    async def execute_step(self, step):
        """åŸ·è¡Œå–®å€‹æ­¥é©Ÿ"""
        tool_name = step["tool"]
        params = step["params"]
        
        if tool_name not in self.tools:
            return {"error": f"Tool {tool_name} not found"}
        
        try:
            result = await self.tools[tool_name](**params)
            return {"success": True, "data": result}
        except Exception as e:
            return {"success": False, "error": str(e)}
```

### ğŸ”¬ Lab å¯¦ä½œï¼šè¨‚å–®æŸ¥è©¢åŠ©ç†

```python
# Lab: èƒ½è™•ç†æ··åˆèªè¨€å’Œæ¨¡ç³Šè¼¸å…¥çš„è¨‚å–®åŠ©ç†
class SmartOrderAssistant:
    def __init__(self):
        self.workflow = WorkflowChain()
        self.data_processor = DirtyDataProcessor()
        self.agent = SimpleAgent()
        
        # è¨­å®šå·¥ä½œæµ
        self.setup_workflow()
    
    def setup_workflow(self):
        """è¨­å®šå·¥ä½œæµç¨‹"""
        self.workflow \
            .add_step("clean_input", self.clean_input) \
            .add_step("extract_info", self.extract_order_info) \
            .add_step("search_order", self.search_order) \
            .add_step("check_status", self.check_order_status) \
            .add_step("generate_response", self.generate_response)
    
    async def clean_input(self, context):
        """æ­¥é©Ÿ 1ï¼šæ¸…ç†è¼¸å…¥"""
        raw_input = context["input"]
        
        # è™•ç†é«’è³‡æ–™
        cleaned = self.data_processor.clean(raw_input)
        
        # è™•ç†ç‰¹æ®Šæƒ…æ³
        if "mixed_language" in cleaned["issues_found"]:
            # çµ±ä¸€èªè¨€
            cleaned["cleaned"] = await self.translate_to_primary(cleaned["cleaned"])
        
        if "emotional" in cleaned["issues_found"]:
            # è¨˜éŒ„æƒ…ç·’ç‹€æ…‹
            context["emotional_state"] = "high"
            context["response_tone"] = "empathetic"
        
        return cleaned
    
    async def extract_order_info(self, context):
        """æ­¥é©Ÿ 2ï¼šæå–è¨‚å–®è³‡è¨Š"""
        cleaned_input = context["steps"]["clean_input"]["cleaned"]
        
        prompt = f"""
        å¾ä»¥ä¸‹æ–‡å­—ä¸­æå–è¨‚å–®ç›¸é—œè³‡è¨Šï¼š
        {cleaned_input}
        
        æå–ï¼š
        - order_id: è¨‚å–®è™Ÿï¼ˆå¦‚æœæœ‰ï¼‰
        - product_name: ç”¢å“åç¨±
        - issue: å•é¡Œæè¿°
        - date: ç›¸é—œæ—¥æœŸ
        
        å¦‚æœè³‡è¨Šä¸å®Œæ•´ï¼Œæ¨™è¨˜ needs_clarification: true
        """
        
        extracted = await llm.complete(prompt, response_format={"type": "json"})
        
        # é©—è­‰æå–çµæœ
        if not extracted.get("order_id"):
            # å˜—è©¦æ¨¡ç³ŠåŒ¹é…
            extracted["possible_orders"] = await self.fuzzy_search_orders(cleaned_input)
        
        return extracted
    
    async def search_order(self, context):
        """æ­¥é©Ÿ 3ï¼šæœå°‹è¨‚å–®"""
        order_info = context["steps"]["extract_info"]
        
        if order_info.get("order_id"):
            # ç²¾ç¢ºæœå°‹
            order = await self.db.find_order(order_info["order_id"])
        elif order_info.get("possible_orders"):
            # è®“ç”¨æˆ¶ç¢ºèª
            return {
                "found_multiple": True,
                "orders": order_info["possible_orders"],
                "needs_confirmation": True
            }
        else:
            # æ ¹æ“šå…¶ä»–è³‡è¨Šæœå°‹
            orders = await self.db.search_orders(
                customer_id=context.get("customer_id"),
                product=order_info.get("product_name"),
                date_range=self.parse_date_range(order_info.get("date"))
            )
            
            if len(orders) == 1:
                order = orders[0]
            elif len(orders) > 1:
                return {"found_multiple": True, "orders": orders}
            else:
                return {"found": False}
        
        return {"found": True, "order": order}
    
    async def generate_response(self, context):
        """æ­¥é©Ÿ 5ï¼šç”Ÿæˆå›æ‡‰"""
        order_result = context["steps"]["search_order"]
        
        # æ ¹æ“šä¸åŒæƒ…æ³ç”Ÿæˆå›æ‡‰
        if not order_result.get("found"):
            response = "æŠ±æ­‰ï¼Œæˆ‘æ‰¾ä¸åˆ°ç›¸é—œè¨‚å–®ã€‚è«‹ç¢ºèªè¨‚å–®è™Ÿæˆ–æä¾›æ›´å¤šè³‡è¨Šã€‚"
        elif order_result.get("found_multiple"):
            response = self.format_multiple_orders(order_result["orders"])
        else:
            order = order_result["order"]
            status = context["steps"]["check_status"]
            response = self.format_order_status(order, status)
        
        # æ ¹æ“šæƒ…ç·’ç‹€æ…‹èª¿æ•´èªæ°£
        if context.get("emotional_state") == "high":
            response = self.add_empathy(response)
        
        return response
    
    async def handle_user_input(self, user_input):
        """ä¸»å…¥å£ï¼šè™•ç†ç”¨æˆ¶è¼¸å…¥"""
        
        # æ¸¬è©¦å„ç¨®é«’è¼¸å…¥
        test_inputs = [
            "æˆ‘çš„è¨‚å–®ï¼ˆè¨‚å–®è™Ÿï¼šORD12345ï¼‰åˆ°å“ªäº†ï¼Ÿï¼Ÿï¼Ÿ",  # æ­£å¸¸
            "Check my order status ORDER12345 è¬è¬",      # æ··åˆèªè¨€
            "wtf æˆ‘çš„æ±è¥¿å‘¢ï¼ï¼ï¼éƒ½ä¸‰å¤©äº†ï¼ï¼ï¼",         # æƒ…ç·’åŒ–
            "è¨‚å–®",                                        # ä¸å®Œæ•´
            "æˆ‘ä¸Šé€±è²·çš„è—ç‰™è€³æ©Ÿæ€éº¼é‚„æ²’åˆ°",                # æ¨¡ç³Š
            "ORD1234... ä¸å°æ˜¯ ORD12346",                 # çŸ›ç›¾
        ]
        
        results = []
        for test_input in test_inputs:
            print(f"\nè™•ç†è¼¸å…¥: {test_input}")
            
            try:
                result = await self.workflow.execute(test_input)
                response = result["steps"]["generate_response"]
                
                print(f"æ¸…ç†å¾Œ: {result['steps']['clean_input']['cleaned']}")
                print(f"æå–è³‡è¨Š: {result['steps']['extract_info']}")
                print(f"å›æ‡‰: {response}")
                
                results.append({
                    "input": test_input,
                    "success": True,
                    "response": response
                })
                
            except Exception as e:
                print(f"éŒ¯èª¤: {e}")
                results.append({
                    "input": test_input,
                    "success": False,
                    "error": str(e)
                })
        
        # çµ±è¨ˆæˆåŠŸç‡
        success_rate = sum(1 for r in results if r["success"]) / len(results)
        print(f"\næˆåŠŸç‡: {success_rate:.1%}")
        
        return results

# åŸ·è¡Œæ¸¬è©¦
assistant = SmartOrderAssistant()
results = await assistant.handle_user_input("æ¸¬è©¦è¼¸å…¥")
```

### ğŸ’¡ å¯¦æˆ°å¿ƒå¾—
> ğŸ”§ **é«’è³‡æ–™æ˜¯å¸¸æ…‹ï¼Œä¸æ˜¯ä¾‹å¤–**ã€‚ä¸€å€‹ç”¢å“ç´šçš„ LLM æ‡‰ç”¨ï¼Œ50% çš„ç¨‹å¼ç¢¼éƒ½åœ¨è™•ç†å„ç¨®é‚Šç•Œæƒ…æ³ã€‚

---

## ğŸ“˜ æ¨¡çµ„ 7ï¼šå¯é æ€§èˆ‡å¯è§€æ¸¬æ€§ï¼ˆç¬¬ 7 é€±ï¼‰

### ğŸ¯ å­¸ç¿’ç›®æ¨™
å»ºç«‹ç›£æ§é«”ç³»ï¼Œå„ªåŒ–æˆæœ¬èˆ‡æ€§èƒ½

### ğŸ“š æ ¸å¿ƒå…§å®¹

#### 7.1 æˆæœ¬å·¥ç¨‹å¯¦æˆ°

```python
class CostEngineering:
    """æˆæœ¬å·¥ç¨‹ç³»çµ±"""
    
    def __init__(self):
        self.token_prices = {
            "gpt-4": {"input": 0.03, "output": 0.06},      # per 1K tokens
            "gpt-3.5-turbo": {"input": 0.001, "output": 0.002},
            "claude-3-opus": {"input": 0.015, "output": 0.075},
            "claude-3-sonnet": {"input": 0.003, "output": 0.015}
        }
        
        self.optimizers = {
            "batching": BatchingOptimizer(),
            "caching": CachingOptimizer(),
            "routing": ModelRouter(),
            "compression": PromptCompressor()
        }
    
    def optimize_request(self, request):
        """å„ªåŒ–å–®å€‹è«‹æ±‚"""
        original_cost = self.estimate_cost(request)
        optimized = request.copy()
        
        # 1. å£“ç¸® Prompt
        optimized["prompt"] = self.optimizers["compression"].compress(
            request["prompt"]
        )
        
        # 2. é¸æ“‡æœ€ä½³æ¨¡å‹
        optimized["model"] = self.optimizers["routing"].select_model(
            task_type=request["task_type"],
            quality_requirement=request.get("quality", 0.8),
            budget=request.get("budget", float('inf'))
        )
        
        # 3. æª¢æŸ¥å¿«å–
        cached = self.optimizers["caching"].get(optimized["prompt"])
        if cached:
            return {
                "response": cached,
                "cost": 0,
                "source": "cache",
                "savings": original_cost
            }
        
        # 4. æ‰¹æ¬¡è™•ç†
        if self.optimizers["batching"].should_batch(optimized):
            batch_id = self.optimizers["batching"].add_to_batch(optimized)
            return {
                "batch_id": batch_id,
                "estimated_wait": self.optimizers["batching"].estimate_wait(),
                "estimated_savings": original_cost * 0.3
            }
        
        final_cost = self.estimate_cost(optimized)
        
        return {
            "optimized_request": optimized,
            "original_cost": original_cost,
            "optimized_cost": final_cost,
            "savings": original_cost - final_cost,
            "savings_percentage": (original_cost - final_cost) / original_cost * 100
        }
    
    def estimate_cost(self, request):
        """ä¼°ç®—æˆæœ¬"""
        model = request["model"]
        prompt_tokens = self.count_tokens(request["prompt"])
        
        # ä¼°ç®—è¼¸å‡º tokensï¼ˆç¶“é©—å€¼ï¼‰
        estimated_output = prompt_tokens * 0.8
        
        input_cost = (prompt_tokens / 1000) * self.token_prices[model]["input"]
        output_cost = (estimated_output / 1000) * self.token_prices[model]["output"]
        
        return input_cost + output_cost
```

#### 7.2 å¯è§€æ¸¬æ€§æ¶æ§‹

```python
class ObservabilitySystem:
    """å¯è§€æ¸¬æ€§ç³»çµ±"""
    
    def __init__(self):
        self.metrics = MetricsCollector()
        self.tracer = DistributedTracer()
        self.logger = StructuredLogger()
        self.profiler = PerformanceProfiler()
        
        # é—œéµæŒ‡æ¨™
        self.key_metrics = {
            "latency": {"threshold": 2000, "unit": "ms"},
            "error_rate": {"threshold": 0.01, "unit": "%"},
            "token_cost": {"threshold": 0.1, "unit": "$"},
            "user_satisfaction": {"threshold": 0.85, "unit": "score"}
        }
    
    def trace_request(self, request_id):
        """è¿½è¹¤è«‹æ±‚å…¨æµç¨‹"""
        
        @self.tracer.span("llm_request")
        async def traced_execution():
            span = self.tracer.current_span()
            
            # è¨˜éŒ„è«‹æ±‚å±¬æ€§
            span.set_attributes({
                "request.id": request_id,
                "request.model": request.get("model"),
                "request.prompt_length": len(request.get("prompt", "")),
                "request.timestamp": datetime.now().isoformat()
            })
            
            try:
                # Prompt è™•ç†éšæ®µ
                with self.tracer.span("prompt_processing"):
                    prompt = await self.process_prompt(request)
                    span.set_attribute("prompt.tokens", self.count_tokens(prompt))
                
                # LLM å‘¼å«éšæ®µ
                with self.tracer.span("llm_call"):
                    start_time = time.time()
                    response = await self.call_llm(prompt)
                    latency = (time.time() - start_time) * 1000
                    
                    span.set_attributes({
                        "llm.latency_ms": latency,
                        "llm.response_tokens": self.count_tokens(response),
                        "llm.total_tokens": self.count_tokens(prompt + response)
                    })
                
                # å¾Œè™•ç†éšæ®µ
                with self.tracer.span("post_processing"):
                    final_response = await self.post_process(response)
                
                # è¨˜éŒ„æˆåŠŸmetrics
                self.metrics.record("request_success", 1)
                self.metrics.record("request_latency", latency)
                
                return final_response
                
            except Exception as e:
                # è¨˜éŒ„éŒ¯èª¤
                span.set_status("ERROR")
                span.set_attribute("error.message", str(e))
                
                self.metrics.record("request_error", 1)
                self.logger.error(f"Request failed: {e}", extra={
                    "request_id": request_id,
                    "error_type": type(e).__name__
                })
                
                raise
        
        return await traced_execution()
```

#### 7.3 A/B æ¸¬è©¦æ¡†æ¶

```python
class ABTestingFramework:
    """A/B æ¸¬è©¦æ¡†æ¶"""
    
    def __init__(self):
        self.experiments = {}
        self.results = defaultdict(lambda: {
            "impressions": 0,
            "conversions": 0,
            "total_latency": 0,
            "total_cost": 0,
            "errors": 0
        })
    
    def create_experiment(self, name, variants):
        """å‰µå»ºå¯¦é©—"""
        self.experiments[name] = {
            "variants": variants,
            "traffic_split": self.calculate_traffic_split(len(variants)),
            "created_at": datetime.now(),
            "status": "running"
        }
    
    async def run_variant(self, experiment_name, user_id):
        """åŸ·è¡Œè®Šé«”"""
        experiment = self.experiments[experiment_name]
        
        # åˆ†é…è®Šé«”
        variant = self.assign_variant(user_id, experiment)
        variant_config = experiment["variants"][variant]
        
        # åŸ·è¡Œä¸¦è¿½è¹¤
        start_time = time.time()
        try:
            # æ ¹æ“šè®Šé«”é…ç½®åŸ·è¡Œ
            result = await self.execute_variant(variant_config)
            
            # è¨˜éŒ„æˆåŠŸmetrics
            self.results[f"{experiment_name}_{variant}"]["impressions"] += 1
            self.results[f"{experiment_name}_{variant}"]["total_latency"] += (time.time() - start_time)
            
            # è¨ˆç®—æˆæœ¬
            cost = self.calculate_cost(variant_config, result)
            self.results[f"{experiment_name}_{variant}"]["total_cost"] += cost
            
            return result
            
        except Exception as e:
            self.results[f"{experiment_name}_{variant}"]["errors"] += 1
            raise
    
    def analyze_experiment(self, experiment_name):
        """åˆ†æå¯¦é©—çµæœ"""
        experiment = self.experiments[experiment_name]
        analysis = {}
        
        for variant in experiment["variants"]:
            key = f"{experiment_name}_{variant}"
            data = self.results[key]
            
            if data["impressions"] > 0:
                analysis[variant] = {
                    "conversion_rate": data["conversions"] / data["impressions"],
                    "avg_latency": data["total_latency"] / data["impressions"],
                    "avg_cost": data["total_cost"] / data["impressions"],
                    "error_rate": data["errors"] / data["impressions"],
                    "sample_size": data["impressions"]
                }
                
                # è¨ˆç®—çµ±è¨ˆé¡¯è‘—æ€§
                if len(analysis) > 1:
                    analysis[variant]["statistical_significance"] = \
                        self.calculate_significance(analysis)
        
        # æ¨è–¦ç²å‹è€…
        winner = self.recommend_winner(analysis)
        
        return {
            "analysis": analysis,
            "winner": winner,
            "confidence": self.calculate_confidence(analysis),
            "recommendation": self.generate_recommendation(analysis, winner)
        }
```

### ğŸ”¬ Lab å¯¦ä½œï¼šæˆæœ¬å„ªåŒ–å¯¦é©—

```python
# Lab: Streaming vs Batch å¯¦é©—
class StreamingVsBatchExperiment:
    def __init__(self):
        self.ab_test = ABTestingFramework()
        self.cost_engine = CostEngineering()
        self.observability = ObservabilitySystem()
        
        # å®šç¾©å¯¦é©—è®Šé«”
        self.setup_experiment()
    
    def setup_experiment(self):
        """è¨­å®šå¯¦é©—"""
        self.ab_test.create_experiment(
            name="streaming_vs_batch",
            variants={
                "streaming": {
                    "mode": "streaming",
                    "model": "gpt-3.5-turbo",
                    "batch_size": 1,
                    "timeout": 30
                },
                "batch_small": {
                    "mode": "batch",
                    "model": "gpt-3.5-turbo",
                    "batch_size": 5,
                    "timeout": 10
                },
                "batch_large": {
                    "mode": "batch",
                    "model": "gpt-3.5-turbo",
                    "batch_size": 20,
                    "timeout": 5
                }
            }
        )
    
    async def run_test(self, num_requests=100):
        """åŸ·è¡Œæ¸¬è©¦"""
        tasks = []
        
        for i in range(num_requests):
            user_id = f"user_{i % 10}"  # æ¨¡æ“¬ 10 å€‹ç”¨æˆ¶
            
            # å‰µå»ºæ¸¬è©¦è«‹æ±‚
            request = {
                "prompt": f"Test prompt {i}: Explain quantum computing",
                "user_id": user_id,
                "timestamp": datetime.now()
            }
            
            # åŸ·è¡Œå¯¦é©—
            task = self.process_with_monitoring(request, user_id)
            tasks.append(task)
            
            # æ¨¡æ“¬çœŸå¯¦æµé‡
            await asyncio.sleep(random.uniform(0.1, 0.5))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆ†æçµæœ
        self.analyze_results(results)
    
    async def process_with_monitoring(self, request, user_id):
        """å¸¶ç›£æ§çš„è™•ç†"""
        
        # é–‹å§‹è¿½è¹¤
        with self.observability.tracer.span("experiment_request") as span:
            span.set_attribute("user_id", user_id)
            
            try:
                # åŸ·è¡Œè®Šé«”
                result = await self.ab_test.run_variant(
                    "streaming_vs_batch",
                    user_id
                )
                
                # è¨˜éŒ„è©³ç´°metrics
                self.observability.metrics.record("request_success", 1, {
                    "variant": result.get("variant"),
                    "user_id": user_id
                })
                
                # æ¨¡æ“¬ç”¨æˆ¶è¡Œç‚ºï¼ˆè½‰æ›ï¼‰
                if self.simulate_user_satisfaction(result):
                    variant_key = f"streaming_vs_batch_{result['variant']}"
                    self.ab_test.results[variant_key]["conversions"] += 1
                
                return result
                
            except Exception as e:
                self.observability.logger.error(f"Experiment failed: {e}")
                raise
    
    def simulate_user_satisfaction(self, result):
        """æ¨¡æ“¬ç”¨æˆ¶æ»¿æ„åº¦"""
        # åŸºæ–¼å»¶é²å’Œå“è³ªè¨ˆç®—æ»¿æ„åº¦
        latency_score = 1.0 if result.get("latency", 0) < 2000 else 0.5
        quality_score = result.get("quality", 0.8)
        
        satisfaction = (latency_score * 0.4 + quality_score * 0.6)
        
        # éš¨æ©Ÿæ±ºå®šæ˜¯å¦è½‰æ›
        return random.random() < satisfaction
    
    def analyze_results(self, results):
        """åˆ†ææ¸¬è©¦çµæœ"""
        analysis = self.ab_test.analyze_experiment("streaming_vs_batch")
        
        print("\n" + "="*60)
        print("ğŸ“Š A/B æ¸¬è©¦çµæœåˆ†æ")
        print("="*60)
        
        for variant, data in analysis["analysis"].items():
            print(f"\nğŸ”¹ è®Šé«”: {variant}")
            print(f"   è½‰æ›ç‡: {data['conversion_rate']:.2%}")
            print(f"   å¹³å‡å»¶é²: {data['avg_latency']:.2f}ç§’")
            print(f"   å¹³å‡æˆæœ¬: ${data['avg_cost']:.4f}")
            print(f"   éŒ¯èª¤ç‡: {data['error_rate']:.2%}")
            print(f"   æ¨£æœ¬æ•¸: {data['sample_size']}")
        
        print(f"\nğŸ† ç²å‹è€…: {analysis['winner']}")
        print(f"ğŸ“ˆ ä¿¡å¿ƒåº¦: {analysis['confidence']:.2%}")
        print(f"\nğŸ’¡ å»ºè­°: {analysis['recommendation']}")
        
        # æˆæœ¬ç¯€çœåˆ†æ
        self.calculate_cost_savings(analysis)
    
    def calculate_cost_savings(self, analysis):
        """è¨ˆç®—æˆæœ¬ç¯€çœ"""
        baseline = analysis["analysis"].get("streaming", {})
        
        print("\nğŸ’° æˆæœ¬ç¯€çœåˆ†æ:")
        for variant, data in analysis["analysis"].items():
            if variant != "streaming":
                savings = baseline.get("avg_cost", 0) - data["avg_cost"]
                savings_pct = (savings / baseline.get("avg_cost", 1)) * 100
                
                print(f"   {variant}: ç¯€çœ ${savings:.4f}/request ({savings_pct:.1f}%)")
                
                # æœˆåº¦é ä¼°
                monthly_requests = 100000
                monthly_savings = savings * monthly_requests
                print(f"      é ä¼°æœˆç¯€çœ: ${monthly_savings:,.2f}")

# åŸ·è¡Œå¯¦é©—
experiment = StreamingVsBatchExperiment()
await experiment.run_test(num_requests=100)
```

### ğŸ’¡ å¯¦æˆ°å¿ƒå¾—
> ğŸ“Š **æ•¸æ“šé©…å‹•æ±ºç­–**ï¼šä¸è¦æ†‘æ„Ÿè¦ºå„ªåŒ–ï¼Œè¦ç”¨ A/B æ¸¬è©¦è­‰æ˜ã€‚æˆ‘å€‘æ›¾ç¶“ä»¥ç‚º streaming ä¸€å®šæ›´å¥½ï¼Œçµæœç™¼ç¾æ‰¹æ¬¡è™•ç†åœ¨æŸäº›å ´æ™¯ä¸‹æˆæœ¬é™ä½ 70%ã€‚

---

## ğŸ“˜ æ¨¡çµ„ 8ï¼šæ‡‰ç”¨è½åœ°èˆ‡ UX å¼·åŒ–ï¼ˆç¬¬ 8 é€±ï¼‰

### ğŸ¯ å­¸ç¿’ç›®æ¨™
å°‡æ‡‰ç”¨æ¨å‘ç”Ÿç”¢ç’°å¢ƒï¼ŒåŠ å¼·ç”¨æˆ¶é«”é©—

### ğŸ“š æ ¸å¿ƒå…§å®¹

#### 8.1 ç”¢å“æŒ‡æ¨™é«”ç³»è¨­è¨ˆ

```python
class ProductMetrics:
    """ç”¢å“æŒ‡æ¨™ç³»çµ±"""
    
    def __init__(self):
        # æŠ€è¡“æŒ‡æ¨™
        self.technical_metrics = {
            "latency_p50": {"target": 1000, "unit": "ms", "weight": 0.2},
            "latency_p95": {"target": 3000, "unit": "ms", "weight": 0.1},
            "error_rate": {"target": 0.01, "unit": "%", "weight": 0.2},
            "token_cost_per_request": {"target": 0.05, "unit": "$", "weight": 0.2}
        }
        
        # æ¥­å‹™æŒ‡æ¨™
        self.business_metrics = {
            "csat": {"target": 0.85, "unit": "score", "weight": 0.3},  # å®¢æˆ¶æ»¿æ„åº¦
            "resolution_rate": {"target": 0.80, "unit": "%", "weight": 0.3},  # è§£æ±ºç‡
            "fallback_rate": {"target": 0.10, "unit": "%", "weight": 0.2},  # é™ç´šç‡
            "escalation_rate": {"target": 0.05, "unit": "%", "weight": 0.2}  # è½‰äººå·¥ç‡
        }
        
        # ç¶œåˆå¥åº·åˆ†æ•¸
        self.health_score_calculator = HealthScoreCalculator()
    
    def calculate_health_score(self, current_metrics):
        """è¨ˆç®—å¥åº·åˆ†æ•¸"""
        technical_score = 0
        business_score = 0
        
        # è¨ˆç®—æŠ€è¡“åˆ†æ•¸
        for metric, config in self.technical_metrics.items():
            current = current_metrics.get(metric, 0)
            target = config["target"]
            
            # åå‘æŒ‡æ¨™ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
            if metric in ["latency_p50", "latency_p95", "error_rate", "token_cost_per_request"]:
                score = min(1.0, target / max(current, 0.001))
            else:
                score = min(1.0, current / target)
            
            technical_score += score * config["weight"]
        
        # è¨ˆç®—æ¥­å‹™åˆ†æ•¸
        for metric, config in self.business_metrics.items():
            current = current_metrics.get(metric, 0)
            target = config["target"]
            
            # åå‘æŒ‡æ¨™
            if metric in ["fallback_rate", "escalation_rate"]:
                score = min(1.0, target / max(current, 0.001))
            else:
                score = min(1.0, current / target)
            
            business_score += score * config["weight"]
        
        # ç¶œåˆåˆ†æ•¸
        overall_score = technical_score * 0.4 + business_score * 0.6
        
        return {
            "overall": overall_score,
            "technical": technical_score,
            "business": business_score,
            "status": self.get_status(overall_score),
            "recommendations": self.generate_recommendations(current_metrics)
        }
    
    def get_status(self, score):
        """ç²å–ç‹€æ…‹"""
        if score >= 0.9:
            return "ğŸŸ¢ Excellent"
        elif score >= 0.7:
            return "ğŸŸ¡ Good"
        elif score >= 0.5:
            return "ğŸŸ  Needs Improvement"
        else:
            return "ğŸ”´ Critical"
    
    def generate_recommendations(self, metrics):
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []
        
        # å»¶é²å•é¡Œ
        if metrics.get("latency_p95", 0) > 5000:
            recommendations.append("âš¡ è€ƒæ…®ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹æˆ–å¢åŠ å¿«å–")
        
        # æˆæœ¬å•é¡Œ
        if metrics.get("token_cost_per_request", 0) > 0.1:
            recommendations.append("ğŸ’° å„ªåŒ– Prompt é•·åº¦æˆ–ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹")
        
        # æ»¿æ„åº¦å•é¡Œ
        if metrics.get("csat", 1) < 0.7:
            recommendations.append("ğŸ˜Š æ”¹å–„å›æ‡‰å“è³ªï¼Œè€ƒæ…®å¢åŠ äººå·¥å¯©æ ¸")
        
        # é™ç´šç‡å•é¡Œ
        if metrics.get("fallback_rate", 0) > 0.2:
            recommendations.append("ğŸ”§ æª¢æŸ¥ä¸»è¦æœå‹™ç©©å®šæ€§ï¼Œå„ªåŒ–é™ç´šç­–ç•¥")
        
        return recommendations
```

#### 8.2 LLM ç‰¹æœ‰çš„ UX æ¨¡å¼

```python
class LLMSpecificUX:
    """LLM ç‰¹æœ‰çš„ UX å…ƒä»¶"""
    
    def __init__(self):
        self.components = {
            "thinking_indicator": ThinkingIndicator(),
            "confidence_display": ConfidenceDisplay(),
            "quick_fix": QuickFixComponent(),
            "streaming_renderer": StreamingRenderer()
        }
    
    def create_thinking_indicator(self):
        """æ€è€ƒéç¨‹å¯è¦–åŒ–"""
        return {
            "type": "thinking_indicator",
            "stages": [
                {"id": "understanding", "label": "ç†è§£å•é¡Œ", "duration": 500},
                {"id": "searching", "label": "æœå°‹è³‡æ–™", "duration": 1000},
                {"id": "analyzing", "label": "åˆ†æä¸­", "duration": 1500},
                {"id": "generating", "label": "ç”Ÿæˆå›ç­”", "duration": 800}
            ],
            "animations": {
                "dots": "...",
                "spinner": "â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â ",
                "progress": "â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘"
            }
        }
    
    def create_confidence_display(self, response, confidence_score):
        """ä¿¡å¿ƒåº¦é¡¯ç¤º"""
        if confidence_score > 0.9:
            indicator = "âœ… é«˜ä¿¡å¿ƒåº¦"
            color = "green"
            disclaimer = None
        elif confidence_score > 0.7:
            indicator = "âš ï¸ ä¸­ç­‰ä¿¡å¿ƒåº¦"
            color = "yellow"
            disclaimer = "æ­¤å›ç­”å¯èƒ½ä¸å®Œå…¨æº–ç¢ºï¼Œå»ºè­°é€²ä¸€æ­¥ç¢ºèª"
        else:
            indicator = "âš¡ ä½ä¿¡å¿ƒåº¦"
            color = "red"
            disclaimer = "æ­¤å›ç­”åƒ…ä¾›åƒè€ƒï¼Œå¼·çƒˆå»ºè­°äººå·¥ç¢ºèª"
        
        return {
            "type": "confidence_display",
            "score": confidence_score,
            "indicator": indicator,
            "color": color,
            "disclaimer": disclaimer,
            "show_sources": confidence_score < 0.8  # ä½ä¿¡å¿ƒæ™‚é¡¯ç¤ºä¾†æº
        }
    
    def create_quick_fix_component(self, response):
        """å¿«é€Ÿä¿®æ­£å…ƒä»¶"""
        return {
            "type": "quick_fix",
            "actions": [
                {
                    "id": "regenerate",
                    "label": "ğŸ”„ é‡æ–°ç”Ÿæˆ",
                    "hotkey": "Ctrl+R",
                    "action": "regenerate_response"
                },
                {
                    "id": "report",
                    "label": "ğŸš© å›å ±å•é¡Œ",
                    "hotkey": "Ctrl+F",
                    "action": "report_issue"
                },
                {
                    "id": "edit",
                    "label": "âœï¸ ç·¨è¼¯å›ç­”",
                    "hotkey": "Ctrl+E",
                    "action": "edit_response"
                }
            ],
            "feedback_options": [
                "ä¸æº–ç¢º",
                "ä¸å®Œæ•´",
                "ä¸ç›¸é—œ",
                "èªæ°£ä¸ç•¶"
            ]
        }
    
    def create_streaming_ux(self):
        """ä¸²æµ UX è¨­è¨ˆ"""
        return {
            "type": "streaming",
            "features": {
                "typing_effect": {
                    "enabled": True,
                    "speed": 50,  # å­—/ç§’
                    "variable_speed": True  # æ¨¡æ“¬çœŸäººæ‰“å­—ç¯€å¥
                },
                "partial_render": {
                    "enabled": True,
                    "chunk_size": 10,  # æ¯ 10 å€‹å­—æ¸²æŸ“ä¸€æ¬¡
                    "markdown_preview": True
                },
                "interrupt": {
                    "enabled": True,
                    "hotkey": "Esc",
                    "show_stop_button": True
                },
                "progress": {
                    "show_token_count": True,
                    "show_estimated_time": True,
                    "show_cost": False  # ç”Ÿç”¢ç’°å¢ƒä¸é¡¯ç¤ºæˆæœ¬
                }
            }
        }
```

#### 8.3 éƒ¨ç½²æª¢æŸ¥æ¸…å–®

```python
class DeploymentChecklist:
    """éƒ¨ç½²æª¢æŸ¥æ¸…å–®"""
    
    def __init__(self):
        self.checks = {
            "security": [
                ("prompt_injection_protection", self.check_prompt_injection),
                ("api_key_management", self.check_api_keys),
                ("rate_limiting", self.check_rate_limits),
                ("data_privacy", self.check_data_privacy)
            ],
            "reliability": [
                ("fallback_strategy", self.check_fallback),
                ("error_handling", self.check_error_handling),
                ("timeout_configuration", self.check_timeouts),
                ("retry_logic", self.check_retry_logic)
            ],
            "performance": [
                ("response_time", self.check_response_time),
                ("concurrent_requests", self.check_concurrency),
                ("cache_configuration", self.check_cache),
                ("model_optimization", self.check_model_optimization)
            ],
            "monitoring": [
                ("logging_setup", self.check_logging),
                ("metrics_collection", self.check_metrics),
                ("alerting_rules", self.check_alerts),
                ("dashboard_setup", self.check_dashboards)
            ],
            "cost": [
                ("budget_limits", self.check_budget_limits),
                ("cost_tracking", self.check_cost_tracking),
                ("optimization_rules", self.check_optimizations),
                ("billing_alerts", self.check_billing_alerts)
            ]
        }
    
    def run_all_checks(self):
        """åŸ·è¡Œæ‰€æœ‰æª¢æŸ¥"""
        results = {}
        total_passed = 0
        total_checks = 0
        
        for category, checks in self.checks.items():
            results[category] = {}
            
            for check_name, check_func in checks:
                result = check_func()
                results[category][check_name] = result
                
                if result["passed"]:
                    total_passed += 1
                total_checks += 1
        
        # ç”Ÿæˆå ±å‘Š
        return self.generate_report(results, total_passed, total_checks)
    
    def generate_report(self, results, passed, total):
        """ç”Ÿæˆéƒ¨ç½²å ±å‘Š"""
        score = passed / total * 100
        
        report = f"""
        ========================================
        ğŸ“‹ éƒ¨ç½²å°±ç·’æª¢æŸ¥å ±å‘Š
        ========================================
        
        ç¸½åˆ†: {score:.1f}/100
        ç‹€æ…‹: {'âœ… å¯ä»¥éƒ¨ç½²' if score > 80 else 'âŒ éœ€è¦æ”¹é€²'}
        
        è©³ç´°çµæœ:
        """
        
        for category, checks in results.items():
            report += f"\n{category.upper()}:\n"
            for check, result in checks.items():
                status = "âœ…" if result["passed"] else "âŒ"
                report += f"  {status} {check}: {result['message']}\n"
        
        # é—œéµå•é¡Œ
        critical_issues = self.find_critical_issues(results)
        if critical_issues:
            report += "\nâš ï¸ é—œéµå•é¡Œ:\n"
            for issue in critical_issues:
                report += f"  - {issue}\n"
        
        return report
```

### ğŸ”¬ Lab å¯¦ä½œï¼šå®Œæ•´å®¢æœ Bot éƒ¨ç½²

```python
# Lab: éƒ¨ç½²ä¸€å€‹ç”Ÿç”¢ç´šå®¢æœ Bot
class ProductionCustomerBot:
    def __init__(self):
        self.metrics = ProductMetrics()
        self.ux = LLMSpecificUX()
        self.deployment = DeploymentChecklist()
        
        # åˆå§‹åŒ–å„å€‹çµ„ä»¶
        self.setup_components()
    
    def setup_components(self):
        """è¨­å®šæ‰€æœ‰çµ„ä»¶"""
        # æ ¸å¿ƒåŠŸèƒ½
        self.llm_handler = LLMHandler()
        self.fallback_system = GracefulDegradation()
        self.cache = ResponseCache()
        
        # ç›£æ§
        self.monitoring = ObservabilitySystem()
        self.analytics = AnalyticsTracker()
        
        # UX å…ƒä»¶
        self.ui_components = {
            "thinking": self.ux.create_thinking_indicator(),
            "confidence": None,  # å‹•æ…‹ç”Ÿæˆ
            "quick_fix": self.ux.create_quick_fix_component(None),
            "streaming": self.ux.create_streaming_ux()
        }
    
    async def handle_customer_query(self, query, session_id):
        """è™•ç†å®¢æˆ¶æŸ¥è©¢ - å®Œæ•´æµç¨‹"""
        
        # 1. é–‹å§‹è¿½è¹¤
        trace_id = self.monitoring.start_trace()
        
        try:
            # 2. é¡¯ç¤ºæ€è€ƒæŒ‡ç¤ºå™¨
            await self.show_thinking_process()
            
            # 3. è™•ç†æŸ¥è©¢ï¼ˆå¸¶é™ç´šï¼‰
            response_data = await self.fallback_system.handle_request({
                "prompt": query,
                "session_id": session_id,
                "trace_id": trace_id
            })
            
            # 4. è¨ˆç®—ä¿¡å¿ƒåº¦
            confidence = self.calculate_confidence(response_data)
            
            # 5. æº–å‚™ UX å…ƒä»¶
            ui_response = {
                "message": response_data["response"],
                "confidence": self.ux.create_confidence_display(
                    response_data["response"],
                    confidence
                ),
                "quick_actions": self.ux.create_quick_fix_component(
                    response_data["response"]
                ),
                "metadata": {
                    "strategy_used": response_data.get("strategy_used"),
                    "response_time": response_data.get("total_time"),
                    "trace_id": trace_id
                }
            }
            
            # 6. è¨˜éŒ„ metrics
            await self.record_metrics(response_data, confidence)
            
            # 7. ç”¨æˆ¶åé¥‹æ”¶é›†
            ui_response["feedback_widget"] = self.create_feedback_widget(trace_id)
            
            return ui_response
            
        except Exception as e:
            # éŒ¯èª¤è™•ç†
            self.monitoring.record_error(e, trace_id)
            return self.create_error_response(e)
        
        finally:
            # çµæŸè¿½è¹¤
            self.monitoring.end_trace(trace_id)
    
    async def show_thinking_process(self):
        """é¡¯ç¤ºæ€è€ƒéç¨‹"""
        stages = self.ui_components["thinking"]["stages"]
        
        for stage in stages:
            # ç™¼é€ç‹€æ…‹æ›´æ–°åˆ°å‰ç«¯
            await self.send_status_update({
                "type": "thinking",
                "stage": stage["id"],
                "label": stage["label"]
            })
            
            # æ¨¡æ“¬è™•ç†æ™‚é–“
            await asyncio.sleep(stage["duration"] / 1000)
    
    def calculate_confidence(self, response_data):
        """è¨ˆç®—å›æ‡‰ä¿¡å¿ƒåº¦"""
        factors = {
            "strategy_level": {
                "try_primary": 1.0,
                "try_cache": 0.9,
                "try_simple_model": 0.7,
                "try_template": 0.5,
                "try_human": 0.3
            },
            "response_time": 1.0 if response_data.get("total_time", 0) < 2 else 0.8,
            "cache_hit": 0.9 if response_data.get("cache_hit") else 1.0
        }
        
        strategy = response_data.get("strategy_used", "unknown")
        confidence = factors["strategy_level"].get(strategy, 0.5)
        confidence *= factors["response_time"]
        
        if response_data.get("cache_hit"):
            confidence *= factors["cache_hit"]
        
        return min(confidence, 1.0)
    
    async def record_metrics(self, response_data, confidence):
        """è¨˜éŒ„å„é …æŒ‡æ¨™"""
        metrics = {
            "latency_p50": response_data.get("total_time", 0) * 1000,
            "error_rate": 0 if response_data.get("success") else 1,
            "token_cost_per_request": response_data.get("cost", 0),
            "csat": confidence,  # æš«æ™‚ç”¨ä¿¡å¿ƒåº¦ä»£æ›¿
            "resolution_rate": 1 if confidence > 0.7 else 0,
            "fallback_rate": 1 if response_data.get("degradation_level", 0) > 0 else 0,
            "escalation_rate": 1 if response_data.get("strategy_used") == "try_human" else 0
        }
        
        # è¨˜éŒ„åˆ°ç›£æ§ç³»çµ±
        for metric, value in metrics.items():
            self.monitoring.metrics.record(metric, value)
        
        # è¨ˆç®—å¥åº·åˆ†æ•¸
        health = self.metrics.calculate_health_score(metrics)
        
        # å¦‚æœå¥åº·åˆ†æ•¸ä½ï¼Œç™¼é€è­¦å ±
        if health["overall"] < 0.5:
            await self.send_alert(f"Health score critical: {health['overall']:.2f}")
    
    def create_feedback_widget(self, trace_id):
        """å‰µå»ºåé¥‹å°å·¥å…·"""
        return {
            "type": "feedback",
            "trace_id": trace_id,
            "options": [
                {"emoji": "ğŸ‘", "value": "helpful"},
                {"emoji": "ğŸ‘", "value": "not_helpful"},
                {"emoji": "ğŸ˜•", "value": "confusing"},
                {"emoji": "ğŸ¯", "value": "accurate"},
                {"emoji": "âŒ", "value": "wrong"}
            ],
            "allow_text": True,
            "placeholder": "å‘Šè¨´æˆ‘å€‘å¦‚ä½•æ”¹é€²..."
        }
    
    async def run_deployment_checks(self):
        """åŸ·è¡Œéƒ¨ç½²å‰æª¢æŸ¥"""
        print("ğŸš€ é–‹å§‹éƒ¨ç½²å‰æª¢æŸ¥...\n")
        
        report = self.deployment.run_all_checks()
        print(report)
        
        # æ¨¡æ“¬ä¸€äº›æŒ‡æ¨™
        test_metrics = {
            "latency_p50": 800,
            "latency_p95": 2500,
            "error_rate": 0.005,
            "token_cost_per_request": 0.03,
            "csat": 0.88,
            "resolution_rate": 0.82,
            "fallback_rate": 0.08,
            "escalation_rate": 0.03
        }
        
        health = self.metrics.calculate_health_score(test_metrics)
        
        print("\nğŸ“Š ç³»çµ±å¥åº·å ±å‘Š:")
        print(f"  æ•´é«”åˆ†æ•¸: {health['overall']:.2f} - {health['status']}")
        print(f"  æŠ€è¡“åˆ†æ•¸: {health['technical']:.2f}")
        print(f"  æ¥­å‹™åˆ†æ•¸: {health['business']:.2f}")
        
        if health["recommendations"]:
            print("\nğŸ’¡ å„ªåŒ–å»ºè­°:")
            for rec in health["recommendations"]:
                print(f"  {rec}")
        
        return health["overall"] > 0.7

# æ¸¬è©¦éƒ¨ç½²
bot = ProductionCustomerBot()

# 1. åŸ·è¡Œéƒ¨ç½²æª¢æŸ¥
ready = await bot.run_deployment_checks()

if ready:
    print("\nâœ… ç³»çµ±å·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥éƒ¨ç½²ï¼")
    
    # 2. æ¸¬è©¦ä¸€äº›æŸ¥è©¢
    test_queries = [
        "æˆ‘çš„è¨‚å–®ä»€éº¼æ™‚å€™åˆ°ï¼Ÿ",
        "å¦‚ä½•ç”³è«‹é€€æ¬¾ï¼Ÿ",
        "ä½ å€‘çš„å®¢æœé›»è©±æ˜¯å¤šå°‘ï¼Ÿ"
    ]
    
    for query in test_queries:
        print(f"\næ¸¬è©¦æŸ¥è©¢: {query}")
        response = await bot.handle_customer_query(query, "test_session")
        print(f"å›æ‡‰: {response['message'][:100]}...")
        print(f"ä¿¡å¿ƒåº¦: {response['confidence']['indicator']}")
else:
    print("\nâŒ ç³»çµ±å°šæœªæº–å‚™å¥½ï¼Œè«‹è§£æ±ºä¸Šè¿°å•é¡Œå¾Œå†éƒ¨ç½²ã€‚")
```

### ğŸ’¡ å¯¦æˆ°å¿ƒå¾—
> ğŸš€ **UX æ±ºå®šæˆæ•—**ï¼šæŠ€è¡“å†å¥½ï¼Œç”¨æˆ¶é«”é©—å·®å°±æ˜¯å¤±æ•—ã€‚èŠ± 50% çš„æ™‚é–“åœ¨ UX ä¸Šä¸ç‚ºéã€‚

---

## ğŸ“ çµ‚æ¥µå°ˆé¡Œï¼šCapstone Project

### ğŸ¯ å°ˆé¡Œç›®æ¨™
æ•´åˆæ‰€æœ‰æ¨¡çµ„çŸ¥è­˜ï¼Œé–‹ç™¼ä¸€å€‹**çœŸæ­£å¯ç”¨çš„ LLM æ‡‰ç”¨**

### ğŸ“ å°ˆé¡Œé¸é …

#### Option 1: æ™ºæ…§å®¢æœåŠ©æ‰‹ï¼ˆæ¨è–¦æ–°æ‰‹ï¼‰
- **åŠŸèƒ½è¦æ±‚**ï¼šå¤šè¼ªå°è©±ã€RAGã€æƒ…ç·’ç®¡ç†ã€å·¥å–®ç³»çµ±æ•´åˆ
- **æŠ€è¡“æŒ‘æˆ°**ï¼šè™•ç†æƒ…ç·’åŒ–å®¢æˆ¶ã€å¤šèªè¨€æ”¯æ´ã€é™ç´šç­–ç•¥
- **è©•åˆ†é‡é»**ï¼šå›æ‡‰å“è³ªã€è™•ç†æ•ˆç‡ã€ç”¨æˆ¶æ»¿æ„åº¦

#### Option 2: ç¨‹å¼ç¢¼å¯©æŸ¥ Copilot
- **åŠŸèƒ½è¦æ±‚**ï¼šç¨‹å¼ç¢¼åˆ†æã€bug æª¢æ¸¬ã€é‡æ§‹å»ºè­°ã€æœ€ä½³å¯¦è¸æª¢æŸ¥
- **æŠ€è¡“æŒ‘æˆ°**ï¼šç†è§£ç¨‹å¼ç¢¼ä¸Šä¸‹æ–‡ã€æä¾›å¯åŸ·è¡Œå»ºè­°
- **è©•åˆ†é‡é»**ï¼šå»ºè­°æº–ç¢ºæ€§ã€å¯¦ç”¨æ€§ã€æ•´åˆä¾¿åˆ©æ€§

#### Option 3: æ™ºæ…§æ–‡ä»¶åŠ©ç†
- **åŠŸèƒ½è¦æ±‚**ï¼šæ–‡ä»¶ä¸Šå‚³ã€æ™ºæ…§å•ç­”ã€æ‘˜è¦ç”Ÿæˆã€å¼•ç”¨è¿½è¹¤
- **æŠ€è¡“æŒ‘æˆ°**ï¼šè™•ç†å¤§æ–‡ä»¶ã€æº–ç¢ºå¼•ç”¨ã€å¤šæ ¼å¼æ”¯æ´
- **è©•åˆ†é‡é»**ï¼šæª¢ç´¢æº–ç¢ºæ€§ã€å›ç­”å®Œæ•´æ€§ã€å¼•ç”¨å¯é æ€§

### ğŸ“Š è©•åˆ†æ¨™æº–

| é¢å‘ | æ¬Šé‡ | è©•åˆ†è¦é» |
|------|------|---------|
| **æŠ€è¡“å¯¦ç¾** | 40% | API æ•´åˆå®Œæ•´æ€§ã€éŒ¯èª¤è™•ç†å®Œå–„åº¦ã€æ•ˆèƒ½å„ªåŒ–ç¨‹åº¦ã€æˆæœ¬æ§åˆ¶èƒ½åŠ› |
| **ç”¢å“å®Œæˆåº¦** | 40% | UX æµæš¢æ€§ã€åŠŸèƒ½å®Œæ•´æ€§ã€å¯¦éš›è§£æ±ºå•é¡Œèƒ½åŠ›ã€éƒ¨ç½²å°±ç·’ç¨‹åº¦ |
| **å‰µæ–°æ€§** | 20% | å ´æ™¯å‰µæ„ã€Prompt è¨­è¨ˆå·§æ€ã€UX å·®ç•°åŒ–ã€æŠ€è¡“äº®é» |

### ğŸ† å„ªç§€å°ˆé¡Œç¯„ä¾‹

```python
class ExcellentCapstoneExample:
    """å„ªç§€å°ˆé¡Œç¯„ä¾‹ï¼šæ™ºæ…§ HR åŠ©æ‰‹"""
    
    def __init__(self):
        # å®Œæ•´çš„æ¨¡çµ„æ•´åˆ
        self.modules = {
            "prompt_manager": PromptManager(),        # æ¨¡çµ„ 1
            "api_handler": LLMAPIPatterns(),         # æ¨¡çµ„ 2
            "context_engine": MemoryArchitecture(),  # æ¨¡çµ„ 3
            "rag_system": SimpleRAG(),               # æ¨¡çµ„ 4
            "product_logic": ProductDecisionFramework(), # æ¨¡çµ„ 5
            "workflow": WorkflowChain(),             # æ¨¡çµ„ 6
            "monitoring": ObservabilitySystem(),     # æ¨¡çµ„ 7
            "ux_system": LLMSpecificUX()            # æ¨¡çµ„ 8
        }
        
        # å‰µæ–°é»
        self.innovations = {
            "multi_persona": self.handle_multiple_personas,
            "proactive_suggestions": self.generate_proactive_suggestions,
            "sentiment_routing": self.route_by_sentiment
        }
    
    async def demo_flow(self):
        """å±•ç¤ºå®Œæ•´æµç¨‹"""
        
        # å ´æ™¯ï¼šå“¡å·¥è©¢å•è«‹å‡æ”¿ç­–
        query = "æˆ‘æƒ³è«‹ç—…å‡ï¼Œéœ€è¦ä»€éº¼æ‰‹çºŒï¼Ÿæœ€è¿‘å£“åŠ›å¾ˆå¤§..."
        
        # 1. æƒ…ç·’æª¢æ¸¬èˆ‡è·¯ç”±ï¼ˆå‰µæ–°é»ï¼‰
        sentiment = await self.detect_sentiment(query)
        if sentiment["stress_level"] > 0.7:
            # åˆ‡æ›åˆ°é—œæ‡·æ¨¡å¼
            self.modules["prompt_manager"].switch_mode("empathetic")
        
        # 2. å¤šè§’è‰²è™•ç†ï¼ˆå‰µæ–°é»ï¼‰
        response = await self.handle_with_persona(
            query,
            persona="hr_counselor"  # ä¸åªæ˜¯ HRï¼Œé‚„æœ‰å¿ƒç†é—œæ‡·
        )
        
        # 3. ä¸»å‹•å»ºè­°ï¼ˆå‰µæ–°é»ï¼‰
        suggestions = await self.generate_proactive_suggestions(query)
        # ä¾‹å¦‚ï¼šå»ºè­° EAP æœå‹™ã€æ¨è–¦æ¸›å£“è³‡æº
        
        # 4. å®Œæ•´çš„ç”¢å“åŒ–è¼¸å‡º
        return {
            "primary_response": response,
            "care_package": {
                "detected_stress": True,
                "eap_resources": self.get_eap_resources(),
                "anonymous_counseling": self.get_counseling_link()
            },
            "next_steps": suggestions,
            "follow_up_scheduled": self.schedule_follow_up(query)
        }
```

---

## ğŸš€ å­¸ç¿’è·¯å¾‘ç¸½çµ

### ğŸ¯ å¿«é€Ÿé€šé“ï¼ˆ4 é€±å…¼è·å­¸ç¿’ï¼‰

**Week 1-2: åŸºç¤é€Ÿæˆ**
- Day 1-3: Prompt Engineering + ç‰ˆæœ¬ç®¡ç†
- Day 4-5: API æ¨¡å¼ + UX åŸºç¤
- Day 6-7: ç°¡å–® RAG + Context ç®¡ç†
- **ç”¢å‡º**: èƒ½å°è©±çš„ Bot Demo

**Week 3: æ ¸å¿ƒå¯¦æˆ°**
- Day 1-2: ç”¢å“æ€ç¶­ + æˆæœ¬å„ªåŒ–
- Day 3-4: Workflow è¨­è¨ˆ
- Day 5-7: å¯é æ€§åŸºç¤
- **ç”¢å‡º**: æœ‰ fallback çš„ç©©å®š Bot

**Week 4: å°ˆé¡Œè¡åˆº**
- Day 1-5: é–‹ç™¼å®Œæ•´å°ˆé¡Œ
- Day 6-7: éƒ¨ç½²èˆ‡å„ªåŒ–
- **ç”¢å‡º**: å¯éƒ¨ç½²çš„ MVP

### ğŸ“ å®Œæ•´é€šé“ï¼ˆ8 é€±æ·±åº¦å­¸ç¿’ï¼‰

æŒ‰ç…§å®Œæ•´ 8 å€‹æ¨¡çµ„å¾ªåºæ¼¸é€²ï¼Œæ¯é€±ä¸€å€‹æ¨¡çµ„ï¼Œæœ€å¾Œç”¨ 2 é€±å®Œæˆå°ˆé¡Œã€‚

---

## ğŸ’¼ è·æ¶¯ç™¼å±•å»ºè­°

### ğŸ“ˆ æŠ€èƒ½æˆé•·è·¯ç·šåœ–

```
åˆç´šï¼ˆ0-6å€‹æœˆï¼‰
â”œâ”€â”€ æŒæ¡ Prompt Engineering
â”œâ”€â”€ ç†Ÿæ‚‰ä¸»æµ LLM API
â””â”€â”€ èƒ½é–‹ç™¼ç°¡å–® Bot

ä¸­ç´šï¼ˆ6-18å€‹æœˆï¼‰
â”œâ”€â”€ ç²¾é€š RAG å’Œ Context ç®¡ç†  
â”œâ”€â”€ è¨­è¨ˆè¤‡é›œ Workflow
â”œâ”€â”€ å„ªåŒ–æˆæœ¬å’Œæ€§èƒ½
â””â”€â”€ é ˜å°å°å‹å°ˆæ¡ˆ

é«˜ç´šï¼ˆ18å€‹æœˆ+ï¼‰
â”œâ”€â”€ æ¶æ§‹å¤§å‹ LLM ç³»çµ±
â”œâ”€â”€ åˆ¶å®šæŠ€è¡“æ¨™æº–
â”œâ”€â”€ åŸ¹è¨“åœ˜éšŠ
â””â”€â”€ æ¨å‹•å‰µæ–°

å°ˆå®¶ï¼ˆ3å¹´+ï¼‰
â”œâ”€â”€ å®šç¾©ç”¢å“ç­–ç•¥
â”œâ”€â”€ è·¨åœ˜éšŠå”ä½œ
â”œâ”€â”€ é–‹æºè²¢ç»
â””â”€â”€ è¡Œæ¥­å½±éŸ¿åŠ›
```

### ğŸ’° è–ªè³‡åƒè€ƒï¼ˆå¹´è–ªï¼‰

| ç­‰ç´š | å°ç£ | ä¸­åœ‹ | ç¾åœ‹ |
|------|------|------|------|
| åˆç´š | 60-90è¬ TWD | 20-35è¬ RMB | $70-100K |
| ä¸­ç´š | 90-150è¬ TWD | 35-60è¬ RMB | $100-150K |
| é«˜ç´š | 150-250è¬ TWD | 60-100è¬ RMB | $150-250K |
| å°ˆå®¶ | 250è¬+ TWD | 100è¬+ RMB | $250K+ |

---

## ğŸ¯ ç«‹å³è¡Œå‹•è¨ˆåŠƒ

### ä»Šå¤©å°±é–‹å§‹ï¼ˆDay 1ï¼‰

```python
# Step 1: è¨­å®šç’°å¢ƒ
pip install openai langchain pinecone-client

# Step 2: ç¬¬ä¸€å€‹ Prompt Manager
class MyFirstPromptManager:
    def __init__(self):
        self.prompts = {
            "v1": "You are a helpful assistant.",
            "v2": "You are a helpful and empathetic assistant."
        }
        self.current = "v1"
    
    def get_prompt(self):
        return self.prompts[self.current]

# Step 3: æ¸¬è©¦ç‰ˆæœ¬å·®ç•°
manager = MyFirstPromptManager()
for version in ["v1", "v2"]:
    manager.current = version
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": manager.get_prompt()},
            {"role": "user", "content": "I'm feeling stressed"}
        ]
    )
    print(f"{version}: {response.choices[0].message.content}")
```

### æœ¬é€±ç›®æ¨™ï¼ˆWeek 1ï¼‰

- [ ] å®Œæˆæ¨¡çµ„ 1-2 çš„å­¸ç¿’
- [ ] å¯¦ä½œ 3 å€‹ä¸åŒçš„ Prompt ç­–ç•¥
- [ ] æ¸¬è©¦ 3 ç¨® UX æ¨¡å¼
- [ ] å»ºç«‹ç¬¬ä¸€å€‹å¸¶é©—è­‰çš„ API å‘¼å«

### æœ¬æœˆç›®æ¨™ï¼ˆMonth 1ï¼‰

- [ ] å®Œæˆæ¨¡çµ„ 1-6
- [ ] é–‹ç™¼ä¸€å€‹å®Œæ•´çš„ FAQ Bot
- [ ] å¯¦ç¾æˆæœ¬å„ªåŒ–ï¼Œé™ä½ 50% é–‹éŠ·
- [ ] éƒ¨ç½²åˆ°æ¸¬è©¦ç’°å¢ƒ

---

## ğŸ“š å­¸ç¿’è³‡æºæ¨è–¦

### ğŸ“– å¿…è®€è³‡æº
- [OpenAI Cookbook](https://cookbook.openai.com/) - å®˜æ–¹æœ€ä½³å¯¦è¸
- [LangChain Docs](https://docs.langchain.com/) - æ‡‰ç”¨é–‹ç™¼æ¡†æ¶
- [Pinecone Learning Center](https://www.pinecone.io/learn/) - å‘é‡è³‡æ–™åº«å…¥é–€

### ğŸ“ é€²éšèª²ç¨‹
- [Building LLM Apps with LangChain.js](https://www.deeplearning.ai/short-courses/building-applications-vector-databases/) - DeepLearning.AI
- [LLM Application Development](https://www.coursera.org/learn/llm-applications) - Coursera
- [Full Stack LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/) - FSDL

### ğŸ›  é–‹ç™¼å·¥å…·
- **Prompt æ¸¬è©¦**: [Promptfoo](https://promptfoo.dev/), [Langfuse](https://langfuse.com/)
- **ç›£æ§å¹³å°**: [Helicone](https://helicone.ai/), [Langsmith](https://smith.langchain.com/)
- **å‘é‡è³‡æ–™åº«**: [Pinecone](https://pinecone.io/), [Weaviate](https://weaviate.io/)

### ğŸ‘¥ ç¤¾ç¾¤è³‡æº
- [r/LocalLLaMA](https://reddit.com/r/LocalLLaMA) - Reddit ç¤¾ç¾¤
- [LLM Discord](https://discord.gg/llm) - Discord è¨è«–ç¾¤
- [AI Builder Club](https://aibuilder.club/) - é–‹ç™¼è€…ç¤¾ç¾¤

---

## ğŸ çµèªï¼šæˆç‚ºå¸‚å ´éœ€è¦çš„ LLM æ‡‰ç”¨å·¥ç¨‹å¸«

é‚„è¨˜å¾—é–‹é ­æåˆ°çš„å•é¡Œå—ï¼Ÿç‚ºä»€éº¼ 90% çš„ LLM æ‡‰ç”¨éƒ½åªæ˜¯ã€Œç©å…·ã€ï¼Ÿ

ç¾åœ¨ä½ æœ‰ç­”æ¡ˆäº†ï¼š**å› ç‚ºç¼ºä¹æ‡‰ç”¨å·¥ç¨‹çš„ç³»çµ±åŒ–æ€ç¶­ã€‚**

é€éé€™ 8 å€‹æ¨¡çµ„çš„å­¸ç¿’ï¼Œä½ å°‡æŒæ¡ï¼š
- âœ… å¦‚ä½•ç®¡ç† Prompt è³‡ç”¢ï¼Œè€Œä¸åªæ˜¯å¯« Prompt
- âœ… å¦‚ä½•è¨­è¨ˆé™ç´šç­–ç•¥ï¼Œè€Œä¸æ˜¯ç¥ˆç¦± API ä¸æœƒæ›
- âœ… å¦‚ä½•å„ªåŒ–æˆæœ¬æ•ˆç›Šï¼Œè€Œä¸æ˜¯ç‡’éŒ¢åš Demo
- âœ… å¦‚ä½•æ‰“é€ ç”¨æˆ¶é«”é©—ï¼Œè€Œä¸åªæ˜¯æŠ€è¡“ç‚«æŠ€

**å¸‚å ´ç¾ç‹€**ï¼š
- æ¯å®¶ä¼æ¥­éƒ½æƒ³å°å…¥ AIï¼Œä½†ç¼ºä¹è½åœ°èƒ½åŠ›
- å¤§é‡è·ç¼ºæ¹§ç¾ï¼Œä½†åˆæ ¼äººæ‰ç¨€ç¼º
- è–ªè³‡æŒçºŒä¸Šæ¼²ï¼Œæ©Ÿæœƒçª—å£æ­£åœ¨é–‹å•Ÿ

**ä½ çš„æ©Ÿæœƒ**ï¼š
- ä¸éœ€è¦ PhDï¼Œä¸éœ€è¦æ‡‚æ¨¡å‹åŸç†
- åªè¦ 2-3 å€‹æœˆï¼Œå°±èƒ½æŒæ¡æ ¸å¿ƒæŠ€èƒ½
- å¸‚å ´éœ€æ±‚æ˜¯ç³»çµ±å·¥ç¨‹å¸«çš„ **5-10 å€**

### ğŸš€ æœ€å¾Œçš„è¡Œå‹•å‘¼ç±²

**ä¸è¦å†ç­‰äº†ã€‚** æ¯å¤©éƒ½æœ‰æ–°çš„ LLM æ‡‰ç”¨ä¸Šç·šï¼Œæ¯å¤©éƒ½æœ‰å…¬å¸åœ¨æ‰¾ LLM æ‡‰ç”¨å·¥ç¨‹å¸«ã€‚

ç¾åœ¨å°±é–‹å§‹ï¼š
1. **ä»Šå¤©**ï¼šè¨­å®šç’°å¢ƒï¼Œå¯«ç¬¬ä¸€å€‹ Prompt Manager
2. **æœ¬é€±**ï¼šå®Œæˆæ¨¡çµ„ 1-2ï¼Œåšå‡º Demo
3. **æœ¬æœˆ**ï¼šé–‹ç™¼ç¬¬ä¸€å€‹å®Œæ•´æ‡‰ç”¨
4. **ä¸‰å€‹æœˆå¾Œ**ï¼šæˆç‚ºåˆæ ¼çš„ LLM æ‡‰ç”¨å·¥ç¨‹å¸«

è¨˜ä½ï¼š**åœ¨ AI æ™‚ä»£ï¼Œæ‡‰ç”¨å±¤æ‰æ˜¯ä¸»æˆ°å ´ã€‚**

ç³»çµ±å·¥ç¨‹å¸«å»ºé€ å¼•æ“ï¼Œä½†**æ‡‰ç”¨å·¥ç¨‹å¸«é–‹è»Šä¸Šè·¯**ã€‚

å•é¡Œæ˜¯â€”â€”**ä½ æº–å‚™å¥½ä¸Šè·¯äº†å—ï¼Ÿ**

---

**æœ€å¾Œæ›´æ–°**ï¼š2025å¹´8æœˆ  
**ä½œè€…**ï¼šIan Chou  
**è¯çµ¡æ–¹å¼**ï¼šian@citrine.com

---

### ğŸ¯ èª²ç¨‹å ±åè³‡è¨Š

**LLM OS æ‡‰ç”¨å·¥ç¨‹å¸«è¨“ç·´ç‡Ÿ**
- é–‹èª²æ™‚é–“ï¼šæ¯æœˆç¬¬ä¸€é€±
- èª²ç¨‹é•·åº¦ï¼š8é€±ï¼ˆç·šä¸Šï¼‰
- ä¸Šèª²æ–¹å¼ï¼šç›´æ’­æˆèª² + å¯¦ä½œ Lab + å°ˆé¡Œè¼”å°
- çµæ¥­è­‰æ›¸ï¼šå®Œæˆå°ˆé¡Œå¯ç²å¾—èªè­‰
- æ—©é³¥å„ªæƒ ï¼šå‰ 20 åå ±åäº« 8 æŠ˜

[ç«‹å³å ±å](https://citrine.top/courses/llm-app-engineer) | [ä¸‹è¼‰èª²ç¨‹å¤§ç¶±](https://citrine.top/download/syllabus)

---

> ğŸ’¡ **è¨˜ä½**ï¼šLLM ä¸æ˜¯é­”æ³•ï¼Œæ˜¯æ–°æ™‚ä»£çš„ APIã€‚æŒæ¡å®ƒï¼Œå°±æŒæ¡äº†æœªä¾† 10 å¹´çš„è·æ¶¯å„ªå‹¢ã€‚

---

## ğŸ§­ å¿«é€Ÿå°èˆª

### ğŸ“‹ æ–‡ç« å¤§ç¶±
- [å‰è¨€ï¼šç‚ºä»€éº¼ 90% çš„ LLM æ‡‰ç”¨éƒ½åªæ˜¯ã€Œç©å…·ã€ï¼Ÿ](#å‰è¨€ç‚ºä»€éº¼-90-çš„-llm-æ‡‰ç”¨éƒ½åªæ˜¯ç©å…·)
- [LLM æ‡‰ç”¨å·¥ç¨‹å¸« vs ç³»çµ±å·¥ç¨‹å¸«](#-llm-æ‡‰ç”¨å·¥ç¨‹å¸«-vs-ç³»çµ±å·¥ç¨‹å¸«)
- [å­¸ç¿’ç›®æ¨™èˆ‡èƒ½åŠ›å‡ºå£](#-å­¸ç¿’ç›®æ¨™èˆ‡èƒ½åŠ›å‡ºå£)
- [å®Œæ•´èª²ç¨‹æ¶æ§‹ï¼ˆ8 é€±é€Ÿæˆï¼‰](#-å®Œæ•´èª²ç¨‹æ¶æ§‹8-é€±é€Ÿæˆ)
- [æ¨¡çµ„ 1ï¼šPrompt for Applications](#-æ¨¡çµ„-1prompt-for-applicationsç¬¬-1-é€±)
- [æ¨¡çµ„ 2ï¼šLLM API èˆ‡ UX æ•´åˆ](#-æ¨¡çµ„-2llm-api-èˆ‡-ux-æ•´åˆç¬¬-2-é€±)
- [æ¨¡çµ„ 3ï¼šContext Engineering å¿…ä¿®åŸºç¤](#-æ¨¡çµ„-3context-engineering-å¿…ä¿®åŸºç¤ç¬¬-3-é€±)
- [æ¨¡çµ„ 4ï¼šçŸ¥è­˜æ›è¼‰èˆ‡ RAG åŸºç¤](#-æ¨¡çµ„-4çŸ¥è­˜æ›è¼‰èˆ‡-rag-åŸºç¤ç¬¬-4-é€±)
- [æ¨¡çµ„ 5ï¼šç”¢å“æ€ç¶­ for LLM Apps](#-æ¨¡çµ„-5ç”¢å“æ€ç¶­-for-llm-appsç¬¬-5-é€±)
- [æ¨¡çµ„ 6ï¼šæ‡‰ç”¨ç´š Workflow](#-æ¨¡çµ„-6æ‡‰ç”¨ç´š-workflowç¬¬-6-é€±)
- [æ¨¡çµ„ 7ï¼šå¯é æ€§èˆ‡å¯è§€æ¸¬æ€§](#-æ¨¡çµ„-7å¯é æ€§èˆ‡å¯è§€æ¸¬æ€§ç¬¬-7-é€±)
- [æ¨¡çµ„ 8ï¼šæ‡‰ç”¨è½åœ°èˆ‡ UX å¼·åŒ–](#-æ¨¡çµ„-8æ‡‰ç”¨è½åœ°èˆ‡-ux-å¼·åŒ–ç¬¬-8-é€±)
- [çµ‚æ¥µå°ˆé¡Œï¼šCapstone Project](#-çµ‚æ¥µå°ˆé¡Œcapstone-project)
- [å­¸ç¿’è·¯å¾‘ç¸½çµ](#-å­¸ç¿’è·¯å¾‘ç¸½çµ)
- [è·æ¶¯ç™¼å±•å»ºè­°](#-è·æ¶¯ç™¼å±•å»ºè­°)
- [ç«‹å³è¡Œå‹•è¨ˆåŠƒ](#-ç«‹å³è¡Œå‹•è¨ˆåŠƒ)

### ğŸ”— ç›¸é—œæ–‡ç« æ¨è–¦
- [ğŸ§­ å»ºé€ ä½ çš„ LLM OSï¼šå¾ Prompt åˆ° Production çš„å®Œæ•´å»ºæ§‹æŒ‡å—](/blog/2025-08-llm-os-complete-guide) - LLM OS ç³»çµ±æ¶æ§‹æ·±åº¦æŒ‡å—
- [ğŸš€ AI ç·¨ç¨‹åŠ©æ‰‹æ¯”è¼ƒï¼šClaude vs ChatGPT vs Gemini é–‹ç™¼è€…å¯¦æˆ°è©•æ¸¬](/blog/2025-06-ai-coding-assistant-comparison-claude-chatgpt-gemini-developers) - AI ç·¨ç¨‹å·¥å…·å°æ¯”
- [ğŸ“Š å°ç£è»Ÿé«”å·¥ç¨‹å¸«å°±æ¥­å¸‚å ´åˆ†æï¼š2025 å¹´è¶¨å‹¢èˆ‡æ©Ÿæœƒ](/blog/2025-06-taiwan-software-engineer-job-market-analysis) - è·æ¶¯ç™¼å±•åƒè€ƒ
- [ğŸ”§ è¦æ ¼é©…å‹•é–‹ç™¼ï¼š2025 å¹´è»Ÿé«”é–‹ç™¼çš„æ–°å…¸ç¯„](/blog/2025-07-specification-driven-development-2025-paradigm) - æ–°èˆˆé–‹ç™¼æ¨¡å¼

### ğŸ’¬ äº’å‹•èˆ‡è¨è«–
- åœ¨ä¸‹æ–¹ç•™è¨€åˆ†äº«ä½ çš„å­¸ç¿’å¿ƒå¾—
- åŠ å…¥æˆ‘å€‘çš„ [Discord ç¤¾ç¾¤](https://discord.gg/llm) è¨è«–æŠ€è¡“å•é¡Œ
- è¿½è¹¤ [@ian_chou](https://twitter.com/ian_chou) ç²å–æœ€æ–° AI é–‹ç™¼è³‡è¨Š

---

**ğŸ“§ è¨‚é–±æ›´æ–°**ï¼šæƒ³è¦æ”¶åˆ°æ›´å¤š AI é–‹ç™¼ç›¸é—œæ–‡ç« ï¼Ÿ[è¨‚é–±æˆ‘å€‘çš„é›»å­å ±](https://citrine.top/newsletter)